// vim:set ft=asciidoc cc=80 tw=80:
= Chapter 4: Kubernetes and Contrail integration 
:toc: right
:toclevels: 3
//:toc-placement: preamble
:source-highlighter: pygments
:source-highlighter: coderay
:source-highlighter: prettify
:highlightjs-theme: googlecode
:coderay-linenums-mode: table
:coderay-linenums-mode: inline
:numbered:

This chapter expalins contrail role in kubernetes world. we'll start with a
section about contrail kubernetes integration architecture, where you will learn
how kubernetes objects are handled in contrail. those objects are NS, pod,
service, ingress, network policy and etc. then we'll look into implementation of
each of them in detail. we also introduce some contrail objects whenever needed.
multiple interface pod is a highlight of contrail's advantages as one of
kubernetes network CNI over other implementations, so we cover that also. in the
end, we will demonstate service chain using Juniper CSRX container.

== Contrail-Kubernetes architecture 
=== Why contrail with Kubernetes ?

Now after we have seen the main concepts of Kubernetes in chapter 2 and 3, what
could be the gain in adding Contrail to standard Kubernetes deployment ?

in brief, Contrail offers common deployment for multiple environments
(OpenStack, Kubernetes, etc) as well it enriches Kubernetes networking and
security capabilities.

When it comes to deployment for multiple environments, Yes containers is the
current trend to build applications, but don’t expect everyone to migrate
everything from VM to containers that fast (This is not to mention the nested
approach where containers are hosted in VM). if we add to the picture
workload fully or partially run in the public cloud, we end up feeling the
misery for network and security administrators where Kubernetes becomes just
one thing to manage Network and security. 

administrator in many organization manage individual orchestrator/manager for
each environment. OpenStack or VMware NSX for VM, Kubernetes or Mesos for
Containers, AWS console.  and here what contrail could put the network and
security administrators out of their misery is it provides dynamic end-to-end
networking policy and control for any cloud, any workload, and any deployment.

from a single user interface contrail translates abstract workflows into
specific policies, simplifying the orchestration of virtual overlay connectivity
across all environments by building and securing virtual networks that connect
BMS, VM and Containers located in private or public cloud. 

A very common way to deploy Kubernetes is to launch its POD in VMs orchestrated
by OpenStack. this is one of the many use cases of contrail doing its magic.

In this book we won’t cover contrail integration with other environments as we
focus only in Kubernetes. But any feature that we explain in here could be
extended for other environments.

what we mean by contrail enriching standard Kubernetes deployment?

Eventhough kubernetes does not provide the networking, it imposes fundamental
requirements on the networking implementation and it is taken care by all CNI
("Container Network Interface") providers. contrail is one of the cni providers. you
can refer to https://kubernetes.io/docs/concepts/cluster-administration/networking/
for more available CNI providers.

kubernets has defined some fundamental requirements in networking implementation:

. pods on a node can communicate with all pods on all nodes without NAT
. agents on a node (e.g. system daemons, kubelet) can communicate with all
  pods on that node
. pods in the host network of a node can communicate with all pods on all
  nodes without NAT
  
with these requirements to all CNI plugins implementations, Kubernetes offers
flat network connectivity with some security feature confined in a cluster, but
Contrail could offer on top of that:

. namespaces and services customized isolations for segmentations and
  multi-tenancy
. distributed loadbalancing and firewall with extensive centralized flow and
  logs insight
. rich security policy using tags that can extend to other environment
  (OpenStack, VMWare, BMS, AWS ,..,etc)
. service chaining

In this chapter we will cover some of these aspects, but first let’s talk about
Kubernetes/contrail architecture and the object mapping 

=== contrail-kube-manager

A new components of contrail has been added called `contrail-Kube-manager`,
abbreviated as `KM`. what it does basically is to watch kubernetes apiserver for
interested kubernetes resources, and translates into Contrail controller object.
the following figure illustratesthe basic work flow:

.contrail kubernetes architecture

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/kubemanager.png[]
//image::https://raw.githubusercontent.com/pinggit/kubernetes-contrail-day-one/master/diagrams/kubemanager.png?token=AAPRSHE5SF522ETPA6NAUDK5D7PHS[]
//image::https://github.com/aymanaborabh/kubernetes-contrail-day-one/blob/master/diagrams/kube-manager-chapter%204.png[]
image::https://user-images.githubusercontent.com/2038044/63705791-e9846f00-c7fb-11e9-8ba7-0638aee5d16f.png[]

////
ping: 

=== contrail-kube-manager

.contrail

image::https://user-images.githubusercontent.com/2038044/59642949-fb2f0380-9134-11e9-86d2-1035e5b901b7.png[]

.kubernetes
image::https://user-images.githubusercontent.com/2038044/59642835-94a9e580-9134-11e9-9053-80505cb1ba75.png[]

.contrail kubernetes
image::https://user-images.githubusercontent.com/2038044/59642699-1a796100-9134-11e9-8a58-fb529b329cba.png[]

////

=== kubernetes to contrail object mapping

So not much of change of the regular contrail that we have seen before and all
of that is happening behind the scene.
what we have to be aware of it before dealing with Kubernetes/contrail is the
object mapping. because contrail is single interface managing multiple
environments - as explained before – each environment has its own acronym and
terms hence the need for this mapping, which will be done by a plugin. in
kubernetes `contrail-kube-manager` does this. 

NOTE: contrail has specific plugins for each environments/orchestrator.
 
For example, Namespace in Kubernetes are intended for segmentation between
multiple teams, or projects as if we are creating virtual cluster. In contrail
the similar concept would be named as project so when you create a namespace in
Kubernetes it will automatically create an equivalent project in contrail. more
on that will come later on for now kindly make yourself familiar with this list
of object mapping 

.contrail kubernetes object mapping

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/chapter%204%20contrail%20-%20k8s%20mapping.png[]
//image::https://user-images.githubusercontent.com/2038044/60748774-6bc08780-9f5f-11e9-91ae-2ec496cab987.png[]
image::https://user-images.githubusercontent.com/2038044/63705887-1cc6fe00-c7fc-11e9-8c4f-733676cf663a.png[]

== contrail command
Before getting into deeper, we just want to introduce contrail-command(CC) which
is the new user interface (UI) available from contrail 5.0.1. throughout this book
we use both CC and old UI to demonstrate most of lab studies. just keep in mind that
in the future CC will be the only UI and the "legacy" one will be deprecated. 

in CC, the functions and settings are groups in a a "main menu". it is also the
entry point from where you can navigate through different functions. 

.contrail command main menu
image::https://user-images.githubusercontent.com/2038044/60282872-ed684380-98d5-11e9-92f7-e1df07c5fecf.png[]

in order to get this menu, click on group name right next to the "contrail command"
logo on the upper left of the UI. in the above screen capture that group is
"Infrastructure", but regardless it can be any group, just click it and you will get
the main menu, then from there you can select and jump into all other settings.

Again our focus is not CC. we are trying to give base insight about CC which would be
helpful for our primary goal of the book.

== contrail namespaces and isolation

=== namespace introduction

In chapter3 you`ve read about `namespace` or `NS` in kubernetes. in the
beginning of this chapter we've mentioned object mappings between kubernetes and
contrail. in this section we'll see how NS works in contrail environments and
how contrail extends the feature.

one analogy we`ve given when introducing `namespace` concept is openstack
`project`, or `tenant`. that is exactly how contrail is looking at it. whenever
a new `namespace` object is created, `contrail-kube-manager` (KM) gets noticed
about the object creation event and it will create the corresponding `project`
in contrail. 

To differentiate between multiple kubernetes clusters in contrail,
a kubernetes cluster name will be added to the kubernetes NS or project name.
the default kubernetes cluster name is `k8s`.  so if you create a kubernetes NS
`ns-user-1`, `k8s-ns-user-1` project will be created in contrail and you can see
the same in the contrail GUI.

.contrail command: projects
image::https://user-images.githubusercontent.com/2038044/60316467-8fb91300-9938-11e9-9de6-429b56429868.png[]

****
the kubernetes `cluster name` is configurable, during deployment process only.
if you don't configure it `k8s` will be the default. once the cluster is
created, the name can not be changed anymore. to view the `cluster name`, go to
`contrail-kube-manager` (KM) docker and check its the configuration file.

.to locate the `KM` docker container
----
$ docker ps -a | grep  kubemanager
2260c7845964  ...snipped...  ago  Up  2  minutes  kubemanager_kubemanager_1
----

.to login to the `KM` container
----
$ docker exec -it kubemanager_kubemanager_1 bash
----

.find the `cluster_name` option
----
$ grep cluster /etc/contrail/contrail-kubernetes.conf
cluster_name=k8s        #<---
cluster_project={}
cluster_network={}
----

****

NOTE: in the rest part of this book we will refer all these terms `namespace`,
`NS`, `tenant`, `project` interchangeably.

=== Non-isolated NS

you are aware that kubernetes basic networking requirement is a "flat"/"NATless"
network - any pod can talk to any pod in any namespace, any cni providers should
ensure that. consequently in kubernetes by default all namespaces are **not**
isolated.

NOTE: the term "isolated" and "non-isolated" are in the context of (contrail)
networking only. 

.k8s-default-pod-network and k8s-default-service-network

To provide networking for all non-isolated namespace, there should be a
**common** VRF (virtual routing and forwarding table) or RI (routing instance).
in contrail kubernetes environment, two "default" VNs are pre-configured in k8s
default NS, for pod and service respectively. correspondingly there are
2 VRFs each with same name as their correspondingly VN. 

the name of the two VNs/VRFs are in this format:

    <k8s-cluster-name>-<namespace name>-[pod|service]-network

so for `default` NS with a default cluster name `k8s`, the two VN/VRF names will
become:

* `k8s-default-pod-network`: pod VN/VRF, with the default subnet `10.32.0.0/12`
* `k8s-default-service-network`: service VN/VRF, with a default subnet `10.96.0.0/12`

NOTE: the default subnet for pod or service is configurable.

it is important to know that these 2 default VNs are **shared** between all of
the "non-isolated" namespaces. what that means is, they will be available for
any new non-isolated NS that you create, implicitly.  that is why pods from
all non-isolated NS including default NS can talk to each other.

on the other hand, any VNs that you create will be isolated with other VNs,
regardless of same or different NS. communication between pods in two different
VNs requires contrail network policy.

NOTE: later when you read about kubernetes `service`, you may wonder why packets
destined service VN/VRF can reach the backend pod in pod VN/VRF? the answer is
also contrail network policy. by default contrail network policy is enabled
between service and pod network which allows packets arriving service VN/VRF to
reach the pod, and vice versa. 

for the isolated NS, however, it will be a different story.

=== Isolated NS 

in contrast, "isolated" namespace, will have its own default pod-network and
service-network, accordingly two new VRFs are also created for each "isolated"
namspace. The same flat-subnets `10.32.0.0/12` and `10.96.0.0/12` are shared by
the pod and service networks in the isolated namespaces. however since the
networks are with a different VRF, by default it is isolated with other NS.
pods launched in isolated NS can only talk to service and pods on the same
namespace. Additional configurations, e.g. policy, is required to make the pod
being able to reach the network outside of current namespace.

to illustrate this concept let's take an example. suppose you have 3 namespaces,
the `default` NS and two user NS: `ns-non-isolated` and `ns-isolated`.
in each NS you create one user VN: `vn-left-1`. you will end up to have
following VN/VRFs in contrail:

.NS default

* default-domain:k8s-default:k8s-default-pod-network
* default-domain:k8s-default:k8s-default-service-network
* default-domain:k8s-default:k8s-vn-left-1-pod-network

.NS ns-non-isolated

* default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network

.NS ns-isolated

* default-domain:k8s-ns-isolated:k8s-ns-isolated-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-service-network
* default-domain:k8s-ns-isolated:k8s-vn-left-1-pod-network

NOTE: The above name is mentioned in the FQDN format. In contrail domain is the
top-level object, followed by project/tenant and followed by virtual-networks.

////
* default-domain:k8s-default:k8s-default-pod-network:k8s-default-pod-network
* default-domain:k8s-default:k8s-default-service-network:k8s-default-service-network
* default-domain:k8s-default:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
* default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-pod-network:k8s-ns-isolated-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-service-network:k8s-ns-isolated-service-network
* default-domain:k8s-ns-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
////

this can be illustrated in below diagram:

.NS and VN
image::https://user-images.githubusercontent.com/2038044/63223271-13e18700-c181-11e9-8fe4-987cf935a05b.png[]

here is the yaml file to create an isolated namespace:

----
$ cat ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation" : "true"
  name: ns-isolated
----

to create the NS:

----
kubectl create -f ns-isolated.yaml

$ kubectl get ns
NAME          STATUS    AGE
contrail      Active    8d
default       Active    8d
ns-isolated   Active    1d  #<---
kube-public   Active    8d
kube-system   Active    8d
----

the annotations under metadata are something additional comparing to standard
(non-isolated) k8s namespace, the value of `true` indicates this is an isolated
NS:

  annotations:
    "opencontrail.org/isolation" : "true"

this part of the definition is Juniper's extension. `contrail-kube-manager`
(`KM`) , reads the namespace `metadata` from `kube-apiserver`, parses the
information defined in the `annotations` object, and sees that the `isolation`
flag is set to `true`. it then creates the tenant with the correponding routing
instances(one for pod and one for service) instead of using the default ns
routing instances for the isolated namespace. fundamentally that is how the
"isolation" is implemented. 

in the following sections we'll verify how the routing isolation works.

=== communication between pod in different namespaces

create a non-isolated namespace and an isolated namespace:

----
$ cat ns-non-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ns-non-isolated

$ cat ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation": "true"
  name: ns-isolated

$ kubectl apply -f ns-non-isolated.yaml
namespace/ns-non-isolated created

$ kubectl apply -f ns-isolated.yaml
namespace/ns-isolated created

$ kubectl get ns | grep isolate
ns-isolated       Active   79s
ns-non-isolated   Active   73s
----

in both NS and the default NS, create a deployment to launch a pod:

----
$ kubectl apply -f deployment-cirros.yaml -n default
deployment.extensions/cirros created

$ kubectl apply -f deployment-cirros.yaml -n ns-non-isolated
deployment.extensions/cirros created

$ kubectl apply -f deployment-cirros.yaml -n ns-isolated
deployment.extensions/cirros created

$ kubectl get pod -o wide -n default
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-tjfn6  1/1    Running  0         13s  10.47.255.242  cent333  <none>

$ kubectl get pod -o wide -n ns-non-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-nrxq6  1/1    Running  0         23s  10.47.255.248  cent222  <none>

$ kubectl get pod -o wide -n ns-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222  <none>
----

ping between all pods in 3 namespaces

----
#default ns to non-isolated new ns: succeed
$ kubectl -n default exec -it cirros1-85fc7dd848-tjfn6 -- ping 10.47.255.248
PING 10.47.255.248 (10.47.255.248): 56 data bytes
64 bytes from 10.47.255.248: seq=0 ttl=63 time=1.600 ms
^C
--- 10.47.255.248 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.600/1.600/1.600 ms

#default ns to isolated new ns: fail
$ kubectl -n default exec -it cirros1-85fc7dd848-tjfn6 -- ping 10.47.255.239
PING 10.47.255.239 (10.47.255.239): 56 data bytes
^C
--- 10.47.255.239 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
----

the test result shows that, bidirectional communication between two non-isolated
namespaces (namespace `ns-non-isolated` and `default` in this case) works, but
traffic from non-isolated NS (`default` NS) toward isolated NS does not pass
through. what about traffic within the same isolated NS? 

with the power of the `deployment` we can quickly test it out: in isolated NS
`ns-isolated`, clone one more pod by `scale` the deployment with `replicas=2`
and ping between the 2 pods:

----
$ kubectl scale deployment cirros --replicas=2
$ kubectl get pod -o wide -n ns-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222  <none>
cirros-85fc7dd848-215k8  1/1    Running  0         8s   10.47.255.238  cent333  <none>

$ kubectl -n ns-isolated exec -it cirros-85fc7dd848-6l7j2 -- ping 10.47.255.238
PING 10.47.255.238 (10.47.255.238): 56 data bytes
64 bytes from 10.47.255.238: seq=0 ttl=63 time=1.470 ms
^C
--- 10.47.255.238 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.470/1.470/1.470 ms
----

the ping packet passes through now. to summarize the test results: 

* traffic is isolated between an isolated NS and all other tenant in the cluster
* traffic is not isolated in same NS 

NOTE: pod-level isolation can be achieved via kubernetes network policy, or
security groups in contrail. 
this will be covered later in this chapter.

== contrail floating IP

//(with type of loadBalancer or nodePort) 

=== overlay Internet access

we've discussed and tested the communication between pods in the same or
different NS. so far we've only tested it **inside** of the same cluster. what
about communication with devices **outside** of the cluster? you may already
know that in traditional (openstack) contrail environment, there are many ways
for the overlay entities (typically a VM) to access the Internet, the 3
frequently used methods among them are:

* floating IP
* fabric SNAT
* logical router

the prefered kubernetes solution to expose any service is via `service` 
and `Ingress` objects which you've read about and got the idea in chapter 3.
in contrail kubernetes environment, floating IP is used in the service and Ingress
implementation to expose them to outside of the cluster. later in this chapter
we'll have a very detail discussion for each of these two objects. befor that,
in this section, we'll review the "floating IP" basis and look at how it works
with kubernetes.

NOTE: `fabric SNAT` and `logical router` are used by overlay workloads(VM and
POD) to reach the internet and the reverse direction is not possible. `floating
IP` however, supports both direction - you can configure it to support ingress
traffic, egress traffic, or both and default is bi-direction. in this book we
focus on `floating IP` only. you can refer contrail documents for detail
information about fabric SNAT and logical router.

=== floating IP introduction

`floating IP`, or `FIP` for short, is a "traditional" concept that contrail
supports since very early releases. Essentially it is an openstack concept to
"map" a VM IP, which is typically a private IP address, to a public IP (the
"floating IP" in this context) that is reachable from the outside of the
cluster. Internally the one to one mapping is implemented by NAT. whenever a
vrouter receives packets from outside of the cluster destined to the floating
IP, it will translate it to the VM's private IP and forward the packet to the
VM. similarly it will do the translation on reverse direction. Eventually both
VM and Internet host can talk to each other, and both can initiate the
communication.

NOTE: vrouter is a contrail forwarding plane resides in each compute node handles
workloads traffic

the figure below illustrates the basic work flow of FIP:

.Floating IP
//image::https://user-images.githubusercontent.com/2038044/60388331-be8cd180-9a7d-11e9-8ff7-c202ed9f7349.png[]
//image::https://user-images.githubusercontent.com/2038044/60556767-b8faea00-9d10-11e9-84bb-0e40e3edcc3d.png[]
//image::https://user-images.githubusercontent.com/2038044/60357106-b448d580-99a0-11e9-8ad2-31e15102b6bd.png[]
//image::https://user-images.githubusercontent.com/2038044/63227026-0ee7fc00-c1b0-11e9-8e59-d247ec8d7b2e.png[]
image::https://user-images.githubusercontent.com/2038044/63263460-a4d66200-c256-11e9-8d83-012ae4a8ab26.png[]

here are some highlights regarding FIP:

* a FIP is associated with a VM's `port`, or a `VMI` (Virtual Machine
  Interface).
* a FIP is allocated from a `FIP pool`
* a FIP pool is created based on a virtual network(`FIP-VN`)
* the `FIP-VN` will be available to outside of the cluster, by setting matching
  `route-target` (`RT`) attributes of gateway routers VRF table . 
* when a gateway router sees a match with its route import policy in the RT,
  it will load the route into its VRF table. all remote clients connected to
  the VRF will be able to communicate with the FIP.

Regarding the FIP concept and role, there is nothing new in contrail kubernetes
environment. But the usage of floating IP has been extended in kubernetes
`service` and `ingress` object implementation, and it plays an important role
for accessing toward kubernetes `service` and `ingress` from external. 
you can check later sections in this chapter for more details on this.

=== creating FIP pool

creating a FIP pool is a 3 steps process:

* create a public FIP-VN, 
* set `RT` (route-target) for the VN so it can be advertised and imported into
  the gateway router's VRF.
* create a FIP pool based on the public FIP-VN

again this is nothing new but the same steps as with other contrail environment
without kubernetes. however, as you've learned in previous section, with
kubernetes integration a FIP-VN can now be created in a "kubernetes style":

.create a public FIP-VN named `vn-ns-default`

----
$ cat vn-ns-default.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "101.101.101.0/24"
  name: vn-ns-default
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "contrail-k8s-cni"
  }'

$ kubectl apply -f vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default unchanged

$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-ns-default   22d
----

.set the `RT`

if you need the FIP to be reachable from Internet through gateway router, you'll
need to set a route-target to make the VN prefix getting imported in the gateway
router's VRF table. this step is necessary whenever Internet access is required.

.contrail command: setting RT
image::https://user-images.githubusercontent.com/2038044/60751261-b43c6d00-9f80-11e9-93c5-b06aeb642eb0.png[]

NOTE: the UI navigation path to set RT is:
contrail command(CC): main-menu > Overlay > "Virtual Networks" >
k8s-vn-ns-default-pod-network > Edit > "Routing, Bridging and Policies"


////
NOTE: in the later lab demo of `service` or `ingress`, you always need to set the
RT to the public VN whenever they need to be accessed from Internet host, 
////

.create a FIP pool based on the public VN

this is the final step. from contrail command UI, Create a floating IP pool
based on the public VN:

.contrail command: create a FIP pool
image::https://user-images.githubusercontent.com/2038044/60357727-6d5bdf80-99a2-11e9-90c1-98b037cb0c98.png[]

NOTE: the UI navigation path for this setting is: contrail-command: main-menu >
Overlay > Floating IP > Create

TIP: in contrail UI, you can also set the "external" flag in VN "Advanced"
options so that a FIP pool named "public" will automatically be created.

=== FIP pool scope

there are different ways you can refer an floating IP pool in contrail
kubernetes environment, and correspondingly the scope of the pools will also be
different. here are 3 possible levels with descending priority:

* object specific
* Namespace level
* global level

.object specific

this is the most specific level of scope. object specific FIP pool binds itself
only to the object that you specified, it does not affect any other objects in
the same NS or the cluster. E.g. you can specify a service object `web` to get
FIP from FIP pool `pool1`, a service object `dns` to get FIP from another FIP
pool `pool2`, etc.  This gives the most granular control of where the FIP will
be allocated from for an object, the cost is that you need to explicitly specify
it in your yaml file for every object.

.NS level

In a multi tenancy environment each namespace would be associated to a tenant,
and each tenannt would have dedicated FIP pool. In that case it is better to
have a option to define at "NS level" FIP pool, so that all objects created in
that NS will get FIP assignment from that pool. with NS level pool defined
(e.g. `pool-ns-default`), there is no need to specify the FIP-pool name in each
object's yaml file any more. you can still give a different pool name, say
`my-webservie-pool` in an object `webservice` , in that case object `webservice`
will get the FIP from `my-webservice-pool` instead of from the NS level pool
`pool-ns-default`, because the former is more specific.

.global level

the scope of the "global" level pool will be the whole cluster. objects in any
namespaces can use the "global" FIP pool.

you can combine all 3 methods to take advantages of the flexibility. here is a
practical example:

* define a global pool `pool-global-default`, so any objects in a NS that has no
  NS-level or object-level pool defined, will get a FIP from this pool
* for NS `dev`, define a FIP pool `pool-dev`, so all objects created in NS `dev`
  will by default get FIP from `poo-dev`
* for NS `sales`, define a FIP pool `pool-sales`, so all objects created in NS
  `sales` will by default get FIP from `poo-dev`
* for NS `test-only`, do NOT define any NS level pool, so by default objects
  created in it will get FIP from the `pool-global-default`
* when a service `dev-websevice` in NS `dev` needs a FIP from `pool-sales`
  instead of `pool-dev`, specify `pool-sales` in `dev-webservice` object yaml
  file will achieve this goal.

NOTE: Just keep in mind the rule of thumb - the most specific scope will always
prevail.

=== object level FIP pool

let's first take a look at the object-specific FIP pool. here is an example:

----
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb-pool-public-1
  annotations:
    "opencontrail.org/fip-pool": "{'domain': 'default-domain', 'project': 'k8s-ns-user-1', 'network': 'vn-public-1', 'name': 'pool-public-1'}"
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer
----

in this example, service `service-web-lb-pool-public-1` will get an FIP from
pool `pool-public-1`, which is created based on VN `vn-public-1` under current
project `k8s-ns-user-1`. the corresponding kubernetes NS is `ns-user-1`. since
object level FIP pool is assigned for this specific object only, with this
method each new object needs to be assigned a FIP pool explicitly.

=== NS level FIP pool

the next FIP pool scope is in NS level. each NS can define its own FIP
pool.  same way as kubernetes annotations object is used to give a subnet to a
VN, it is also used to specify a FIP pool. the yaml file looks:

----
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    opencontrail.org/isolation: "true"
    opencontrail.org/fip-pool: "{'domain': 'default-domain', 'project': 'k8s-ns-user-1', 'network': 'vn-ns-default', 'name': 'pool-ns-default'}"
  name: ns-user-1
----

in this example, NS `ns-user-1` is given a NS level FIP pool named
`pool-ns-default`, and the corresponding VN is `vn-ns-default`. once the NS
`ns-user-1` is created with this yaml file, any new service which requires an
FIP, if not created with the object-specific pool name in its yaml file, will
get a FIP allocated from this pool. In practice, most NS (especially
those isolated NS) will need its own NS default pool so you will see this
type of configuration very often in field.

=== global level FIP pool

to specify a global level FIP pool, you need to give the full qualified pool
name (domain > project > network > name) in contrail-kube-manager('KM') docker's
configuration file(`/etc/contrail/contrail-kubernetes.conf`). This file is
automatically generated by the docker during its bootup based on its ENV
parameters, which can be found in '/etc/contrail/common_kubemanager.env` file in
master node:

----
$ cat /etc/contrail/common_kubemanager.env
VROUTER_GATEWAY=10.169.25.1
CONTROLLER_NODES=10.85.188.19
KUBERNETES_API_NODES=10.85.188.19
RABBITMQ_NODE_PORT=5673
CLOUD_ORCHESTRATOR=kubernetes
KUBEMANAGER_NODES=10.85.188.19
CONTRAIL_VERSION=master-latest
KUBERNETES_API_SERVER=10.85.188.19
TTY=True
ANALYTICS_SNMP_ENABLE=True
STDIN_OPEN=True
ANALYTICS_ALARM_ENABLE=True
ANALYTICSDB_ENABLE=True
CONTROL_NODES=10.169.25.19
----

as you can see, this `.env` file contains important environmental parameters
about the setup. to specify a `global FIP pool`, add following line in it:

----
KUBERNETES_PUBLIC_FIP_POOL={'domain': 'default-domain','name': 'pool-global-default','network': 'vn-global-default','project': 'k8s-ns-user-1'}
----

it reads: the global FIP pool is called `pool-global-default`, and it
is defined based on a VN `vn-global-default` under project `k8s-ns-user-1`.
which indicates that the corresponding kubernetes namespace is `ns-user-1`.

now with that piece of configuration placed, you can "re-compose" the
`contrail-kube-manager` docker container to make the change take effect.
essentially you need to tear it down and then bring it back up:

----
$ cd /etc/contrail/kubemanager/
$ docker-compose down;docker-compose up -d
Stopping kubemanager_kubemanager_1 ... done
Removing kubemanager_kubemanager_1 ... done
Removing kubemanager_node-init_1   ... done
Creating kubemanager_node-init_1 ... done
Creating kubemanager_kubemanager_1 ... done
----

now the global FIP pool is specified for the cluster.

NOTE: In all three scopes, FIP is automatically allocated and associated
only to service and ingress objects. If the FIP has to be associated to a
POD it has to be done manually. we'll talk about this in next section.


=== creating FIP for PODS

once FIP pool is created and available, an FIP can be allocated from the FIP
pool for the objects that requires one. this can be done by associating an FIP
to a VMI (VM or pod interface),

you can manually create a FIP out of a FIP pool in contrail UI, and then
associate it with a pod VMI.

.create FIP
image::https://user-images.githubusercontent.com/2038044/61014424-567b9c80-a355-11e9-832e-3a7f33d2590e.png[]

.associate a FIP in a pod interface
image::https://user-images.githubusercontent.com/2038044/61014684-aa3ab580-a356-11e9-92e7-882e21dd6657.png[]

NOTE: make sure the FIP pool is shared to the project where FIP is going to be
created.

=== advertising FIP

once a FIP is associated to a pod interface, it will be advertised to the MP-BGP
peers, which are typically gateway routers.

following screenshot shows how to add/edit a BGP peer.

.contrail command: select "main-menu" > INFRASTRUCTURE: "Cluster" > "Advanced Options"
image::https://user-images.githubusercontent.com/2038044/61074698-4c55ae80-a3e6-11e9-81d5-5efa962cbdb5.png[]

.contrail command: select "BGP router" > "create"
image::https://user-images.githubusercontent.com/2038044/63260144-2bd30c80-c24e-11e9-973a-aa911e7d2ae1.png[]

.edit BGP peer parameters
image::https://user-images.githubusercontent.com/2038044/61074999-0cdb9200-a3e7-11e9-80a3-b180d6454267.png[]

input all the BGP peer information, don't forget to associate the controller(s),
which is shown next:

.associate the peer to a controller
image::https://user-images.githubusercontent.com/2038044/61075110-4d3b1000-a3e7-11e9-8eec-ece0304ce4d8.png[]

from the dropdown of `peer` under `Associated Peers`, select the controller(s)
to peer with this new BGP router that you are trying to add.  click `save` when
done. a new BGP peer with ROUTER TYPE "router" will pop up.

.a new BGP router in the BGP router list
//image::https://user-images.githubusercontent.com/2038044/61074880-be2df800-a3e6-11e9-82af-7e58ccd7e710.png[]
image::https://user-images.githubusercontent.com/2038044/61079058-1289a580-a3f0-11e9-93a7-85eb53397a32.png[]

now we've added a peer BGP router as type "router". for local BGP speaker which
is with type "control-node", we just need to double check the parameters by
clicking `edit` button. in our test we want to build MP-IBGP neighborship
between contrail controller and gateway router, so we make sure the ASN and
"Address Families" matches on both end.

.contrail controller BGP parameters: ASN
image::https://user-images.githubusercontent.com/2038044/61075264-94c19c00-a3e7-11e9-90bd-6006dad35ef0.png[]

now you can check BGP neighborship status in gateway router.

----
labroot@camaro> show bgp summary | match 10.169.25.19
10.169.25.19          60100       2235       2390       0      39    18:19:34 Establ
----

once the neighborship is "Established", BGP routes will be exchanged between the
two speakers, that is the time we'll see that the FIP assigned to the kubernetes
object is advertised by master node (`10.169.25.19`) and learned in the gateway
router.

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.2
Jul 11 01:18:31

k8s-test.inet.0: 8 destinations, 8 routes (8 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.2/32   *[BGP/170] 00:01:42, MED 200, localpref 100, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 47
----

the `detail` version of same command tells more: the FIP route is reflected from
the contrail controller, but "Protocol next hop" being the compute node
(`10.169.25.20`) indicates that the FIP is assigned to a compute node. 
one entity currently running in that compute node own the FIP.

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.2 detail | match "next hop"
Jul 11 01:19:18
                Next hop type: Indirect, Next hop index: 0
                Next hop type: Router, Next hop index: 1453
                Next hop: via gr-2/3/0.32771, selected
                Protocol next hop: 10.169.25.20
                Indirect next hop: 0x900e640 1048601 INH Session ID: 0x70f
----

//in this capture the next hop is on `10.169.25.20`, node `cent222`. 
the dynamic soft GRE configuration make the gateway router automatically create
a soft GRE tunnel interface:

----
labroot@camaro> show interfaces gr-2/3/0.32771
Jul 11 01:19:53
  Logical interface gr-2/3/0.32771 (Index 432) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000 Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` indicates GRE outer IP header, so the "tunnel" is built from
current gateway router whose BGP local address is `192.168.0.204`, to remote 
node `10.169.25.20`, in this case it's one of the contrail compute nodes.

the FIP advertisement process is illustrated in this figure below:

.FIP advertisement
//image::https://user-images.githubusercontent.com/2038044/63262377-c5e98380-c253-11e9-996f-27eecb0df931.png[]
image::https://user-images.githubusercontent.com/2038044/63263090-a4899700-c255-11e9-8e76-cbee47c2faae.png[]

== contrail services

in this section, we look at kubernetes `service` in contrail environment.
specifically, we'll focus on `clusterIP` and `loadbalancer` type of services
that is commonly used in practice. contrail uses its `loadbalancer` object to
implement these two type of services. we'll first review the concept of legacy
contrail neutron loadbalancer, then we'll look into the extended ECMP
loadbalancer object which is the object that these two type of`service` are
based on in contrail, for the rest part of this section we will explore how
`clusterIP` and `loadbalancer` service works in detail, each with a test we
build in our testbed.

=== kubernetes service introduction

service is the core object in kubernetes. in chapter 3 you've learned what is
kubernetes service and how to create a `service` object with yaml file.
functional-wise, a service is running as a layer 4 (transport layer) load
balancer that is sitting between clients and servers. client can be anything
"requesting" a service. server in our context is the backend pods "responding"
the request. the client only sees the "frontend" - a service IP and service port
exposed by a service, it does not (and no need to) care about which backend pods
(and with what "pod IP") actually responds the service request. inside of the
cluster, that `service IP`, also called `cluster IP`, is a kind of virtual IP
(`VIP`). 

NOTE: in contrail environment it is implemented through floating IP.

This design model is very powerful and efficient in one sense that, it covers
the fragility of the possible single point failure that may be caused by
failure of any individual pod providing the service, therefore making a
`service` much more robust from client's perspective.

////
`pod` is the one doing the real work, and in kubernetes it is very "cheap" to
launch pods as needed. in chapter 3 you'll learned how fast it is to scale a rc
and deployment to control numbers of running pods dynamically. However, the
nature of a kubernetes pod is "mortal". to understand that just think of if a
screw of a chair breaks for whatever reason, you won't bother to "repair" it but
instead you just grab a new one.
////

in contrail kubernetes integration environment, all 3 types of services are
supported:

* clusterIP
* nodePort
* loadbalancer

next we'll introduce how service is implemented in contrail environment.

=== contrail service: ECMP loadbalancer

in chapter 3 we've introduced kubernetes default implementation of service
through `kube-proxy`. in there we mentioned CNI providers can have its own
implementations. in contrail, `nodePort` service is implemented by kube-proxy`.
however, `clusterIP` and `loadbalancer` services are implemented by contrail's
`loadbalancer` (`LB`).  

before we dive into the details of kubernetes service in contrail, it will
be good to review the legacy openstack based loadbalancer concept in contrail. 

TIP: for brevity we'll sometimes also refer `loadbalancer` as `LB`.

==== contrail openstack loadbalancer object

contrail loadbalancer is an relatively "old" feature that is supported since version 1.x.
it enables the creation of a pool of VMs serving applications, sharing one
virtual-ip (`VIP`) as the frontend IP towards clients. 
this diagram below illustrates contrail loadbalancer and its components.

.contrail openstack loadbalancer
image::https://user-images.githubusercontent.com/2038044/60641740-1f5c3700-9dfb-11e9-962f-ed67836d8115.png[]

some highlights of this figure:

* the LB is created with a internal VIP `30.1.1.1`. a `LB listener` is also created for each
  listening ports. 
* all backend VMs together compose a `pool` which is with subnet `30.1.1.0/24`,
  same as LB's internal VIP.
* each backend VM in the `pool`, also called a `member`, is allocated an IP from
  the pool subnet `30.1.1.0/24`.
* to expose the LB to external world, it is allocated another VIP which is
  external VIP `20.1.1.1`. 
* a client only sees one external VIP `20.1.1.1`, representing the whole service

.how it works:

* when LB sees a request coming from the client, it does TCP connection proxying. what that
  means is it establishes the TCP connection with the client, extracts the
  clients' HTTP/HTTPS requests, creates a new TCP connection towards one of the
  backend VMs from the pool, and send the request in the new TCP connection.
* when LB gets its response from the VM, it forwards the response to the client.
* when client closes the connection to the LB, the LB may also close its
  connection with the backend VM.

TIP: when client closes its connection to LB, LB may or may not close its
connection to backend VM. depending on the performance or other consideration it
may use a timeout before it tears down the session.

you see that this loadbalancer model is very similar to kubernetes service
concept:

* VIP is the "service IP" 
* backend VM becomes backend pods
* members are added by kubernetes instead of openstack

in fact, contrail re-uses a good part of this model in kubernetes service
implementation. to support service loadbalancing, contrail extends the
loadbalancer with a new driver, with it service will be implemented as "equal
cost multiple path"(ECMP) loadbalancer working in layer 4(transport layer) .
this is the primary difference comparing with the "proxy" mode that the openstack
loadbalancer type does.

*****
.some more implementation details:

* Actullay any loadbalancer can be integrated with contrail via contrail
  component `conrail-svc-monitor`. 
* Each loadbalancer has a loadbalancer driver that is registerd to
  contrail with a `loadbalancer_provider` type.
* `contrail-svc-monitor` listens to contrail `loadbalancer`, `listener`, `pool`
  and `member` objects, it also calls the registered loadbalancer driver to do
  other necessary jobs based on the `loadbalancer_provider` type. 
* contrail by default provides "ecmp loadbalancer" (`loadbalancer_provider` is
  `native`) and "haproxy loadbalancer" (`loadbalancer_provider` is `opencontrail`). 
* The openstack loadbalancer is using "haproxy loadbalancer".
* ingress, on the other hand, is conceptually even closer with the
openstack loadbalancer in the sense that both are layer 7 (application
layer) "proxy" based. more about ingress will be discussed in later section.

*****

==== contrail sevice loadbalancer object

let's take a look at service loadbalancer and the related objects.

.service loadbalancer
//image::https://user-images.githubusercontent.com/2038044/60640833-0f425880-9df7-11e9-91e1-9b0830394aaa.png[]
//image::https://user-images.githubusercontent.com/2038044/60677600-f87c2000-9e4f-11e9-8032-7cffd5f35da7.png[]
//TODO: redraw, add color
//image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]
image::https://user-images.githubusercontent.com/2038044/63821242-3a8a8500-c91a-11e9-81c9-3d93077b3e94.png[]


highlights in this figure:

* Each service is represented by a `loadbalancer` object. 
* the loadbalancer object comes with a `loadbalancer_provider` property. for
  service implementation a new `loadbalancer_provider` type called `native` is
  implemented.  
* for each sevice port a `listener` object is created for the same service `loadbalancer`
* for each `listener` there will be a `pool` object
* the `pool` contains `members`, depending on number of backend pod one pool may
  has multiple `members`
* each member object in the pool will map to one of pod backend

.this is how service works in contrail:

* `contrail-kube-manager` listens `kube-apiserver` for k8s service and when a
  `custerIP` or `loadbalancer` type of `service` is created, a `loadbalancer`
  object with `loadbalancer_provider` property `native` is created
* `loadbalancer` will have a "virtual IP" `VIP`, which is same as the `service
  IP` 
* The `service-ip`/`VIP` will be linked to each backend pod's interface. This is
  done by a ecmp loadbalancer driver.
* the linkage from service-ip to multiple backend pods interface creates an ECMP
  next-hop in contrail, traffic will be loadbalanced from the source pod towards
  one of the backend pod directly. later we'll show the ECMP prefix in the pod's
  VRF table
* `contrail-kube-manager` continues to listen to `kube-apiserver` for any changes,
  based on pod list in `Endpoints` it will knows the most current backend pods, and
  update members in the pool .

the most important thing to understand in this diagram, as we've mentioned, is
that in contrast to the legancy neutron loadbalancer (and the ingress
loadbalancer which we'll discussed later), there is no application layer "proxy"
in this process. contrail service implementation is based on layer 4 (transport
layer) ECMP based loadbalancing. 

////
detail discussions of the LB and all surrounding objects are out
of the scope of this book.


NOTE: technically, the LB has `VIP` only, but it also has a reference toward VMI
object which again has a reference to the `instance-ip`. the `instance-ip` is
the same IP as `service-ip`. to avoid confusions we won't cover these level of
implementation details in this book.
////

////
# k8s-5.md
Till 4.1, service ip is allocated from cluster-network even for isolated
namespaces. So, service from one isolated namespaces can reach service from
another isolated namespace. Security groups in isolated namespace prevents
reachability from other namespaces which also prevents reachablity from outside
of the cluster. In order to provide reachablity to external entity, the security
group would be changed to allow all which defeats the isolation. 

To address this, two virtual-networks would be created in the isolated
namespaces. One is for pods(pod-network) and another one is for
services(service-network). Contrail network-policy would be created between
pod-network and service-network for the reachablity between pods and services.
Service uses the same service-ipam which will be a flat-subnet like pod-ipam. It
is applicable for default namespace as well. Since virtual-networks are isolated
by default in contrail, services from one isolated namespace can not reach
service from another isolated namespace.
////

////

=== contrail clusterIP service

the `clusterIP` type of service is the most simple one. it is the default mode
if the `ServiceType` is not given. 

clusterIP service is exposed on a `clusterIP` and a service port. when client
pods need to access the service it sends request toward this clusterIP and
service port. service "binds" itself to certain backend pods via label mapping
between the two objects. `endpoint` is created for each service as long as there
is at least one matching pod available to be its backend. this model works great
if all requests are coming from the same cluster. the nature of the clusterIP
limits the scope of this service to be only within the same cluster. overall by
default the clusterIP is not reachable from external. 

////

==== navigating the service loadbalancer objects

we've talked a lot about the contrail "loadbalancer object" and you may wonder
what exactly it looks like. now we'll dig a little big deeper to look at the
loadbalancers and the supporting objects: listener, pool, members.

in contrail setup you can pull the object data either from contrail UI, CLI
(`curl`) or third party UI tools based on restapi. in production depending on
which one is available and handy you can select your favorite. 

.explore loadbalancer object with `curl`

with `curl` tool you
just need a FQDN of the URL pointing to the object. 

e.g.: to find the loadbalancer object URL for the service
`service-web-clusterip` from loadbalancers list:

----
$ curl http://10.85.188.19:8082/loadbalancers | \
    python -mjson.tool | grep -C4 `service-web-clusterip`
        {
            "fq_name": [
                "default-domain",
                "k8s-ns-user-1",
                "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
            ],
            "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
            "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        },
----

now with one specific loadbalancer URL, you can pull the specific LB object
details:

----
$ curl \
    http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc \
    | python -mjson.tool
{
    "loadbalancer": {
        "annotations": {
            "key_value_pair": [
                {
                    "key": "namespace",
                    "value": "ns-user-1"
                },
                {
                    "key": "cluster",
                    "value": "k8s"
                },
                {
                    "key": "kind",
                    "value": "Service"
                },
                {
                    "key": "project",
                    "value": "k8s-ns-user-1"
                },
                {
                    "key": "name",
                    "value": "service-web-clusterip"
                },
                {
                    "key": "owner",
                    "value": "k8s"
                }
            ]
        },
        "display_name": "ns-user-1__service-web-clusterip",
        "fq_name": [
            "default-domain",
            "k8s-ns-user-1",
            "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        ],
        "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "id_perms": {
            ...<snipped>...
        },
        "loadbalancer_listener_back_refs": [    #<---
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/loadbalancer-listener/3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc-TCP-8888-3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
                ],
                "uuid": "3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
            }
        ],
        "loadbalancer_properties": {
            "admin_state": true,
            "operating_status": "ONLINE",
            "provisioning_status": "ACTIVE",
            "status": null,
            "vip_address": "10.105.139.153",    #<---
            "vip_subnet_id": null
        },
        "loadbalancer_provider": "native",      #<---
        "name": "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "parent_href": "http://10.85.188.19:8082/project/86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "parent_type": "project",
        "parent_uuid": "86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "perms2": {
            ...<snipped>...
        },
        "service_appliance_set_refs": [
            ...<snipped>...
        ],
        "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "virtual_machine_interface_refs": [
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/virtual-machine-interface/8d64176c-9fc7-491a-a44d-430e187d6b52",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "k8s__Service__service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
                ],
                "uuid": "8d64176c-9fc7-491a-a44d-430e187d6b52"
            }
        ]
    }
}
----

the output is very extensive and includes a whole bunch of details that may not
be of our interests at this moment. but it does tell something interesting:

* in "loadbalancer_properties", the LB use service IP as its VIP
* the LB is connected to a listener by a reference
* `loadbalancer_provider` attribute is `native`, this is a new extension to
  implement layer 4 (transport layer)  ECMP for kubernetes service

.explore LB from UI

in the rest part of the exploration to LB and its related objects, we'll use the
legacy contrail UI.

TIP: you can also use the new contrail command UI to do the same.

for each service there is a LB object, in the below capture it shows 2 LB
objects:

* `ns-user-1-service-web-clusterip`
* `ns-user-1-service-web-clusterip-mp`

.loadbalancer object list
image::https://user-images.githubusercontent.com/2038044/60685179-a0edac80-9e6f-11e9-98c1-e2db001df543.png[]

this indicates 2 services were created. the service loadbalancer object's name
is composed by connecting NS name with service name, hence we can tell the
2 service's name:

* `service-web-clusterip` 
* `service-web-clusterip-mp`

==== loadbalancer

click on the small triangle icon in left of the first loadbalancer object
`ns-user-1-service-web-clusterip` to expand it, then click on `advanced json
view` icon on the right, you will see the similar detail information as what
you've seen in `curl` capture. for example the `VIP`, `loadbalancer_provider`,
`loadbalancer_listener` object that refers it, etc. 

from here you can keep expanding the `loadbalancer_listener` object by clicking
the `+` character to see the detail information of it. you then see a
`loadbalancer_pool`, expand it again you will see `member`. you can repeat this
process to explore through the object data. by the reference all
of these objects are connected to each other and work together.

.loadbalancer
image::https://user-images.githubusercontent.com/2038044/60685370-bca58280-9e70-11e9-8030-2746766082c8.png[]

==== listener

click on the LB name and select "listener", then expand it and display the
details with JSON format, you will get the listener details. the listener is
listening on service port 8888, and it is referenced by a `pool`.

TIP: in order to see the detail parameters of an object in JSON format, click
the triangle in the left of the loadbalancer name to expand it, then click on
the "Advanced JSON view" icon
image:https://user-images.githubusercontent.com/2038044/63659232-4b9e8f00-c77e-11e9-85d5-6a1b7a654f05.png[]
on the up right corner in the expanded view. We'll use the JSON view a lot in
this book to explore different contrail objects.

.listener
image::https://user-images.githubusercontent.com/2038044/60685556-b368e580-9e71-11e9-820f-47fb25aacee4.png[]

==== pool and member
just repeat the exploring process we will get down to the pool and two
`members` in it. the member is with a port of `80`, which maps to the container
targetPort in pod.

.pool
image::https://user-images.githubusercontent.com/2038044/60685626-15c1e600-9e72-11e9-8539-a24ea28b0bf3.png[]

.members
image::https://user-images.githubusercontent.com/2038044/60685682-6fc2ab80-9e72-11e9-804d-5eccd8e055df.png[]

next we'll examine the vrouter VRF table for the pod to show contrail service
loadbalancer ECMP operation details. in order to better understand the "1 to N"
mapping between loadbalancer and listener shown in the loadbalancer object
figure, we'll also give an example of a "multiple port service" in
our setup.  we'll conclude the ClusterIP service section by inspecting the
vrouter flow table to illustrate the service packet workflow.

=== service testbed

before starting our investigation, let's look at our testbed. in this book we
build a setup including the following devices, most of our case studies are
based on it:

* one cenos server running as k8s `master` and contrail controllers
* two cenos servers, each running as a k8s `node` and contrail vrouter
* one Juniper QFX switch running as the underlay "leaf"
* one Juniper MX router running as a gateway router, or a "spine"
* one centos server runs as an Internet host machine

the digaram is here:

//image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]
image::https://user-images.githubusercontent.com/2038044/63596670-bcc92100-c589-11e9-99f1-7340a24cc8fd.png[]


NOTE: To minimize the resource utilization, all "servers" are actually centos
virtual machines created by vmware ESXI hypervisor running in one physical HP
server. this is also the same testbed for ingress.

in appendix you will find all details about the setup. the prerequisites,
software/hardware specifications, sample configuration files, and installation
steps. following the steps you will be able to build a same setup in your lab.

=== contrail ClusterIP service

in chapter 3 we've demonstrated how to create and verify a clusterIP service. in
this section we'll revisit the lab and look at some important details about
contrail specific implementations. we'll continue and add a few more tests to
illustrate the contrail service loadbalancer implementation details.

==== ClusterIP as FIP

this is the yaml file we used to create a `clusterIP` service:

----
$ cat service-web-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
----

let's review what we got from service lab in chapter3:

----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE  SELECTOR
service-web-clusterip  ClusterIP  10.105.139.153  <none>       8888/TCP  45m  app=webserver
----

----
$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ...  IP             NODE     ...  LABELS
cirros                      1/1    Running  ...  10.47.255.237  cent222  ...  app=cirros
webserver-846c9ccb8b-g27kg  1/1    Running  ...  10.47.255.238  cent333  ...  app=webserver
----

////
----
$ kubectl get pod -o wide --show-labels
NAME                              READY STATUS   ... IP             NODE     ... LABELS
cirros                            1/1   Running  ... 10.47.255.237  cent222  ... app=cirros
webserver-846c9ccb8b-kvwvw 1/1   Running  ... 10.47.255.238  cent333  ... app=webserver
----
////

here we see one service is created, with one pod running as its backend. the
label in the pod matches to the SELECTOR in service. the pod name also indicates
this is a deploy-generated pod. later we can scale the deploy for ECMP case
study, for now we'll stick to one pod and examine the ClusterIP implementation
details.

in contrail, a `ClusterIP` is essentially implemented in the form of a FIP.
once a service is created, a FIP will be allocated from the service subnet
and associated to all the backend pod VMI to form the ECMP loadbalancing.
Now all backend pods can be reached via cluserIP(along with the POD IP). 
This clusterIP(FIP) is acting as a "VIP" to the client pods inside of the
cluster.

TIP: Why contrail chose FIP to implement clusterIP? In the previous section, we
have learned that contrail does NAT for FIP and service also needs NAT. So it
is natural to use the FIP for clusterIP. 

For loadbalancer type of service, contrail will allocate a second FIP -
the "EXTERNAL-IP" as the VIP, and the external VIP is advertised outside of
the cluster through gateway router. you will get more details about these later.

from UI we'll see the automatically allocated FIP as ClusterIP.

.ClusterIP as FIP
image::https://user-images.githubusercontent.com/2038044/60973473-57c9ac80-a2f6-11e9-81a7-df74349e9877.png[]

the FIP is also associated with the pod VMI and podIP, in this case the VMI is
representing the pod interface.

.pod interface
image::https://user-images.githubusercontent.com/2038044/60975990-df191f00-a2fa-11e9-9f81-e635c141c7e6.png[]

the interface can be expanded to display more details:

.pod interface detail
//image::https://user-images.githubusercontent.com/2038044/63632000-c6d83780-c5fc-11e9-92a6-6bed7f09a944.png[]
image::https://user-images.githubusercontent.com/2038044/63632051-87f6b180-c5fd-11e9-8695-9ec6fc7c88ca.png[]

expand the `fip_list`, we'll see more information below:

----
fip_list:  {
    list:  {
        FloatingIpSandeshList:  {
            ip_addr: 10.105.139.153
            vrf_name: default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
            installed: Y
            fixed_ip: 10.47.255.238
            direction: ingress
            port_map_enabled: true
            port_map:  {
                list:  {
                    SandeshPortMapping:  {
                    protocol: 6
                    port: 80
                    nat_port: 8888
                    }
                }
            }
        }
    }
}
----

service/clusterIP/FIP 10.105.139.153 maps to podIP/fixed_ip 10.47.255.238.  the
`port_map` tells that port `8888` is a `nat_port`, `6` is the protocol number so
it means protocol TCP. overall, clusterIP:port `10.105.139.153:8888` will be
translated to podIP:targetPort `10.47.255.238:80` and vice versa.

now you understand with FIP representing ClusterIP, NAT will happen in service.
later we'll examine NAT again in the flow table.

==== scale backend pods
in chapter 3 clusterIP service example, we have created a sevice and a backend
pod. to verify the ECMP, let's increase the replica to 2 to generate a second
backend pod. this is a more realistic and rebost model: each pod will now be
backing up each other to avoid a single point failure.

instead of using yaml file to manually create a new webserver pod, with the
"kubernetes spirit" in mind you should think of to `scale` a Deployment,
as what you`ve seen earlier in this book. in our service example we`ve been
using `Deployment` object to spawn our webserver pod on purpose:

----
$ kubectl scale deployment webserver --replicas=2
deployment.extensions/webserver scaled

$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ... IP             NODE     ... LABELS
cirros                      1/1    Running  ... 10.47.255.237  cent222  ... app=cirros
webserver-846c9ccb8b-7btnj  1/1    Running  ... 10.47.255.236  cent222  ... app=webserver
webserver-846c9ccb8b-g27kg  1/1    Running  ... 10.47.255.238  cent333  ... app=webserver

$ kubectl get svc -o wide
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service-web-clusterip   ClusterIP   10.105.139.153   <none>        8888/TCP   45m   app=webserver
----

immediately after you create a new webserver pod by scaling the deployment with
`replicas 2`, a new pod is launched.  we end up having 2 backend pods now, one
is running in same node `cent222` as the client cirros pod, or a "local" node
for cirros pod; the other one is running in the other node `cent333` - the
"remote" node from client pod's perspective.  and the `endpoint` objects get
updated to reflect the current set of backend pods behind the `service`.

----
$ kubectl get ep -o wide
NAME             ENDPOINTS                           AGE
service-web-lb   10.47.255.236:80,10.47.255.238:80   20m
----

NOTE: without `-o wide` option, only first endpoint will be displayed properly.

we go ahead and check the FIP again.

.ClusterIP as FIP (ECMP)
image::https://user-images.githubusercontent.com/2038044/60973157-b2163d80-a2f5-11e9-957a-438642355391.png[]

we see the same FIP, but now it is associated with two podIP, each representing
a seperate pod. 

==== ECMP routing table: control node perspective

first, to examine the ECMP, let's take a look at the routing table in the
controller's routing instance.

.control node routing instance table
image::https://user-images.githubusercontent.com/2038044/60966312-ee41a200-a2e5-11e9-8966-053f0bbc20ea.png[]

the routing instance (RI) has a full name with the following format:

    <DOMAIN>:<PROJECT>:<VN>:<RI>

in most cases RI inheritate the same name from it's VN, so in our case the
full IPv4 routing table has this name:
`default-domain:k8s-ns-user-1:k8s-ns-user-1-pod-network:k8s-ns-user-1-pod-network.inet.0`
the `.inet.0` indicate the routing table type is unicast IPv4. there are many
other tables which is not of our interests right now.

two routing entries with the same exact prefixes of the ClusterIP show up in the
routing table, with two different next hops, each pointing to a different node.
this gives a hint about the route propagation process: both nodes(compute) has
advertised the same clusterIP toward the master(contrail controller), to
indicate the presence of the running backend pods in itself. this route
propagation is via XMPP. master(contrail controller) then reflect the routes to
all other compute nodes.

==== ECMP routing table: compute node perspective

next, starting from the client pod node `cent222`, we'll look at the the pod's
VRF table to understand how the packets are forwarded towards the backend pods

.vrouter vrf table
image::https://user-images.githubusercontent.com/2038044/60680116-18174680-9e58-11e9-9235-48c152959df7.png[]

the most important part of the screenshot is the routing entry `Prefix:
10.105.139.153 / 32 (1 Route)`, it is our ClusterIP address. underneath the
prefix there is a statement `ECMP Composite sub nh count: 2`. this indicates the
prefix has multiple possible next hop to reach. now expand it by clicking the
small triangle icon in the left, you will be given a lot more details about this
prefix.

.vrouter ECMP nexthop
image::https://user-images.githubusercontent.com/2038044/60680345-ece12700-9e58-11e9-9793-2b609918e146.png[]

among all of the details in this outputs, the most important thing that is of
our focus is `nh_index: 87`, which is the next hop ID (`NHID`) for the clusterIP
prefix. from vrouter agent docker, we can further resolve the "Composite" NHID to 
the `sub-NHs`, which is the "member" nexthops under the "Composite" next hop:

TIP: don't forget to execute the vrouter commands from the vrouter docker.
doing it from the host directly may not work.

////
----
[2019-07-04 12:42:06]root@cent222:~
$ docker exec -it vrouter_vrouter-agent_1 nh --get 87
Id:87         Type:Composite      Fmly: AF_INET  Rid:0  Ref_cnt:2          Vrf:2
              Flags:Valid, Policy, Ecmp, Etree Root,
              Valid Hash Key Parameters: Proto,SrcIP,SrcPort,DstIp,DstPort
              Sub NH(label): 51(25) 37(59)              #<---

Id:51         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:18         Vrf:0
              Flags:Valid, MPLSoUDP, Etree Root,        #<---
              Oif:0 Len:14 Data:00 50 56 9e e6 66 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:10.169.25.21

Id:37         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:5          Vrf:2
              Flags:Valid, Etree Root,
              EncapFmly:0806 Oif:8 Len:14               #<---
              Encap Data: 02 30 51 c0 fc 9e 00 00 5e 00 01 00 08 00
----

some important information to highlight from this capture:

* NHID 87 is an "ECMP composite nexthop"
* the ECMP nexthop contains 2 "sub" nexthops: nexthop 51 and nexthop 37, each
  representing a seperate path towards the backend pods
* nexthop 51 represents a "MPLSoUDP" tunnel toward backend pod in the remote
  node, the tunnel is established from current node `cent222`, with source IP
  being local fabric IP `10.169.25.20`, to the other node `cent333` whose fabric
  IP is `10.169.25.21`. if you recall where our two backend pods are located,
  this is the forwarding path between the 2 nodes.
* nexthop 37 represents a "local" path, towards vif 0/8 (`Oif:8`), which is the
  local backend pod's interface. 

////

----
[2019-07-04 12:42:06]root@cent222:~
$ docker exec -it vrouter_vrouter-agent_1 nh --get 87
Id:87         Type:Composite      Fmly: AF_INET  Rid:0  Ref_cnt:2          Vrf:2
              Flags:Valid, Policy, Ecmp, Etree Root,
              Valid Hash Key Parameters: Proto,SrcIP,SrcPort,DstIp,DstPort
              Sub NH(label): 51(43) 37(28)              #<---

Id:43         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:18         Vrf:0
              Flags:Valid, MPLSoUDP, Etree Root,        #<---
              Oif:0 Len:14 Data:00 50 56 9e e6 66 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:10.169.25.21

Id:28         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:5          Vrf:2
              Flags:Valid, Etree Root,
              EncapFmly:0806 Oif:8 Len:14               #<---
              Encap Data: 02 30 51 c0 fc 9e 00 00 5e 00 01 00 08 00
----

some important information to highlight from this capture:

* NHID 87 is an "ECMP composite nexthop"
* the ECMP nexthop contains 2 "sub" nexthops: nexthop 43 and nexthop 28, each
  representing a seperate path towards the backend pods
* nexthop 43 represents a "MPLSoUDP" tunnel toward backend pod in the remote
  node, the tunnel is established from current node `cent222`, with source IP
  being local fabric IP `10.169.25.20`, to the other node `cent333` whose fabric
  IP is `10.169.25.21`. if you recall where our two backend pods are located,
  this is the forwarding path between the 2 nodes.
* nexthop 28 represents a "local" path, towards vif 0/8 (`Oif:8`), which is the
  local backend pod's interface. 

to resolve the vrouter `vif` interface,  use `vif --get 8` command:

----
$ vif --get 8
Vrouter Interface Table
......
vif0/8      OS: tapeth0-304431
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.236  #<---
            Vrf:2 Mcast Vrf:2 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:455  bytes:19110 errors:0
            TX packets:710  bytes:29820 errors:0
            Drops:455
----

the output displays the corresponding local pod interface's name, IP, etc.

==== clusterIP service ECMP workflow

the clusterIP service's loadbalancer ECMP workflow is illustrated in this
figure:

.contrail service loadbalancer ECMP forwarding
//image::https://user-images.githubusercontent.com/2038044/60762382-97f60a00-a02c-11e9-81ad-b1f05d815571.png[]
//image::https://user-images.githubusercontent.com/2038044/60762413-1ce12380-a02d-11e9-8cec-41d5e177bfb9.png[]
//image::https://user-images.githubusercontent.com/2038044/63705462-3287f380-c7fb-11e9-9055-a9f3002708b2.png[]
image::https://user-images.githubusercontent.com/2038044/63705594-885c9b80-c7fb-11e9-897b-ee55e0d7a70f.png[]


this is what happened in the forwarding plane:

* a client pod `cirros` located in node `cent222` needs to access a service
  `service-web-clusterip`, it sends a packet towards the service's clusterIP
  `10.105.139.153` and port `8888`
* `cirros` sends the packet to `node222` vrouter based on the default route.
* vrouter on `node222` got the packet, it checks its corresponding VRF table, get a
  "Composite" nexthop ID `87`, which resolves to two sub-nexthops `51` and `37`,
  representing a remote and local backend pod respectively. this indicates ECMP.
* vrouter on `node222` starts to forward the packet to one of the pod based on
  its ECMP algorithm. Suppose the remote backend pod is selected, the packet will
  be sent through MPLSoUDP tunnel to the remote pod on node `cent333`, after
  establishing the flow in the flow table. all subsequent packets belongs to the
  same flow will follow this same path. same applys to the local path towards local
  backend pod.

//TODO: this is forwarding flow only, also give control plane flow?

==== multiple port service

we've understood how the service layber 4 ECMP works and we've explored the LB
objects in lab. remember in the figure showing the LB and relevant objects,
we saw that one LB may having 2 or more LB listeners. each listener has an
individual backend pool that has one or multiple member(s). 

.service loadbalancer
image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]

in kubernetes, this 1:N mapping between loadbalancer and listeners indicates
a `multiple port service` - one service with multiple ports.
let's look at the yaml file of it:

//.multiple port service
//====
.svc/service-web-clusterip-mp.yaml
----
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip-mp
spec:
  ports:
  - name: port1
    port: 8888
    targetPort: 80
  - name: port2         #<---
    port: 9999
    targetPort: 90
  selector:
    app: webserver
----
//====

what we've added is another item in the `ports` list: a new service port `9999`
that maps to container's `targetPort` `90`. now with two port mappings we have
to give each port a name, `port1` and `port2` respectively.

NOTE: without a port `name` the multiple ports yaml file won't work.

now we apply the yaml file and a new service `service-web-clusterip-mp` with 2
ports is created:

----
$ kubectl apply -f svc/service-web-clusterip-mp.yaml
service/service-web-clusterip-mp created

$ kubectl get svc
NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)            AGE
service-web-clusterip     ClusterIP  10.105.139.153  <none>       8888/TCP           3h8m
service-web-clusterip-mp  ClusterIP  10.101.102.27   <none>       8888/TCP,9999/TCP  4s

$ kubectl get ep
NAME                       ENDPOINTS                           AGE
service-web-clusterip      10.47.255.238:80                    4h18m
service-web-clusterip-mp   10.47.255.238:80,10.47.255.238:90   69m
----

NOTE: to simply the case study we've scaled down the backend deployment's
replicas number to one.

it looks everything is ok, isn't it? the new service comes up with 2 service
ports exposed, `8888` is the old one we've tested in previous examples, and the
new `9999` port should work equally well.

turns out that is not the case.

service port 8888 works:

----
$ kubectl exec -it cirros -- curl 10.101.102.27:8888 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.238
                         Hostname = webserver-846c9ccb8b-g27kg
                                    [giphy]
----

service port 9999 doesn't work:

----
$ kubectl exec -it cirros -- curl 10.101.102.27:9999 | w3m -T text/html | cat
command terminated with exit code 7
curl: (7) Failed to connect to 10.101.102.27 port 9999: Connection refused
----

the request towards port 9999 is rejected. reason is the `targetPort` is not
running in pod container, so there is no way you will get a response from it.

----
$ kubectl exec -it webserver-846c9ccb8b-g27kg -- netstat -lnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/python
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   PID/Program name    Path
----

`readinessProbe` introduced in chater 3 is the official kubernetes tool to
detect this situation, so in case the pod is not "ready", it will be restarted
and you will catch the events.

to resolve this let's start a new server in pod to listen on the new port `90`
also.  one of the easiest way today to start a HTTP server is to use the
`SimpleHTTPServer` module coming with `python` package. in our test we only need
to set its listening port to `90` (the default value is 8080).

----
$ kubectl exec -it webserver-846c9ccb8b-g27kg -- python -m SimpleHTTPServer 90 
Serving HTTP on 0.0.0.0 port 90 ...                                    
----

now the `targetPort` is on, we can start the request towards service port `9999`
again from the `cirros` pod. this time it succeed and get the returned webpage
from python SimpleHTTPServer.

----
$ kubectl exec -it cirros -- curl 10.103.87.232:9999 | w3m -T text/html | cat

Directory listing for /
 ━━━━━━━━━━━━━━━━━━━━━
  • app.py
  • Dockerfile
  • file.txt
  • requirements.txt
  • static/
 ━━━━━━━━━━━━━━━━━━━━━
----

for each incoming request the `SimpleHTTPServer` logs one line output, with an
IP address showing where the request came from. in our case the request coming
from cirros client pod is with the IP `10.47.255.237`.

----
10.47.255.237 - - [04/Jul/2019 23:49:44] "GET / HTTP/1.1" 200 -
----

==== the flow table

so far we've tested clusterIP service, and we see client request is sent towards
the service IP. in contrail environment `vrouter` is the module that does all of
the packet forwarding job. when the `vrouter` in client pod gets the packet, it
looks up the corresponding VRF table in vrouter module for the client pod(`cirros`),
gets the nexthop and resolves the correct egress interface and proper encapsulation.
in our test so far, the client and backend pods are in 2 different nodes, the source
`vrouter` decides the packets need to be sent in MPLSoUDP tunnel, towards the node
where backend pod is running. what interests us the most is:

* how the service IP and backend podIP is translated to each other? 
* is there a way to "capture and see" the two IPs in a flow, "before" and
  "after" the translations for comparison purpose?

the most "straightforward" method you would think of is to capture the packets,
then decode and see. doing that however, may not be as easy as what you've expected.

. first you need to capture the packet at different places:

    * at the pod interface, this is after the address is translated, that part is
      easy
    * the fabric interface, this is before packet is translated and reaches the pod
      interface. here the packets are with MPLSoUDP encapsulation since data plane
      packets are "tunneled" between nodes.

. then you need to copy the pcap file out and load with wireshark to decode. you
probably also need to configure wireshark to recognize the MPLSoUDP encapsulation.

the easier way is to check the vrouter flow table which records IP and port
details about a traffic flow. in this test we will prepare a big file `file.txt`
in backend webserver pod and try to download it from the client pod. 

[TIP]
====
you may wonder to trigger a flow why we don't simply use same curl test to pull
the webpage, as what we've done in early test. in theory that is fine.  the only
problem is that the TCP flow follows the TCP session. in our previous test with
`curl`, the TCP session starts and stops immediately after the webpage is
retrieved, then the vrouter clears the flow right away. you won't be fast enough
to capture the flow table at the right moment. instead, downloading a big file
will hold the TCP session - as long as the file transfer is ongoing the session
will remain, and we can take time to investigate the flow. later on in `ingress`
section we will demonstrate a different method with a one-liner shell script.  

====

now in the cirros pod curl URL, instead of just give root path `/` to list the
files in folder, we try to pull the file: `file.txt`

----
$ kubectl exec -it cirros -- curl 10.103.87.232:9999/file.txt
----

in server pod we see the log indicating the file downloading starts:

----
10.47.255.237 - - [05/Jul/2019 00:41:21] "GET /file.txt HTTP/1.1" 200 -
----

now with the file transfer ongoing, we have enough time to collect the flow table
from both client and server node, in the vrouter docker.

.client node flow table

----
(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1361 Added 1361 Deleted 442 Changed 443Processed 1361 Used Overflow entries 0
(Created Flows/CPU: 305 342 371 343)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    40100<=>340544       10.47.255.237:42332                                 6 (3)
                         10.103.87.232:9999
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):59,  Stats:7878/520046,
 SPort 65053, TTL 0, Sinfo 6.0.0.0)

   340544<=>40100        10.103.87.232:9999                                  6 (3)
                         10.47.255.237:42332
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):68,  Stats:142894/205180194,
 SPort 63010, TTL 0, Sinfo 10.169.25.21)
----

highlights in this output:

* cirros client starts TCP connection from its pod IP `10.47.255.237` and a
  rondom source port, towards the service IP `10.103.87.232` and server port
  `9999`
* the flow TCP flag `SSrEEr` indicates the session is established
  bidirectionally.
* Action `F` means "forwarding". note that there is no special processing like
  `NAT` happening here. 

NOTE: with a filter `--match 15.15.15.2`. only flow entries with Internet Host
IP is printed.

we can conclude, from client's perspective, it only see the service IP. it is
not aware of any backend pod IP at all.

.server node flow table

now look at flow table in server node vrouter docker:

----
(vrouter-agent)[root@cent333 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1116 Added 1116 Deleted 422 Changed 422Processed 1116 Used Overflow entries 0
(Created Flows/CPU: 377 319 76 344)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   238980<=>424192       10.47.255.238:90                                    6 (2->3)
                         10.47.255.237:42332
(Gen: 1, K(nh):24, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):24,
 Stats:8448/202185290,  SPort 62581, TTL 0, Sinfo 3.0.0.0)

   424192<=>238980       10.47.255.237:42332                                 6 (2->2)
                         10.103.87.232:9999
(Gen: 1, K(nh):24, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:8067/419582,  SPort 51018, TTL 0, Sinfo 10.169.25.20)
----

let's look at the second flow entry first - the IPs looks same as the one we
just saw in client side capture.  traffic lands vrouter fabric interface from
remote cirros client node, across MPLSoUDP tunnel. destination IP and port are
service IP and service port respectively. it seems nothing special here.

however, the flow `Action` now is set to `N(DPd)`, not `F`. according to the
header lines in the `flow` command output, this means NAT, or specifically,
`DNAT` (Destination address translation) with `DPAT` (Destination port
translation) - both the service IP and service port are translated, to
backend pod IP and port.

now look at the first flow entry. source IP `10.47.255.238` is the backend pod
IP and source port is python server port `90` opened in backend container .
obviously this is the returning traffic indicating the file downloading is still
ongoing. the `Action` is also NAT(`N`), but this time it is the reverse
operation - source NAT (`SNAT`) and source PAT(`SPAT`). vrouter will
translate backend's source IP source port to the service IP and port, before
putting it into the MPLSoUDP tunnel and returning back to client pod in remote
node.

the complete end to end traffic flow is illustrated here:

.clusterIP service traffic flow (NAT)
//image::https://user-images.githubusercontent.com/2038044/60388198-f7c44200-9a7b-11e9-9b08-f34167b0a2b8.png[]
//image::https://user-images.githubusercontent.com/2038044/60762300-96c3dd80-a02a-11e9-8933-452d3ee074a4.png[]
image::https://user-images.githubusercontent.com/2038044/60763424-32147d00-a042-11e9-813a-a6aa3989c09d.png[]

//TODO: redraw with color

=== contrail LoadBalancer service

in chapter 3 we've briefly talked about LoadBalancer service. in there we
mentioned if the goal is to expose the service to the external world outside of
the cluster, we just specify `ServiceType` as `LoadBalancer` in the service yaml
file. 

whenever a service of `type: LoadBalancer` get created, in contrail environment
what will happen is , not only a `clusterIP` will be allocated and exposed to
other pods within the cluster, but also a `floating ip` from public fip pool
will be assigned to the loadbalancer instance as an "external IP" and exposed to
the public world outside of the cluster. 

while the `clusterIP` is still acting as a `VIP` to the client **inside** of the
cluster, the `floating ip` or `external IP` will essentially act as a `VIP`
facing those client sitting **outside** of the cluster, for example, a remote
Internet host which sends request to the service across the gateway router. 

in this section we'll demonstrate how does the `LoadBalancer` type of service
works in our end to end lab setup, which includes the kubernetes cluster, fabric
switch, gateway router, and Internet host.

==== external IP as FIP

//create `Loadbalancer` service

let's look at the yaml file of a `LoadBalancer` service. it is same as ClusterIP
service except just one more line declaring the service `type`:

----
$ cat service-web-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer    #<---
----

create and verify the service:

----
$ kubectl apply -f service-web-lb.yaml
service/service-web-lb created

$ kubectl get svc -o wide
NAME            TYPE          CLUSTER-IP   EXTERNAL-IP      PORT(S)         AGE    SELECTOR
service-web-lb  LoadBalancer  10.96.89.48  101.101.101.252  8888:32653/TCP  10s    app=webserver
----

comparing with the `clusterIP` service type, this time in the "EXTERNAL-IP"
column there is an IP allocated. if you remember what we've covered in the
"floating IP pool" section, you should understand this "EXTERNAL-IP" is actually
another `FIP`, allocated from the `NS FIP pool` or `global FIP pool` - we did
not give any specific FIP pool information in the service object yaml file, so
based on the algorithm right FIP pool will be used automatically. 

from UI we'll see that for `loadbalancer` service we now have 2 FIPs: one as
clusterIP (internal VIP), the other one as "EXTERNAL-IP" (external VIP):

.2 FIPs for a `loadbalancer` service
image::https://user-images.githubusercontent.com/2038044/63900303-1c7e5c80-c9ce-11e9-85dd-d4282aaa6c46.png[]

both FIPs are associated with the pod interface:

.pod interface
image::https://user-images.githubusercontent.com/2038044/63900502-b6dea000-c9ce-11e9-8537-b44cffc90055.png[]

expend the tap interface, you will see the two FIPs are listed in `fip_list`:

.pod interface detail
image::https://user-images.githubusercontent.com/2038044/63900411-7e3ec680-c9ce-11e9-876f-2deeff06bdef.png[]

////
the `fip_list`, we'll see each FIPs in the list:
----
 fip_list
     FloatingIpSandeshList
       ip_addr: 10.96.89.48
       vrf_name: default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
       installed: Y
       fixed_ip: 10.47.255.238
       direction: ingress
       port_map_enabled: true
       port_map
           SandeshPortMapping
             protocol: 6
             port: 80
             nat_port: 8888
     FloatingIpSandeshList
       ip_addr: 101.101.101.252
       vrf_name: default-domain:k8s-ns-user-1:k8s-vn-ns-default-pod-network:k8s-vn-ns-default-pod-network
       installed: Y
       fixed_ip: 10.47.255.238
       direction: ingress
       port_map_enabled: true
       port_map
           SandeshPortMapping
             protocol: 6
             port: 80
             nat_port: 8888
----
////

now you should understand, the only difference here between the two type of
services, is that for loadbalancer service, an extra FIP is allocated from the
public FIP pool, which is advertised to the gateway router and acts as the
outside-facing VIP. that is how the `loadbalancer` service expose itself to the
external world.

==== examine VRF table in gateway router

in "contrail floating IP" section you've learned how to advertise FIP. here
we'll review the main concepts to understand how it works in contrail `service`
implementation. 

the `route-target` community setting in the FIP VN makes it reachable by the
Internet host, so effectively our service is now also exposed to the Internet
,instead of only to pods inside of the cluster. Examining the gateway router's
VRF table reveals this:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24
Jun 19 03:56:11

k8s-test.inet.0: 23 destinations, 40 routes (23 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:01:11, MED 100, localpref 200, from 10.169.25.19
                      AS path: ?, validation-state: unverified
                    > via gr-2/2/0.32771, Push 40
----

the FIP host route is learned by gateway router, from contrail controller - more
specifically, contrail control node, which acts as a standard MP-BGP VPN `RR`
reflecting routes between compute nodes and the gateway router. A further look
at the detail version of the same route displays more information about this
process:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24 detail
Jun 20 11:45:42

k8s-test.inet.0: 23 destinations, 41 routes (23 active, 0 holddown, 0 hidden)
101.101.101.252/32 (2 entries, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:9
                ......
                Source: 10.169.25.19                    #<---
                Next hop type: Router, Next hop index: 1266
                Next hop: via gr-2/2/0.32771, selected  #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                Indirect next hop: 0x900c660 1048574 INH Session ID: 0x690
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 10:15:38   Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                    ......
                Import Accepted
                VPN Label: 44
                Localpref: 200
                Router ID: 10.169.25.19
                Primary Routing Table bgp.l3vpn.0
----
////
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1020006
                    unknown type 8084 value eac4:1030001
////

* the `source` indicates from which BGP peer the route is learned,
  `10.169.25.19` is the contrail controller (and kubernetes master) in our lab
* `protocol next hop` tells who generates the route. `10.169.25.20` is node
  `cent222` where the backend webserver pod is running
* `gr-2/2/0.32771` is an interface representing the (MPLS over) GRE tunnel
  between the gateway router and node `cent222`.

==== `Loadbalancer` service workflow

to summarize, the FIP given to the service as its external ip is advertised to
gateway router, and get loaded in the router's VRF table. when Internet host
sends a request to the FIP, through MPLSoGRE tunnel the gateway router will
forward it to the compute node where backend pod is locating.

the packet flow is illustrated in this figure:

.`loadbalancer` service workflow
//image::https://user-images.githubusercontent.com/2038044/60563159-a7254100-9d28-11e9-94ca-934b8f870b1e.png[]
image::https://user-images.githubusercontent.com/2038044/63638336-e8fda400-c654-11e9-8938-d98901633b6b.png[]

here is the full story:

* you create a `FIP pool` from a public VN, with route-target the VN is
  advertised to the remote gateway router via MP-BGP 
* you create a pod with a label `app: webserver`, kubernetes decides the pod
  will be created in node `cent222`. via XMPP the node publish the pod IP
* you create a loadbalancer type of service with `service port` and label
  selector `app=webserver`.  kubernetes allocates a service IP.
* kubernetes finds the pod with the matching label and update the `endpoint`
  with the pod IP and port information. 
* contrail create a loadbalancer instance and assign a FIP to it. contrail also
  associate that FIP with the pod interface, so there will be one to one NAT
  operation between the FIP and podIP.
* via XMPP, node `cent222` advertises this FIP to contrail controller `cent111`,
  which then advertises it to the gateway router.
* on receiving the FIP prefix, gateway router checks and see a the RT of the
  prefix matches to what it is expecting, it will import the prefix in local
  VRF. at this moment the gateway learns the nexthop of the FIP is `cent222`, so
  it generate a soft GRE tunnel toward `cent222`.
* when gateway router see a request coming from Internet toward the FIP, through
  the MPLS over GRE tunnel it will send the request to the node `cent222`
* vrouter in the node sees the packets destined to the FIP, it will perform NAT
  so the packets will be sent to the right backend pod.

==== verify the `Loadbalancer` service

To verify the end to end service access from Internet host to the backend pod, 
we will login to the Internet host desktop and launch a browser, with URL
pointing to `http://101.101.101.252:8888`. 

TIP: just to keep in mind that the internet host request has to be sent to the
public **FIP**, not to the **service IP**(**clusterIP**) or backend **podIP**
which are only reachable from inside of the cluster!

this is the returned web page:

image::https://user-images.githubusercontent.com/2038044/60388669-ea5e8600-9a82-11e9-87b9-30a98572f7bb.png[]

TIP:
in our testbed we installed a centos desktop as an Internet host. 
////
in our testbed we installed a centos server as an Internet host. as with any
linux distribution, if you need to login the "GUI", you need to install Xwindow
or linux desktop applications and set it up properly. also you need a web
browser if it does not come with the server.
////

To simplify the test, you can also ssh into the Internet host and test it with
`curl` tool:

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | w3m -T text/html | cat
             Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = webserver-846c9ccb8b-vl6zs
   [giphy.gif]
----

the kubernetes service is available from Internet!

==== `Loadbalancer` service ECMP

so far you've seen how loadbalancer type of service is exposed to the Internet
and how the FIP did the "trick". in ClusterIP service section, you've also seen
how the service loadbalancer ECMP works. what you haven't seen yet is how does
the "ECMP" processing works under loadbalancer type of service. To demonstrate
this again we scale the RC to generate one more backend pod behind the
`service`. 

----
$ kubectl scale rc rc-webserver --replicas=2
replicationcontroller/rc-webserver scaled

$ kubectl get pod -l app=webserver -o wide
NAME                READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
webserver-846c9ccb8b-r9zdt  1/1    Running  0         25m  10.47.255.238  cent333  <none>
webserver-846c9ccb8b-xkjpw  1/1    Running  0         23s  10.47.255.236  cent222  <none>
----

here is the question: with 2 pods on different node as backend now, from the
gatway router's perspective when it get the service request, which node it will
choose to forward the traffic to? let`s check the gateway router`s VRF table
again:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32
Jun 30 00:27:03

k8s-test.inet.0: 24 destinations, 46 routes (24 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 26
                    [BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 26
----

the same FIP prefix is imported as we've seen in previous example, except that
now the same route is learned twice and an additional MPLSoGRE tunnel is
created. previously in ClusterIP service example we use `detail` option in `show
route` command to find the tunnel endpoints, this time we examine the soft GRE
`gr-` interface to find the same:

----
labroot@camaro> show interfaces gr-2/2/0.32771
Jun 30 00:56:01
  Logical interface gr-2/2/0.32771 (Index 392) (SNMP ifIndex 1801)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.21:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None

labroot@camaro> show interfaces gr-2/3/0.32771
  Logical interface gr-2/3/0.32771 (Index 393) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 11
    Output packets: 11
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` of `gr-` interface indicates the two end points of a GRE tunnel:

* `10.169.25.20:192.168.0.204`: tunnel between node `cent222` and gateway router
* `10.169.25.21:192.168.0.204`: tunnel between node `cent333` and gateway router

We end up to have 2 tunnels in the gateway router, each pointing to a different
node where a backend pod is running. now we believe the router will perform
ECMP load balancing between the two GRE tunnel, whenever it got service request
toward the same FIP. let's check it out.

==== verify the `Loadbalancer` service ECMP

to verify the ECMP we'll just pull the webpage a few more time and we expect to
see both podIP displayed eventually.

turns out this never happens!

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = webserver-846c9ccb8b-xkjpw
----

the only webpage we got is from the first backend pod `10.47.255.236`,
`webserver-846c9ccb8b-xkjpw`, running in node `cent222`. the other one never show up.
so the expected ECMP does not happen yet. when we examine the route again with
`detail` or `extensive` keyword we find the root cause:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32 detail | match state
Jun 30 00:48:29
                State: <Secondary Active Int Ext ProtectionCand>
                Validation State: unverified
                State: <Secondary NotBest Int Ext ProtectionCand>
                Validation State: unverified
----

from that we realize that, even if the router learned the same prefix from both
node, only one is `Active` and the other one won't take effect because it is
`NotBest`. therefore, the second route and the corresponding GRE interface
`gr-2/2/0.32771` will never get loaded into the forwarding table:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 00:53:12
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination         Type  RtRef  Next  hop      Type  Index  NhRef  Netif
101.101.101.252/32  user  0      indr  1048597  2
                                Push 26     1272     2 gr-2/3/0.32771
----

this is the default Junos BGP path selection behavior and detail discussion of
it is out of the scope of this book. 

NOTE: for Junos BGP path selection algorithm, check this link:
https://www.juniper.net/documentation/en_US/junos/topics/topic-map/bgp-path-selection.html

the solution is to enable the `multipath vpn-unequal-cost` knob under the VRF:

----
labroot@camaro# set routing-instances k8s-test routing-options multipath vpn-unequal-cost
----

now check the VRF table again:

----
labroot@camaro# run show route table k8s-test.inet.0 101.101.101.252/32
Jun 26 20:09:21

k8s-test.inet.0: 27 destinations, 54 routes (27 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 @[BGP/170] 00:00:04, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/1/0.32771, Push 72
                    [BGP/170] 00:00:52, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 52
                   #[Multipath/255] 00:00:04, metric 100, metric2 0
                       via gr-2/1/0.32771, Push 72
                     > via gr-2/2/0.32771, Push 52
----

a `Multipath` with both GRE interface will be added under the FIP prefix, the
forwarding table reflects the same:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 01:12:36
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination        Type RtRef Next hop    Type Index    NhRef Netif
101.101.101.252/32 user     0             ulst  1048601     2
                                          indr  1048597     2
                                         Push 26     1272     2 gr-2/3/0.32771
                                          indr  1048600     2
                                         Push 26     1277     2 gr-2/2/0.32771
----

now try to pull the webpage from Internet host multiple times with `curl` or web
browser, we see the random result - both backend pod get the request and
responses back.

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = webserver-846c9ccb8b-xkjpw

[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = webserver-846c9ccb8b-r9zdt
----

the end to end packet flow is illustrated here:

.loadbalancer service ECMP
image::https://user-images.githubusercontent.com/2038044/60763675-8e2dd000-a047-11e9-91a6-5fb1319517dc.png[]

== contrail ingress

////
in chapter 3 we've learned that Ingress maps URLs to services with `rules`. this
makes this Ingress section a little bit easier. we don't need to explain
everything that happens in Ingress. instead, we can focus on the Ingress
external IP exposure and service mapping. the rest part of the story is all
about service to backend mapping which we've examined a lot. 

in this chapter we'll introduce details of ingress workflow in contrail
implementation, then we'll use a few test cases to demonstrate and verify how
ingress works exactly in contrail environment
////

in chapter 3 we've learned Ingress basis, the relation to service, Ingress types
and the yaml file of each type.

in this chapter we'll introduce details of ingress workflow in contrail
implementation, then we'll use a few test cases to demonstrate and verify how
ingress works exactly in contrail environment.

=== contrail ingress loadbalancer

like contrail's `service` implementation, contrail `Ingress` is also implemented
through loadbalancer, but with a different `loadbalancer_provider` attribute.
accordingly `contrail-svc-monitor` component takes different actions to
implement `Ingress` in contrail environment.

Remember in "Contrail-Kubernetes architecture" section we gave the "object
mapping" between kubernetes and contrail. in that section you've learned
kubernetes `service` maps to `ECMP loadbalancer (native)` and `Ingress` maps to
`Haproxy loadbalancer`. 

in `service` section when we were exploring the loadbalancer and the relevant
objects (`listener`, `pool`, and `member`), we noticed the loadbalancer's
`loadbalancer_provider` type is `native`. 

        "loadbalancer_provider": "native",

in this section we'll see `loadbalancer_provider` type is `opencontrail`
for Ingress's `loadbalancer`. we'll also look into the similarities and differences
between `service` loadbalancer and `Ingress` loadbalancer.

=== contrail ingress workflow

When an `Ingress` is configured in contrail kubernetes environment, the event
will be noticed by other system components, and a lot of actions will be
triggered.  the deep level implementation is out of the scope of this book, but
in a high level here is the workflow:

. `contrail-kube-manager` keeps listening to the events of`kube-apiserver`
. user creates an `ingress` object (rules)
. `contrail-kube-manager` gets the event from `kube-apiserver`
. `contrail-kube-manager` creates a `loadbalancer` object in contrail
  DB, and set `loadbalancer_provider` type as `opencontrail` for ingress
  (where as it is `native` for `service`).
. As mentioned earlier `contrail-service-monitor` component sees the `loadbalancer`
  creation event, based on `loadbalancer_provider` type, it invokes registered
  loadbalancer driver for the specified `loadbalancer_provider` type:
  - if the `loadbalancer_provider` type is `native`, It will invoke ECMP
    loadbalancer driver for ECMP loadbalancing which we've learned in previous
    section.
  - if the `loadbalancer_provider` type is `opencontrail`, It will invoke
    haproxy loadbalancer driver which triggers haproxy processes to be launched in
    kubernetes nodes.

as you can see, contrail implements `Ingress` with haproxy loadbalancer, this is
what you've read in the section of "contrail kubernetes object mapping". 
also in contrail environment the `contrail-kube-manager` plays `Ingress
controller` role. it reads the Ingress rules that user input and programs them
into the loadbalancer. furthermore:

* for each `Ingress` object, one loadbalancer will be created
* two haproxy processes will be created for `Ingress`, and they are working in
  "active-standby" mode:
  - one compute node runs the "active" haproxy process
  - the other compute node runs the "standby" haproxy process
* both `haproxy` processes are programmed with appropriate configuration, based
  on the rules defined in Ingress object.

=== contrail Ingress traffic flow

client request, as a type of `overlay` traffic, may come from two sources
depending on who initiates the request:

* internal request: requests coming from another pod inside of the cluster
* external request: requests coming from an Internet host outside of the cluster

the only difference between the two, is how the traffic hit the "active"
haproxy. 

an Ingress will be allocated 2 IPs: 

* cluster-internal virtual IP
* external virtual IP
//, contrail implement this with FIP

here is the traffic flow for client request:

.client to Ingress podIP
. for internal request it hits Ingress's "internal" VIP directly. 
. for external request it first hits Ingress's "external" VIP - the FIP, which
  is the one exposed to external, and that is the time when NAT starts to play
  as we've explained in `FIP` section. after NAT processing, traffic is forwarded
  to internal Ingress VIP.

[start=3]
.Ingress podIP to backend service
. from this moment on, both type of requests is processed exactly the same way.
. the requests will be "proxied" to the corresponding service IP. 

[start=5]
.backend service to backend pod
. based on the backend pods' availability, it will be sent to the node where one
  of the backend pods are located and reaches the target pods eventually. 
. In the case that the backend pods are running in a different compute node than
  the one running active haproxy, a MPLS over UDP tunnel is created between the
  two compute node.

here is the end to end service request flow when accessing from a pod in the
cluster:

//TODO: lost the drawing, need to redraw, and give text explain
.Ingress traffic flow: access from internal
image::https://user-images.githubusercontent.com/2038044/61061849-0b9c6c00-a3cb-11e9-8788-cb1c1dedafc4.png[]

here is the end to end service request flow when accessing from Internet host:

.Ingress traffic flow: access from external
//image::https://user-images.githubusercontent.com/2038044/60410376-09017180-9b96-11e9-927e-4cf1d98f2cef.png[]
//image::https://user-images.githubusercontent.com/2038044/61061268-eb1fe200-a3c9-11e9-9d36-191955b766e1.png[]
image::https://user-images.githubusercontent.com/2038044/61061427-3f2ac680-a3ca-11e9-9364-f11bea477319.png[]


////
. the "haproxy driver" will create a service instance (SI) with
  `haproxy-loadbalancer` type of template applied.
. the SI will has a "port tuple" linked to a linux netns VM
. the linux netns VM VM has its VMI, and a reference to an instance-ip
. `contrail-svc-monitor` launches the HAProxy process, with appropriate
  configuration, based on the ingress rules defined in the yaml file.

Whenever an ingress is configured in kubernetes, `contrail-kube-manager` that is
watching the kube-apiserver get the events and creates an loadbalancer object in
contrail-controller.  `contrail-svc-monitor` component listens for the load
balancer objects and takes a different action based on its
`loadbalancer_provider` attribute. when it sees `loadbalancer_provider`
attribute being `opencontrail`, it launches two haproxy processes, each in a
seperate compute node. both `haproxy` processes are programmed with appropriate
configuration based on the ingress rules you defined. the two haproxy processes
work in "active-standby" mode. 

contrail Ingress is also implemented through loadbalancer (like service), but
Ingress's loadbalancer is with a different `loadbalancer_provider` attribute,
which makes `contrail-svc-monitor` takes a different action than what it does
for service. now it is the time to tell that the `loadbalancer_provider` is
`opencontrail`, and accordingly the `contrail-svc-monitor` action is to launch a
haproxy process running with ingress rules in its configuration file. this
basically explains what we see now.

that is only a high level overview about the contrail's implementation of
ingress. 
in fact that for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will generate a service-instance (SI).
next we'll explore the objects in a little bit more details.

this is how it works:

////

contrail supports all 3 types of ingress:

* http-based single-service ingress, 
* simple-fanout ingress
* name-based virtual hosting ingress.

we'll look into each type of ingress.

=== Ingress testbed

in our lab we use the same testbed as what we use for `service` test:

//TODO: redraw with color
.Ingress testbed
image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]

.Ingress test preparation

in order to create and test Ingress, we will also need to create the following
objects in our setup before creating Ingress:

. NS: `ns-user-1`
. FIP-VN: `vn-ns-default`
. FIP-pool: `pool-ns-default`
. client pod: `cirros`

what will hold all of our test objects is the `ns-user-1` NS/project, which
refers to a NS level pool `pool-ns-default` that is to be created manually.  the
NS level pool is based on a VN `vn-ns-default` that has subnet `101.101.101/24`.
later on you will see FIP for Ingress get assigned from this subnet. a pod
`cirros` is needed to start the internal HTTP request towards the Ingress.

[NOTE]
====
if you have yaml files ready for the NS, FIP-VN and cirros pod, to create these
object:

----
$ kubectl apply -f ns/ns-user-1-default-pool.yaml
namespace/ns-user-1 created
$ kubectl apply -f vn/vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default created
$ kubectl apply -f pod/pod-cirros.yaml
pod/cirros created
----

the FIP-pool needs to be created seperately in contrail UI. refer "contrail
floating IP" section for the details

====

////
----
$ kubectl apply -f ingress/ingress-simple-fanout.yaml
ingress.extensions/ingress-sf created
----
////

after these objects are created, we can proceed to look into each type of
Ingress object. next we'll start from `single service Ingress`.

=== `single service Ingress`

single service Ingress is the most basic form of Ingress. it does not define any
rules and its main usage is to expose service to the outside world. it will
proxy all incoming service request to the same "single" backend service.

    www.juniper.net --|                 |
    www.cisco.com   --|  101.101.101.1  |-> webservice
    www.google.com  --|                 |

to demonstrate `single service` type of Ingress, the objects that we need to
create are:

* an `Ingress` object that defines the backend service
* a backend service object
* at least one backend pod for the service

////
additionally, a "client" pod is needed to test the ingress from inside of the
cluster. we can use the same `cirros` pod we've used in earlier examples as
cluster-internal client.

besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `Ingress` objects definition

===== `Ingress` definition

in our single service ingress test lab, we want to achieve this goal:

* request toward any URLs will be directed to `webservice-1` with `servicePort`
  8888

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-ss
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
----

this does not look anything fancy. basically in this `single service Ingress`
there is nothing else but a reference to one "single service" `webserver-1` as
its "backend". all HTTP request will be dispatched to this service, and from
there the request will reach a backend pod. next we'll look at the backend
service.

===== backend `service` definition

we can use exactly the same service as introduced in `service` example. 

----
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  #type: LoadBalancer
----

NOTE: the service `type` is optional. with `Ingress`, `service` does not need to
be exposed to external directly anymore. therefore `LoadBalancer` type of
service is not required. 

===== backend `pod` definition

same as in `service` example, we can use exactly the same `webserver` deployment
to launch backend pods:

----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver
  labels:
    app: webserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

===== an "all in one" yaml file

as usual, we can create an individual yaml file for each of the objects. but
considering in `Ingress`, these objects always need to be created and removed
together, it is better to "merge" definitions of all these objects into one yaml
file. yaml syntax supports this by using a "document delimitor", a `---` line
between each object definition. 

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-ss
spec:
  backend:
    serviceName: service-web-clusterip
    servicePort: 8888
---
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  #type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rc-webserver
  selector:
    matchLabels:
      app: webserver
spec:
  replicas: 1
  selector:
    app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

////
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
////
the benefits of this all-in-one yaml file are:

* you can create/update all objects in the yaml file in one go, using just one `kubectl
  apply` command
* similarly, if anything goes wrong and you need to clean up, you can delete
  all objects created with the yaml file in one `kubectl delete` command
* whenever needed, you can still delete each individual objects
  independently, by giving the object name

TIP: During test process you may need to create and delete all objects as a
whole very often, grouping multiple objects in one yaml file can be very
convenient.

===== deploy the single service Ingress

before applying the yaml file to get all objects created, let's take a quick
look at our two nodes. we want to see if there is any `haproxy` process running
without Ingress, so later after we deploy Ingress we can compare:

----
$ ps aux | grep haproxy
$ 
----

So the answer is no in both node. haproxy will be created only after we create
`Ingress` and the corresponding loadbalancer object is seen by
`contrail-service-monitor`.  we'll check this again after we create an
`Ingress`.

----
$ kubectl apply -f ingress/ingress-single-service.yaml
ingress.extensions/ingress-ss created
service/service-web-clusterip created
replicationcontroller/rc-webserver created
----

the Ingress, one service and one Deployment object are now created.

==== `Ingress` post examination

===== Ingress object

let's start to examine the Ingress object.

----
$ kubectl get ingresses.extensions -o wide
NAME         HOSTS   ADDRESS                       PORTS   AGE
ingress-ss   *       10.47.255.238,101.101.101.1   80      29m

$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1", "kind":"Ingress",
        "metadata":{"annotations":{},"name":"ingress-ss","namespace":"ns-user-1"},
        "spec":{"backend":{"serviceName":"webservice-1", "servicePort":80}}}
    creationTimestamp: 2019-07-18T04:06:29Z
    generation: 1
    name: ingress-ss
    namespace: ns-user-1
    resourceVersion: "845969"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-ss
    uid: 6b48bd8f-a911-11e9-8112-0050569e6cfc
  spec:
    backend:
      serviceName: webservice-1
      servicePort: 80
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

as expected, the backend service is applied to the Ingress properly. In this
`single service Ingress` there is no explicit rules defined to map a certain URL
to a different service - all HTTP requests will be dispatched to the same
backend service.

[TIP]
====
in the `items` -> `metadata` -> `annotations` ->
`kubectl.kubernetes.io/last-applied-configuration` section of the output:

    {"apiVersion":"extensions/v1beta1", "kind":"Ingress",
    "metadata":{"annotations":{},"name":"ingress-ss","namespace":"ns-user-1"},
    "spec":{"backend":{"serviceName":"webservice-1", "servicePort":80}}}

it actually contains the configuration information that you gave. can format it
(with JSON formatting tool like python `json.tool` module) to get a better view:

    {
        "apiVersion": "extensions/v1beta1",
        "kind": "Ingress",
        "metadata": {
            "annotations": {},
            "name": "ingress-ss",
            "namespace": "ns-user-1"
        },
        "spec": {
            "backend": {
                "serviceName": "webservice-1",
                "servicePort": 80
            }
        }
    }

you can do same formatting for all other objects to make it more readable.

====

what may confuse you is the two IP addresses shown here: 

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

we've seen these two subnets in service examples:

* `10.47.255.x` is an cluster-internal `podIP` allocated from the pod's default
  subnet
* `101.101.101.x` is the public `FIP` associated with an internal IP.

but the question is why an Ingress even requires a `podIP` and `FIP`?

////
this is the IP addresses allocated to the haproxy "virtual machine". 
what is a haproxy "virtual machine" anyway? isn't it just a process running in
the compute node? or, does compute node spawned some hidden VMs behind the
scene? 
////

let's hold the answer for now and continue to check service and pod
object created from the all-in-one yaml file. we'll come back to this question
shortly.

===== service objects

----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE  SELECTOR
service-web-clusterip  ClusterIP  10.97.226.91  <none>       8888/TCP  28m  app=webserver
----

the service is also created and allocated a clusterIP. we've seen
this before and it looks nothing special. now look at the pods.

===== backend and client pod

----
$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ... IP             NODE     ... LABELS
cirros                      1/1    Running  ... 10.47.255.237  cent222  ... app=cirros
webserver-846c9ccb8b-9nfdx  1/1    Running  ... 10.47.255.236  cent333  ... app=webserver
----

everything looks fine. there is a backend pod running for the service. we have
learned how selector and label works in service-pod associations so there is
nothing new here. next we'll examine the haproxy and try to make some sense out
of the 2 IPs allocated to Ingress object.

===== haproxy processes

earlier before the Ingress is created, we were looking for haproxy process in
node but could not see anything. let's check it again and see if any magic
happens:

.node `cent222`

----
$ ps aux | grep haproxy
188  23465  0.0  0.0  55440  852  ?  Ss  00:58  0:00  haproxy  
  -f  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf  
  -p  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid  
  -sf  23447
----

.node `cent333`

----
$ ps aux | grep haproxy
188   16335  0.0  0.0  55440  2892  ?  Ss  00:58  0:00  haproxy  
  -f  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf  
  -p  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid  
  -sf  16317
----

right after ingress got created, we see a haproxy process created in each of our
two nodes!

remember earlier when we talk about ingress contrail implementation, we've said
contrail `Ingress` is also implemented through loadbalancer (just like
`service`). Since Ingress's `loadbalancer_provider` type is `opencontrail`,
`'contrail-svc-monitor` invokes haproxy loadbalancer driver. The haproxy driver
generates required haproxy configuration for the ingress rules and triggers
haproxy processes to be launched (in active-standby mode) with the generated
configuration in kubernetes nodes.

////
that is only a high level overview about the contrail's implementation of
ingress. in fact the for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will generate a service-instance (SI).
next we'll explore the objects in a little bit more details.

----
$ ps aux | grep haproxy
188      16335  0.0  0.0  55440  2892 ?        Ss   00:58   0:00 haproxy -f /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf -p /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid -sf 16317
root     18937  0.0  0.0 112716   984 pts/0    S+   01:21   0:00 grep --color=auto haproxy

$ pstree -lnaps 16335
systemd,1 --switched-root --system --deserialize 22
  └─dockerd,6268
      └─docker-containe,6756 --config /var/run/docker/containerd/containerd.toml
          └─docker-containe,3114 -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/5e9c1c3a14cf7e2d5dca2512784b227808890b4d260c9badef9b8aab8aaaa76b -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc
              └─haproxy,16335 -f /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf -p /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid -sf 16317

$ docker ps -a | grep 5e9c
5e9c1c3a14cf        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest         "/entrypoint.sh /usr…"   5 weeks ago         Up 2 weeks                                   vrouter_vrouter-agent_1

$ docker exec -it vrouter_vrouter-agent_1 ps ef
  PID TTY      STAT   TIME COMMAND
17141 pts/0    Ssl+ 233:02 /usr/bin/python /usr/bin/contrail-nodemgr --nodetype=
16837 pts/0    Ss     0:00 -bash USER=root LOGNAME=root HOME=/root PATH=/usr/loc
17090 pts/0    T      0:00  \_ less _fzf_orig_completion_tee=complete -F %s tee
17279 pts/0    T      0:00  \_ less _fzf_orig_completion_tee=complete -F %s tee
19281 pts/0    Sl+    0:00  \_ docker exec -it vrouter_vrouter-agent_1 ps ef _fz
19297 pts/2    Rs+    0:00 ps ef PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/
16646 pts/1    Ss+    0:00 bash PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/u
 3130 pts/0    Ss+    0:00 /bin/bash /entrypoint.sh /usr/bin/contrail-vrouter-ag
 3307 pts/0    Sl+  358:06  \_ /usr/bin/contrail-vrouter-agent HOSTNAME=cent333
 5675 tty1     Ss+    0:00 /sbin/agetty --noclear tty1 linux LANG= PATH=/usr/loc

$ docker exec -it vrouter_vrouter-agent_1 ps 16335
  PID TTY      STAT   TIME COMMAND
16335 ?        Ss     0:00 haproxy -f /var/lib/contrail/loadbalancer/haproxy/5be
----
////


===== Ingress loadbalancer objects

////
.contrail object: SI, port tuple, VMI
image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]
////

we've mentioned "Ingress loadbalancer" for a few time but we haven't looked into
it yet. In `service` section, we've looked into service loadbalancer object in
UI and we inspected some details about the object data structrure. now after we
created Ingress object, let's check the list of loadbalancers object again and
see what Ingress brings in here.

.loadbalancers (configuration > Networking > Floating IPs)
//image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]
//image::https://user-images.githubusercontent.com/2038044/61432850-aa5f2600-a8ff-11e9-9d9f-932a386bf81f.png[]
image::https://user-images.githubusercontent.com/2038044/61433696-ff03a080-a901-11e9-96c1-3dfd4886c322.png[]

2 loadbalancers are generated after we applied the all-in-one yaml file.

* loadbalancer `ns-user-1__ingress-ss` for Ingress `ingress-ss`
* loadbalancer `ns-user-1__webservice-clusterip` for service `webserver-clusterip`

we've learned the service loadbalancer object previously, if you expand the
service you will see more details, but nothing would surprise us now.

.service loadbalancer object (click the triangle in the left of the loadbalancer name)
//image::https://user-images.githubusercontent.com/2038044/61049744-409cc480-a3b3-11e9-8a8e-5cdff7e6a931.png[]
//image::https://user-images.githubusercontent.com/2038044/61050199-64143f00-a3b4-11e9-9f7d-339775a3ae0e.png[]
image::https://user-images.githubusercontent.com/2038044/61433906-8bae5e80-a902-11e9-8039-fc5c5414c15a.png[]

as expected, the service loadbalancer has a ClusterIP, and a listener object
that is listening on port 8888. one thing we want to highlight here again is the
`loadbalancer_provider`. the type is "native", so the action
`contrail-svc-monitor` takes is layer 4 (application layer) ECMP process, which
we've explored a lot in service section. now let's expand Ingress loadbalancer
and look at the details.

.ingress loadbalancer object 
//image::https://user-images.githubusercontent.com/2038044/61021789-f98ddf80-a370-11e9-9cce-30a0c2671bc2.png[]
//image::https://user-images.githubusercontent.com/2038044/61434308-97e6eb80-a903-11e9-8a34-58f4bbaddf30.png[]
image::https://user-images.githubusercontent.com/2038044/64143219-0fea7180-cddd-11e9-9d93-28e08af1f6f2.png[]

some highlights in the figure:

////
* `loadbalancer_provider` is `opencontrail`
* Ingress loadbalancer has a reference to a `service-instance` (SI) object
* `SI` object has a property `ha_mode` set to `active-standby`
* `SI` object has interface IPs same as those in loadbalancer, hence you can see
  the same IP `10.47.255.238`
////
* `loadbalancer_provider` is `opencontrail`
* Ingress loadbalancer has a reference to a `virtual-machine-interface` (VMI)
  object
* the `VMI` object is referred by an `instance-ip` object with an (fixed) IP
  `10.47.255.238` and a `floating-ip` object with an (floating) IP
  `101.101.101.1`

at this moment,  we can explain the Ingress IP `10.47.255.248` seen in ingress.
basically:

* it is an cluster-internal IP address allocated from the default pod network as
  loadbalancer vip
* it is the frontend IP that the Ingress loadbalancer will listen for HTTP
  requests
* it is also what the public FIP `101.101.101.1` maps to with NAT

TIP: in this book we'll refer this private IP with different names
interchangeably: "Ingress Internal IP", "Ingress internal VIP", "Ingress private
IP", "Ingress loadbalancer interface IP", etc.  to differentiate it from the
Ingress public FIP, we can also name it as "Ingress podIP" since the internal vip
is allocated from the pod-network. similarly we'll refer the Ingress public FIP
as "Ingress external IP".

Now to compare the different purposes of these two IPs:

* Ingress `podIP` is the VIP facing other pods inside of the cluster. To reach
  Ingress from inside of the cluster, requests coming from other pods will have
  their destination IP set to Ingress `podIP`.
* Ingress `FIP` is VIP facing "Internet host" outside world. To reach Ingress
  from outside of the cluster, requests coming from Internet hosts need to have
  their destinations IP set to Ingress FIP.  when node receives traffic destined
  to the Ingress FIP from outside of the cluster, vrouter will translate it into
  the Ingress `podIP`

****
the detail Ingress loadbalancer object implementation refers to a SI (service
instance), and the SI again includes other data structure or reference to other
objects (VM, VMI, etc). overall it is more complicated and involves more details
than what we've covered and it is hard to put everything in this book. we've
tailored the details into a high level overview so that important concepts like
haproxy and the two Ingress IPes can be understood.
****

Once a HTTP/HTTPS request arrives to the Ingress `podIP`, from internal or
external, Ingress loadbalancer will do HTTP/HTTPS proxy operation through
haproxy process, and dispatch the requests towards the service and eventually to
the backend pod.

we've seen the haproxy process is running, to examine more details of this proxy
operation, next we can further check its configuration file for the running
parameters details.
//configuration file and confirm the ingress rules are programmed properly.

===== `haproxy.conf` file

in each (compute) node, under `/var/lib/contrail/loadbalancer/haproxy/` folder
there will be a subfolder for each loadbalancer uuid. the file structure looks
like this:

----
  8fd3e8ea-9539-11e9-9e54-0050569e6cfc
  ├── haproxy.conf
  ├── haproxy.pid
  └── haproxy.sock
----

you can check haproxy.conf file for the haproxy configuration:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ......
        ulimit-n 200000
        maxconn 65000
        ......
        stats socket
        /var/lib/contrail/loadbalancer/haproxy/6b48bd8f-a911-11e9-8112-0050569e6cfc/haproxy.sock
            mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend f3a7a6a6-5c6d-4f78-81fb-86f6f1b361cf
        option tcplog
        bind 10.47.255.238:80                                   #<---
        mode http                                               #<---
        option forwardfor
        default_backend b45fb570-bec5-4208-93c9-ba58c3a55936    #<---

backend b45fb570-bec5-4208-93c9-ba58c3a55936                    #<---
        mode http                                               #<---
        balance roundrobin
        option forwardfor
        server 4c3031bb-e2bb-4727-a1c7-95afc580bc77 10.111.216.190:80 weight 1
                                                    ^^^^^^^^^^^^^^^^^
----

the configuration is simple, and here is the illustration of it:

.single service ingress 
//image::https://user-images.githubusercontent.com/2038044/61689786-db6f9a00-acf5-11e9-9625-4ba1e570d354.png[]
image::https://user-images.githubusercontent.com/2038044/63656134-88f32480-c75e-11e9-97e9-5a84ffa874cd.png[]

* the haproxy `frontend` represents the "frontend" of an Ingress, facing clients
* the haproxy `backend` represents the "backend" of an Ingress, facing services.
* the haproxy `frontend` defines a `bind` to the Ingress podIP and `mode` `http`.
  these knobs indicate what the frontend is listening.  
* the haproxy `backend` section defines the `server`, which is backend `service`
  in our case. it has a format of `serviceIP:servicePort`, which is the exact
  `service` object we've created using the all-in-one yaml file.
* the `default_backend` in `frontend` section defines which backend is the
  "default": it will be used when a haproxy receives a URL request that has no
  explicit match anywhere else in the `frontend` section. in this case the
  `default_backend` refers to the only `backend` service `10.111.216.190:80`.
  this is due to the fact that there is no `rules` defined in `single service
  Ingress`, so all HTTP requests will go to the same default_backend service,
  regardless of what URL the client sent.

NOTE: later in `simple fanout Ingress` and `name-based virtual hosting Ingress`
examples, we will see another type of configuration statement
`use_backend...if...` that can be used to force each URL to go to a different
backend.

through this configuration, the haproxy implemented our single service Ingress.

===== gateway router VRF table

we've explored a lot inside of the cluster. now let's look at the gateway
router's VRF table. 

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61

----

Same as in service example, from outside of the cluster, only FIP is visible.
`detail` version of it conveys more information:

----
labroot@camaro> show route table k8s-test 101.101.101.1 detail

k8s-test 24 destinations, 49 routes (24 active, 0 holddown, 0 hidden)
101.101.101.1/32 (1 entry, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:5     #<---
                ......
                Source: 10.169.25.19
                Next hop: via gr-2/2/0.32771, selected
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                Indirect next hop: 0x900d320 1048597 INH Session ID: 0x6f9
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 34         Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                Import Accepted
                VPN Label: 61
                Localpref: 200                          #<---
                Router ID: 10.169.25.19
----

////
                Communities: target:500:500 target:64512:8000016
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1040000
////

//TODO: add diagram

* through XMPP, vrouter advertises the FIP prefix to contrail controller.
  at least 2 pieces of information from the output indicates who represents the
  FIP in this example - node `cent222`:
  - `Protocol next hop` being `10.169.25.20`
  - `Route Distinguisher` being `10.169.25.20:5`
* through MP-BGP, contrail controller "reflects" the FIP prefix to the gateway
  router, `Source: 10.169.25.19` indicates this fact.

so it looks `cent222` is "selected" to be the active haproxy node, and the other
node `cent333` is the standby one. therefore you should expect client request
coming from Internet host goes to node `cent222` first. of course, the overlay
traffic will be carried in MPLS over GRE tunnel, same as what you've seen from
service example. 

the FIP advertisement towards gateway router is exactly the same in all types of
Ingresses.

****
another fact that we've skipped on purpose is the different "local
preference" value used by the active and standby node when advertising FIP
prefix. saying that will involve other complex topics like the active node
selection algorithm and so on. but it is worth to undertand from high level:

both nodes have loadbalancer and haproxy running so both will advertise the FIP
prefix `101.101.101.1` to gateway router. however, they are advertised with
different local preference value. the "Active" node advertise with a value of
`200` and the "standby" node with `100`. contrail controller have both routes
from the 2 nodes, but only the "winning" one will be advertised to the gateway
router. that is why the "other" BGP route is dropped and only one is
displayed.  `Localpref` being `200` proves it is coming from the active compute
node. this applies to both Ingress public FIP route and internal VIP route
advertisement.
****

==== `Ingress` verification: from internal

we've explored a lot about ingress loadbalancer and the related service, pod
objects, etc. now it is time to verify the "end-to-end" test result. since the
`Ingress` serves both inside and outside of the cluster, our verification will
start from the client pod inside of cluster, then from an Internet host outside
of it.

.from inside of cluster

----
$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ kubectl exec -it cirros -- \
curl -H 'Host:www.google.com' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]
$ kubectl exec -it cirros -- \
curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]
----

we still use the `curl` command to trigger HTTP requests towards the ingress's
private IP. the return proves our `Ingress` works: requests towards different
URLs are all proxied to the same backend pods, through the default backend
services `service-web-clusterip`. 

in the fourth request we didn't give a URL via `-H`, `curl` will fill `host`
with the request IP address - `10.47.255.238` in this test, again it goes to the
same backend pod and get the same returned response.

NOTE: The `-H` option is important in Ingress test with `curl`. it carries the
full URL in HTTP payload that the Ingress loadbalancer is waiting for. without
it the HTTP header will carry `Host: 10.47.255.238`, which has no matching rule,
so it will be treated same as with a unknown URL.

==== `Ingress` verification: from external (Internet host)

the more exciting part of the test is to visit the URLs from external. overall
we hope `Ingress` meant to expose services to the Internet host, even though it
does not have to. 

to make sure the URL resolves to the right FIP address, we need to update
`/etc/hosts` file by adding one line in the end - you probably don't want to
just end up with a nice webpage from `Juniper`/`cisco`'s official website as
your test result.

----
# echo "101.101.101.1  www.juniper.net www.cisco.com www.google.com" >> /etc/hosts
# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
101.101.101.1  www.juniper.net www.cisco.com www.google.com     #<---
----

now, from internet host's "desktop", we launch chrome browser, and input one of
the 3 URLs: `www.juniper.net`, `www.cisco.com` or `www.google.com`. By keep
refreshing the pages we can confirm all HTTP request is returned by the same
backend pod.

//image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

.access `www.juniper.net` from Internet host
//image::https://user-images.githubusercontent.com/2038044/63735716-adc6c500-c84e-11e9-9267-0112916c7ea8.png[]
image::https://user-images.githubusercontent.com/2038044/63737822-a951da80-c855-11e9-99ef-1c769f71d8b6.png[]

same result can be seen from `curl` also. the command is exactly the same as
what we've seen when testing from a pod, except this time we send requests to
Ingress external FIP, instead of the Ingress internal podIP.

.from Internet host machine

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]
----

everything works!

next we'll look at the second Ingress type `simple fanout Ingress`. before
movig forward, it is better to clean up everything.  now we can take advantage
of the all-in-one yaml file - everything can be cleared with one `kubectl
delete` command using the same all-in-one yaml file:

----
$ kubectl delete -f ingress/ingress-single-service.yaml
ingress.extensions "ingress-ss" deleted
service "webservice-1" deleted
replicationcontroller "Vwebserver-1-846c9ccb8b" deleted
----

=== `simple fanout Ingress`

//TODO: adjust the TOC, default backend
both `simple fanout Ingress` and `name-based virtual host Ingress` support "URL
routing", the only difference is the former is based on `path` and the latter is
based on `host`.

with `simple fanout Ingress`, based on the URL path and rules, an ingress
loadbalancer directs traffic to different backend services.

    www.juniper.net/qa --|                 |-> webservice-1
                         |  101.101.101.1  |
    www.juniper.net/dev -|                 |-> webservice-2

to demonstrate `simple fan-out` type of Ingress, the objects that we need to
create are:

* an `Ingress` object: defines the rules, mapping 2 paths to 2
  backend services
* 2 backend services objects
* each service requires at least one pod as backend

we use the same `cirros` pod as cluster-internal client we've used in previous
examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `Ingress` objects definition

===== `ingress` definition

in our `simple fanout Ingress` test lab, we want to achieve these goals for host
`www.juniper.net`:

* request toward path `/dev` will be directed to a service `webservice-1`
  with `servicePort` 8888
* request toward path `/qa` will be directed to a service `webservice-2`
  with `servicePort` 8888

here is the corresponding yaml file to implement these goals:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-sf
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: webservice-2
          servicePort: 8888
----

in contrast to `single service Ingress`, in `simple fanout Ingress` object (and
"name-based virtual host Ingress") we see "rules" defined - here it is the
mappings from multiple "paths" to different backend services. 

===== backend `service` definition

since we defined 2 rules each for a `path`, we need two services accordingly. we 
can "clone" the previous service in `single service Ingress` example and
just change the service's name and selector to generate the second service.
e.g.: this is definition of `webservice-1` and `webservice-2` service.

----
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
----

----
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
  #type: LoadBalancer
----

===== backend `pod` definition

because we have 2 backend services now, apparently we also need at least two
backend pods each with a label matching to a service. we can clone the previous
`Deployment` into two and just change the name and label of the second
`Deployment`.

There are the definition of the `Deployment`:

.`Deployment` for webserver-1
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----


.`Deployment` for webserver-2
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

===== deploy `simple fanout Ingress`

same as in `single service Ingress`, we put everything together to get an
"all-in-one" yaml file to test `simple fanout Ingress`:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-sf
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: webservice-2
          servicePort: 8888
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
  #type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

.`apply` the all-in-one yaml file to create all objects

----
$ kubectl apply -f ingress/ingress-simple-fanout.yaml
ingress.extensions/ingress-sf created
service/webservice-1 created
service/webservice-2 created
deployment.extensions/webserver-1 created
deployment.extensions/webserver-2 created
----

the Ingress, two `service` and two `Deployment` objects are now created.

==== `Ingress` post examination

===== ingress objects and ingress loadbalancer

let's look at the kubernetes objects created from the all-in-one yaml file:

.ingress objects
----
$ kubectl get ingresses.extensions
NAME        HOSTS            ADDRESS                      PORTS  AGE
ingress-sf  www.juniper.net  10.47.255.238,101.101.101.1  80     7s

$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ingress-sf","namespace":"ns-user-1"},"spec":{"rules":[{"host":"www.juniper.net","http":{"paths":[{"backend":{"serviceName":"webservice-1","servicePort":8888},"path":"/dev"},{"backend":{"serviceName":"webservice-2","servicePort":8888},"path":"/qa"}]}}]}}
    creationTimestamp: 2019-08-13T06:00:28Z
    generation: 1
    name: ingress-sf
    namespace: ns-user-1
    resourceVersion: "860530"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-sf
    uid: a6e801fd-bd8f-11e9-9072-0050569e6cfc
  spec:
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: webservice-1
            servicePort: 8888
          path: /dev
        - backend:
            serviceName: webservice-2
            servicePort: 8888
          path: /qa
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

the "rules" are defined properly, within each rule there is a mapping from a
`path` to the corresponding `service`. we see same Ingress internal podIP and
external FIP as we've seen in the previous `single service Ingress` example:

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

That is why from gateway router's perspective, there is no differences between
all types of Ingress. in all cases a public FIP will be allocated to the Ingress
and it is advertised to the gateway router:

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61
----

now check backend services and pods:

.service objects
----
$ kubectl get svc -o wide
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE    SELECTOR
webservice-1  ClusterIP  10.111.234.187  <none>       8888/TCP  4m46s  app=webserver-1
webservice-2  ClusterIP  10.97.77.82     <none>       8888/TCP  4m46s  app=webserver-2
----

////
.the real capture
----
$ kubectl get svc -o wide
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   SELECTOR
webservice-1   ClusterIP   10.96.51.227    <none>        8888/TCP   85m   app=webserver-1
webservice-2   ClusterIP   10.100.156.38   <none>        8888/TCP   85m   app=webserver-2
----
////

.backend and client pod
----
$ kubectl get pod -o wide
NAME                          READY  STATUS   ... AGE  IP             NODE    ..
cirros                        1/1    Running  ... 44d  10.47.255.237  cent222 ..
webserver-1-846c9ccb8b-wns77  1/1    Running  ... 13m  10.47.255.236  cent333 ..
webserver-2-846c9ccb8b-t75d8  1/1    Running  ... 13m  10.47.255.235  cent333 ..

$ kubectl get pod -o wide -l app=webserver-1
NAME                          READY  STATUS   ... AGE  IP             NODE    ..
webserver-1-846c9ccb8b-wns77  1/1    Running  ... 156m 10.47.255.236  cent333 ..

$ kubectl get pod -o wide -l app=webserver-2
NAME                          READY  STATUS   ... AGE  IP             NODE    ..
Vwebserver-2-846c9ccb8b-t75d8  1/1    Running  ... 156m 10.47.255.235  cent333 ..
----

two services are created, each with a different clusterIP allocated.
for each service there is a backend pod. later when we verify Ingress from
client we'll see these podIPs in the returned web pages.

.contrail Ingress loadbalancer object
comparing with `single service Ingress`, in here the only difference is one more
`service` loadbalancer:

.`simple fanout Ingress` loadbalancers (UI: configuration > Networking > Floating IPs)
//image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]
//image::https://user-images.githubusercontent.com/2038044/61432850-aa5f2600-a8ff-11e9-9d9f-932a386bf81f.png[]
//image::https://user-images.githubusercontent.com/2038044/61433696-ff03a080-a901-11e9-96c1-3dfd4886c322.png[]
image::https://user-images.githubusercontent.com/2038044/61790253-befe5b00-ade4-11e9-8a97-40c7d924b7e6.png[]

totally 3 loadbalancers are generated in this test:

* loadbalancer `ns-user-1__ingress-sf` for Ingress `ingress-sf`
* loadbalancer `ns-user-1__webservice-1` for service `webserver-1`
* loadbalancer `ns-user-1__webservice-2` for service `webserver-2`

we won't explore the details of each objects again this time since we've
investigated the key parameters of `service` and `Ingress` loadbalancers in
`single service Ingress` - there is really nothing new here.

===== haproxy process and haproxy.cfg file

in `single service Ingress` example, we've demonstrated the two haproxy
processes invoked by `contrail-svc-monitor` when it sees `loadbalancer` appears
with `loadbalancer_provider` set to `opencontrail`. in the end of that example,
after we removed the `single service Ingress`, since there is no more `Ingress`
left in the cluster, the two haproxy processes will be killed.  now with a new
Ingress creation, two new haproxy processes are invoked again:

.node `cent222`

----
$ ps aux | grep haproxy
188   29706  0.0  0.0  55572   2940  ?      Ss  04:04  0:00  haproxy  
    -f /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.conf 
    -p /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.pid 
    -sf  29688
----

.node `cent333`

----
[root@b4s42 ~]# ps aux | grep haproxy
188   1936  0.0  0.0  55572   896  ?      Ss  04:04  0:00  haproxy  
    -f /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.conf
    -p /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.pid  
    -sf  1864
----

what interests us is how the `simple fanout Ingress` "rules" are programmed in
the haproxy.conf file this time. let's look at the haproxy configuration file:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583
$ cat haproxy.conf
global
    daemon
    user haproxy
    group haproxy
    log /var/log/contrail/lbaas/haproxy.log.sock local0
    log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
    tune.ssl.default-dh-param 2048
    ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:......
    ulimit-n 200000
    maxconn 65000
    stats socket
        /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.sock
        mode 0666 level user

defaults
    log global
    retries 3
    option redispatch
    timeout connect 5000
    timeout client 300000
    timeout server 300000

frontend acd9cb38-30a7-4eb1-bb2e-f7691e312625
    option tcplog
    bind 10.47.255.238:80
    mode http
    option forwardfor
    acl 020e371c-e222-400f-b71f-5909c93132de_host hdr(host) -i www.juniper.net
    acl 020e371c-e222-400f-b71f-5909c93132de_path path /qa
    use_backend 020e371c-e222-400f-b71f-5909c93132de if
        020e371c-e222-400f-b71f-5909c93132de_host
        020e371c-e222-400f-b71f-5909c93132de_path


    acl 46f7e7da-0769-4672-b916-21fdd15b9fad_host hdr(host) -i www.juniper.net
    acl 46f7e7da-0769-4672-b916-21fdd15b9fad_path path /dev
    use_backend 46f7e7da-0769-4672-b916-21fdd15b9fad if
        46f7e7da-0769-4672-b916-21fdd15b9fad_host
        46f7e7da-0769-4672-b916-21fdd15b9fad_path


backend 020e371c-e222-400f-b71f-5909c93132de
    mode http
    balance roundrobin
    option forwardfor
    server c13b0d0d-6e4a-4830-bb46-2377ba4caf23 10.97.77.82:8888 weight 1

backend 46f7e7da-0769-4672-b916-21fdd15b9fad
    mode http
    balance roundrobin
    option forwardfor
    server d58689c2-9e59-494b-bffd-fb7a62b4e17f 10.111.234.187:8888 weight 1
----

NOTE: the configuration file is slightly formatted to make it fit to a page
width. 
//also we moved the default backend to the end.

the configuration looks a little bit more complicated than the one for `single
service Ingress`, but the most important part of it is looks pretty
straightforward.

* the haproxy `frontend` section: it now defines URLs. each URL is represented
  by a pair of `acl` statement, one for `host`, and the other for `path`. in a
  nutshell, `host` is the domain name and `path` is what follows the `host` in
  the URL string. here for `simple fanout Ingress` there are is a host
  `www.juniper.net` with two different paths: `\dev` and `\qa`.
* the haproxy `backend` section: now we see 2 of them. for each `path` there is
  a dedicated service.
* `use_backend...if...` command in `frontend` section: this statement declares
  the ingress rules: "if" the URL request includes a specified `path` that
  matches to what is programmed in one of the two ACLs pairs, "use" the
  corresponding "backend" (that is a service), to forward the traffic. 
  
for example, `acl 020e371c-e222-400f-b71f-5909c93132de_path path /qa` defines
path `/qa`. if the URL request contains such a path, haproxy will "use_backend"
`020e371c-e222-400f-b71f-5909c93132de`, which you can find in `backend` section.
The backend is a UUID referring to `server c13b0d0d-6e4a-4830-bb46-2377ba4caf23
10.97.77.82:8888 weight 1`, which essentially is a service. you can identify
this by looking at the `serviceIP:port`: `10.97.77.82:8888`.

////
* `default_backend` defines which backend is the "default": it will be used when
  a haproxy receives a URL request that is other than the two defined one
////

this configuration file can be illustrated in this figure:

.simple fanout service
//image::https://user-images.githubusercontent.com/2038044/61764125-0f09fd00-ada6-11e9-8a30-61e3a2ed2db3.png[]
//image::https://user-images.githubusercontent.com/2038044/61790000-1cde7300-ade4-11e9-9a78-d1f5c13c7046.png[]
//image::https://user-images.githubusercontent.com/2038044/62929112-c8496a80-bd87-11e9-8b53-fdc85ca959d0.png[]
image::https://user-images.githubusercontent.com/2038044/63776303-1e9dc980-c8af-11e9-8c25-cdbc7d6cc73a.png[]

with this `proxy.conf` file, the haproxy implements our `simple fanout Ingress`:

* if the full URL composes `host` "www.juniper.net" and `path` "/dev" , request
  will be dispatched to `webservice-1` (`10.111.234.187:8888`)
* if the full URL composes `host` "www.juniper.net" and `path` "/qa" , request
  will be dispatched to `webservice-2` (`10.97.77.82:8888`)
* for any other URLs the request will be dropped because there is no
  corresponding backend service defined for it.

NOTE: in practice we often need a the `default_backend` service to process all
those HTTP request with no matching URLs in the rules. we've seen it in the
previous examle of `single service Ingress`. later in `name-based virtual
hosting Ingress` section we'll combine the `use_backend` and `default_backend`
together to privde this type of flexibility.


==== `Ingress` verification: from internal

===== problem of old method

we'll compose similar curl commands to test `simple fanout Ingress`. this time
we give different paths and see how Ingress distribute them.

----
$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.238/dev
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server.  If you entered the URL
manually please check your spelling and try again.</p>
----

it doesn't work!

the reason is our webserver coming with the backend pods does not support the
`path` `dev` or `qa` in the URL. 

[TIP]
=====
checking the webserver's code reveals the issue:

----
$ kubectl exec -it webserver-1-846c9ccb8b-wns77 bash
root@webserver-1-846c9ccb8b-wns77:/app# ls
Dockerfile  app.py  requirements.txt  static
root@webserver-1-846c9ccb8b-wns77:/app# cat app.py
# Simple Web-Server
from flask import Flask
import subprocess
app = Flask(__name__)
def workers():
    cmd_ip = 'ifconfig | sed -n 2p | cut -d ":" -f2 | cut -d " " -f1 | tr -d "\n"'
    cmd_hostname = 'hostname | tr -d "\n"'
    ip_addr = str(subprocess.check_output(cmd_ip, shell=True))
    hostname = str(subprocess.check_output(cmd_hostname, shell=True))
    return '''
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b> pod</h2><br><h3>IP address = ''' + ip_addr + '''<br>Hostname = ''' + hostname + '''</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
'''
@app.route('/')                 #<---
def root():
    return workers()
@app.route('/contrail')         #<---
def contrail():
    return workers()
if __name__ == '__main__':
    app.run(debug=True,host='0.0.0.0', port=80)
----

in the code only path `/` and `/contrail` is supported.

=====

===== workaround

there are several ways to workaround the problem:

* change the current server code and restart the server
* create a new server that supports URLs with the 2 paths

in this section we will demonstrate how to use an existing python module to
create a HTTP server, and make it to serve the "path" that we give in the URL:

* in each backend pods we'll create a new HTTP server.
* the server is created from python module `SimpleHTTPServer` coming with
  the webserver image of each pods.
* to avoid confliction, the new HTTP servers will listen on a different port
  other than default port `80`, which had been used by the existing server
  process.
* accordingly, we update the `targetPort` (from `80` to `90`) of `service` in
  order to deliver the request to the new HTTP server.
  
NOTE: don't forget to delete the old services and create the new ones with the
updated parameters.

===== create web pages

in order to make the webserver responding to the URL with given paths, we can
create some web pages with file names being same as the `path`: `dev`, `qa`,
`abc` and etc. to make the test to return consistent output with the one
returned by the old server, we simply use the same template for our web page.
make sure to change the IP address and Hostname to the value of the pod that
this file is going to be copied to. for example pod `webserver-1-846c9ccb8b-wns77` is
with IP `10.47.255.236`, so we generate a file `dev` for this pod:

.generate a webpage `dev`
----
cat <<EOF > dev
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    webserver-1-846c9ccb8b-wns77</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
EOF
----

.copy the webpage into the backend pod

----
$ kubectl cp dev Vwebserver-1-846c9ccb8b-wns77:/app/dev
$ kubectl cp dev Vwebserver-1-846c9ccb8b-wns77:/app/qa
$ kubectl cp dev Vwebserver-1-846c9ccb8b-wns77:/app/abc
$ kubectl exec -it Vwebserver-1-846c9ccb8b-wns77 -- ls -lt
total 24
-rw-r--r--. 1 root root 364 Aug 13 06:26 abc    #<---
-rw-r--r--. 1 root root 364 Aug 13 06:26 dev    #<---
-rw-r--r--. 1 root root 364 Aug 13 06:26 qa     #<---
-rw-r--r--. 1 root root 937 Apr 21  2017 app.py
drwxr-xr-x. 2 root root  23 Apr 21  2017 static
-rw-r--r--. 1 root root 254 Apr  6  2017 Dockerfile
-rw-r--r--. 1 root root   6 Apr  6  2017 requirements.txt
----

similarly, just change the IP and Hostname of this webpage to the vaule of the
other backend pod and then copy into it:

//$ sed 's/10.47.255.236/10.47.255.235/g; s/webserver-846c9ccb8b-wns77/Vwebserver-2-846c9ccb8b-t75d8/g' dev > dev2

----
$ sed 's/10.47.255.236/10.47.255.235/g' dev > temp
$ sed 's/webserver-1-846c9ccb8b-wns77/webserver-2-846c9ccb8b-t75d8/g' temp > dev2
$ kubectl cp dev2 Vwebserver-2-846c9ccb8b-t75d8:/app/dev
$ kubectl cp dev2 Vwebserver-2-846c9ccb8b-t75d8:/app/qa
$ kubectl cp dev2 Vwebserver-2-846c9ccb8b-t75d8:/app/abc
$ kubectl exec -it Vwebserver-2-846c9ccb8b-t75d8 -- ls -lt
total 24
-rw-r--r--. 1 root root 366 Aug 13 06:39 abc    #<---
-rw-r--r--. 1 root root 366 Aug 13 06:39 dev    #<---
-rw-r--r--. 1 root root 366 Aug 13 06:39 qa     #<---
-rw-r--r--. 1 root root 937 Apr 21  2017 app.py
drwxr-xr-x. 2 root root  23 Apr 21  2017 static
-rw-r--r--. 1 root root 254 Apr  6  2017 Dockerfile
-rw-r--r--. 1 root root   6 Apr  6  2017 requirements.txt
----

TIP: we create a page `abc` that has no corresponding path to test the scenario
when incoming HTTP request contains an "unknown" path.

===== start new web server

for each pod open a seperate terminal, then start a web server with python
module `SimpleHTTPServer`, listening on port 90:

----
$ kubectl exec -it Vwebserver-1-846c9ccb8b-wns77 -- python -m SimpleHTTPServer 90
Serving HTTP on 0.0.0.0 port 90 ...
$ kubectl exec -it Vwebserver-2-846c9ccb8b-t75d8 -- python -m SimpleHTTPServer 90
Serving HTTP on 0.0.0.0 port 90 ...
----

////
create some files with name being same as the `paths`: `dev`, `qa`, and `abc`.

----
kubectl exec -it Vwebserver-1-846c9ccb8b-s2zn9 -- bash -c "echo Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249 > dev"
kubectl exec -it Vwebserver-1-846c9ccb8b-s2zn9 -- bash -c "echo Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249 > qa"
kubectl exec -it Vwebserver-1-846c9ccb8b-s2zn9 -- bash -c "echo Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249 > abc"
kubectl exec -it Vwebserver-2-846c9ccb8b-k9x26 -- bash -c "echo Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248 > dev"
kubectl exec -it Vwebserver-2-846c9ccb8b-k9x26 -- bash -c "echo Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248 > qa"
kubectl exec -it Vwebserver-2-846c9ccb8b-k9x26 -- bash -c "echo Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248 > abc"
----

////

===== the new all-in-one yaml file

the last step before testing our Ingress is to update the previous services
`targetPort` 80 in the "all-in-one" yaml file with the new HTTP server's port
90. delete the current services and apply the updated all-in-one yaml file
again:

.updated yaml file ingress-simple-fanout2.yaml (diff only)
----
$ diff ingress-simple-fanout.yaml ingress-simple-fanout2.yaml
26c26
<     targetPort: 80
---
>     targetPort: 90
38c38
<     targetPort: 80
---
>     targetPort: 90
----

.delete the old services and apply yaml file again
----
$ kubectl delete svc/webservice-1
service "webservice-1" deleted

$ kubectl delete svc/webservice-2
service "webservice-2" deleted

$ kubectl apply -f ingress/ingress-simple-fanout2.yaml
ingress.extensions/ingress-sf unchanged
service/webservice-1 created    #<---
service/webservice-2 created    #<---
deployment.extensions/webserver-1 unchanged
deployment.extensions/webserver-2 unchanged
----

only the two services are updated and deleted, so when we apply the new yaml
file, only these two objects are recreated - all other objects keep unchanged.

===== test the URL with different paths

----
$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/dev | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-1-846c9ccb8b-wns77
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/qa | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-t75d8
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/abc | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.
----

////
----
$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.238/dev
Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249

$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.250/qa
Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248

$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.250/abc
<html><body><h1>503 Service Unavailable</h1>
No server is available to handle this request.
</body></html>

$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.250
<html><body><h1>503 Service Unavailable</h1>
No server is available to handle this request.
</body></html>

----
////

//again we use the `curl` command to trigger HTTP requests towards the ingress's loadbalancer IP. 
the return proves our `Ingress` works: the 2 requests towards
"/qa" and "/dev" paths are proxied to 2 different backend pods, through 2
backend services `webservice-1` and `webservice-2` respectively. 

the third request with a path `abc` composes a "unknown" URL which does not have
a matching service in `Ingress` configuration, so it won't be served. same for
the last 2 requests, without a path, or with a different Host, the URL
become unknown to our Ingress so it won't be served.

you may think that we should add more rules to include these scenarios. doing
that works fine but not scalable apparently - you can never cover all possible
paths and URLs that could come into your server. as we mentioned earlier, one
solution is to use `default_backend` service to process all "other" HTTP
requests.  we'll cover this in the next example.

////
same rule applies to the fourth request. without given a URL via `-H`, `curl`
will fill `host` with the request IP address, `10.47.255.238` in this test, and
since that "URL" does not have a defined backend service so the default backend
service will be used. 

in our test, we have just one backend pod for each
service, so the podIP in returned webpage tells who is who. except in the second
test the returned podIP `10.47.255.235` represent `webservice-2`, all other three
tests returns podIP for `webservice-1`, as expected.
////

==== `Ingress` verification: from external (Internet host)

////
with chrome, this time we launch two chrome page side by side, and input URLs
`www.juniper.net/qa` and `www.juniper.net/dev`. By keep refreshing the 2 pages
we can confirm "qa" page is always returned by RC `rc-webserver-1` pod
`10.47.255.236`, "dev" page is always returned by RC `rc-webserver-2` pod
`10.47.255.235`. 

//TODO: capture
image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

same result can be seen from `curl` also. 
////

to test `simple fanout Ingress` from outside of the cluster, the command is the
same as what we've seen when initiating the HTTP request from inside of a pod,
except this time we are initiating from an Internet host. we will send the HTTP
requests to the Ingress's public FIP, instead of its internal podIP.

from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1/qa | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-t75d8
                                    [giphy]

$ curl -H 'Host:www.juniper.net' 101.101.101.1/dev | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-wns77
                                    [giphy]

$ curl -H 'Host:www.juniper.net' 101.101.101.1/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ curl -H 'Host:www.juniper.net' 101.101.101.1/abc | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ curl -H 'Host:www.cisco.com' 101.101.101.1/dev | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.
----

=== `name based virtual hosting Ingress`

`Name based virtual hosting Ingress` support routing HTTP traffic to multiple
host names at the same IP address. based on the URL and rules, an Ingress
loadbalancer directs traffic to different backend services, and each service
direct traffic to its backend pods. 

    www.juniper.net --|                 |-> webservice-1
                      |  101.101.101.1  |
    www.cisco.com   --|                 |-> webservice-2

to demonstrate `virtual host` type of Ingress, the objects that we need to
create are same as previous `simple fanout Ingress`:

* an `Ingress` object: the rules, mapping 2 URLs to 2 backend services
* 2 backend services objects
* each service requires at least one pod as backend

//again we use the same `cirros` pod as cluster-internal HTTP client we've used in
//earlier examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `Ingress` objects definition

===== `ingress` definition

in our virtual host ingress test lab, we define the following rules:

* request toward URL `www.juniper.net` will be directed to a service `webservice-1`
  with `servicePort` 8888
* request toward URL `www.cisco.com` will be directed to a service `webservice-2`
  with `servicePort` 8888
* request toward any URLs other than these 2, will be directed to `webservice-1`
  with `servicePort` 8888. Effectively we want `webservice-1` to become the
  default backend service in here.

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: webservice-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: webservice-2
              servicePort: 8888
            path: /
----

.backend `service` and `pod` definition

same exact service and Deployment definition that were used in `simple fanout
Ingress` can be used here.

===== an "all in one" yaml file

----
$ cat ingress/ingress-test.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: webservice-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: webservice-2
              servicePort: 8888
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

.`apply` the all-in-one yaml file to create Ingress and all necessary objects

----
$ kubectl apply -f ingress/ingress-virtual-host-test.yaml
ingress.extensions/ingress-vh created
service/webservice-1 created
service/webservice-2 created
deployment.extensions/webserver-1 created
deployment.extensions/webserver-2 created
----

the `Ingress`, two services and two `Deployment` objects are now created.

==== `Ingress` post examination

===== examine ingress objects

let's start to look at the `Ingress` object.

----
$ kubectl get ingresses.extensions -o wide
NAME        HOSTS                          ADDRESS                      PORTS  AGE
ingress-vh  www.juniper.net,www.cisco.com  10.47.255.248,101.101.101.1  80     8m27s
----

//the internal and external Ingress IP remains the same, but 
comparing with `simple fanout Ingress`, this time we see two hosts instead of
one. Each host represents a domain name.

----
$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    ......
    generation: 1
    name: ingress-vh
    namespace: ns-user-1
    resourceVersion: "830991"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-vh
    uid: 8fd3e8ea-9539-11e9-9e54-0050569e6cfc
  spec:
    backend:
      serviceName: webservice-1
      servicePort: 8888
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: webservice-1
            servicePort: 8888
          path: /
    - host: www.cisco.net
      http:
        paths:
        - backend:
            serviceName: webservice-2
            servicePort: 8888
          path: /
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

the rules are defined properly, within each rule there is a mapping from a
`host` to the corresponding service. 

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

the services, pods and FIP prefix advertisement to gateway router behavior are
all exactly the same as those in `simple fanout Ingress`.

===== exploring Ingress loadbalancer objects

//TODO: should skip if simple fan-out Ingress can capture this.
////
.contrail object: SI, port tuple, VMI
image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]
////

3 loadbalancers are generated after we applied the all-in-one yaml file.

* 1 for Ingress
* 2 for services

loadbalancers created in this test is almost the same as the ones created in
`simple fanout Ingress` test:

.loadbalancers
image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]

next we can check haproxy configuration file for `name-based virtual host
Ingress`.

===== examine `haproxy.conf` file

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
        ulimit-n 200000
        maxconn 65000
        stats socket /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.sock mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend acf8b96d-b322-4bc2-aa8e-0611baa43b9f

        option tcplog
        bind 10.47.255.248:80                   #<---Ingress loadbalancer podIP
        mode http
        option forwardfor

        #map www.juniper.net to backend "xxx4e6a681ec8e6", which maps to "webservice-1"
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host hdr(host) -i www.juniper.net
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path path /
        use_backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6 if
            77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host
            77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path

        #map URL www.cisco.net to backend "xxx44d1ca50a92f", which maps to "webservice-2"
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host hdr(host) -i www.cisco.net
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path path /
        use_backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f if
            1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host
            1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path

        #map other URLs, to default backend "xxx4e6a681ec8e6"
        default_backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46

backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6    #<---webservice-1
        mode http
        balance roundrobin
        option forwardfor
        server 33339e1c-5011-4f2e-a276-f8dd37c2cc51 10.101.158.92:8888 weight 1

backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f    #<---webservice-2
        mode http
        balance roundrobin
        option forwardfor
        server aa0cde60-2526-4437-b943-6f4eaa04bb05 10.104.4.232:8888 weight 1

backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46    #<---default
        mode http
        balance roundrobin
        option forwardfor
        server e8384ee4-7270-4272-b765-61488e1d3e9c 10.101.158.92:8888 weight 1
----

//NOTE: the config file is slightly formatted to make it fit to a page width.
//also we moved the default backend to the end.

here are the highlights:

* the haproxy `frontend` section defines each URL, or `host`, and its path. here
  the 2 hosts are `www.juniper.net` and `www.cisco.com`. both `path` is `/`. 
* the haproxy `backend` section defines the "servers", which is all `service` in
  our case. it has a format of `serviceIP:servicePort`, which is the 
  `service` we've created.
* `use_backend...if...` command in `frontend` section declares the ingress
  rules: `if` the request includes a specified URL and path, "use" the
  corresponding "backend" to forward the traffic
* `default_backend` defines the service that will act as the "default": it will
  be used when a haproxy receives a URL request that is has no explicit match in
  the defined rules.

this configuration file can be illustrated in this figure:

image::https://user-images.githubusercontent.com/2038044/63788692-e785e300-c8c3-11e9-92be-a37ac7d83104.png[]

through this configuration, the haproxy implemented our ingress:

////
.haproxy frontend:

* 10.47.255.248:80 is the frontend IP and port facing clients

.haproxy backend:
////

* `www.juniper.net` and `/` composes the full URL, request will be dispatched to
  `webservice-1` (`10.101.158.92:8888`)
* `www.cisco.net` and `/` composes the full URL, request will be dispatched to
   `webservice-2` (`10.104.4.232:8888`)
* other URLs goes to default backend which is service `webservice-1`. 

Next we'll verify these behaviors.

==== `Ingress` verification: from internal

////
we've explored a lot about ingress configuration and objects examination, now it
is time to verify the test result. since the `Ingress` serves both inside and
outside of the cluster, our verification will start from the cirros pod
inside of cluster, then from the Internet host outside of it.
////

.from inside of cluster

----
$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-m2272

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.google.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
                                    [giphy]
----

//we still use the `curl` command to trigger HTTP requests towards the ingress's
//loadbalancer IP. 
the return proves our `Ingress` works: the 2 requests towards
"juniper" and "cisco" URL is proxied to 2 different backend pods, through 2
backend services `webservice-1` and `webservice-2` respectively. the third request
towards "google" is a "unknown" URL which does not have a matching service in
`Ingress` configuration, so it goes to the default backend service -
`webservice-1` and reaches the same backend pod.

same rule applies to the fourth request. without given a URL via `-H`, `curl`
will fill `host` with the request IP address, `10.47.255.238` in this test, and
since that "URL" does not have a defined backend service so the default backend
service will be used. in our test, for each service we use backend pods spawned
by same Deployment, so the podIP in returned webpage tells who is who. except in
the second test the returned podIP `10.47.255.235` represent `webservice-2`, all
other three tests returns podIP for `webservice-1`, as expected.

==== `Ingress` verification: from external (Internet host)

From internet host's "desktop", we launch two chrome page side by side, and
input URLs `www.juniper.net` and `www.cisco.com`. keep refreshing the 2 pages
we can confirm "juniper" page is always returned by Deployment `webserver-1` pod
`10.47.255.236`, "cisco" page is always returned by Deployment `webserver-2` pod
`10.47.255.235`. we launch a third chrome page and input `www.google.com`, we
see "google" page is returned by the same pod serving "Juniper" URL.

.`name based virtual hosting Ingress`: access `www.juniper.net` from Internet host
image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

same result can be seen from `curl` also. 

.from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
                                    [giphy]

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-m2272

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
                                    [giphy]

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
                                    [giphy]
----

=== Multiple Ingress Controllers

we can have multiple ingress controllers along with contrail ingress
controller. please refer Ingress section in chapter 3 which has the
required info.

== packet flow in contrail: an end to end view

so far we've looked at `floating IP`, `service`, and `Ingress` in details.  You
probably found that all these objects are related to each other in certain
aspects. in contrail, both `service` and `Ingress` are implemented based on
`loadbalancers` (but with different `loadbalancer_provider` types).
conceptually, `Ingress` is designed based on `service`.  VIP of both type of
loadbalancers are implemented based on `floating IP`.  however, the forwarding
mode for `service` and `Ingress` are quite different. with `service` forwarding
it's "one hop" procedure: source NAT and source PAT take place in ingress
direction, destination NAT and destination PAT take places in egress direction;
while with `Ingress` forwarding, the traffic takes "two hops" to arrive the
destination: the request first goes to the active haproxy, which then start the
HTTP/HTTPS level proxy and do the `service` forwarding to reach the final pod.

in order to undrestand the detail packet flow in contrail kubernetes
environment, let's examine the end to end service access from external in our
`Ingress` lab setup. We'll examine the forwarding state step by step, starting
from Internet host, through gateway router, then active haproxy node, to the
final destination node and pod. 

after this section, you will get a deep understanding to the packet 
flow and be able to troubleshoot the forwarding plane problems in contrail
kubernetes environment. 

=== testing methodology

==== available tools

you've seen this figure in `Ingress` section:

.Ingress traffic flow: access from external
image::https://user-images.githubusercontent.com/2038044/61061427-3f2ac680-a3ca-11e9-9364-f11bea477319.png[]

earlier we looked at the external gateway router's VRF routing table and use the
next hops information to find out which node get the packet from the client. in
practice, very often you need to find out the same from the cluster and nodes
themselves. a contrail cluster typically comes with a group of tools that you
can use to inspect the packet flow and forwarding state. in `service` section
you've seen the usage of `flow`, `nh`, `vif`, etc. in this section we'll revisit
these tools. furthermore, we'll introduce some more useful tools to give
additional information about the packet flow.

the tools are:

* `curl`: with debug option
* `tcpdump`: packet capture tool
* `flow`: vrouter flow table
* `rt`: vrouter VRF routing table
* `nh`: vrouter route next hop information
* `vif`: vrouter interface information
* shell script

//we'll then conclude the ingress section by a figure showing the end to end flow.

==== "problem" of curl

//`curl` is a powerful tool and it has many options to support versatile useful
//features for testing HTTP protocol. 
the `curl` tool implementation will always close the TCP session right after the
HTTP request is responded. although this is a safe and clean behavior in
practice, it may bring some difficulties to our test.  in this lab we want to
"hold" a HTTP transaction and look into the details.  However, a TCP flow entry
in vrouter will also always be cleared when TCP session closes.  the problem is,
`curl` get its job done pretty fast. it open the session, send request, get
response, then close the session. in fact this process is too fast to allow us
any time to capture the flow table. as soon as you hit "enter" to send the
`curl` command, the command returns in less than 1 or 2 seconds. by the time you
type in `flow` command in compute node, everything is done and you end up with
empty table.  we may actually prefer the connection to remain open for a while
so we can take time to capture the flow table.

==== workarounds

there are some methods to workaround that. here are some examples:

.large file transfer

one method is to install a large file in the webserver and try to pull it with
`curl`, that way the file transfer process "holds" the TCP session.  we've seen
this method in "service" section.

.shell script

another method is to automate and repeat the test over and over. we can run a
shell script in compute node to collect flow table periodically, and another
script in Internet host to keep sending request with `curl`.  over the time we
will have a good chance to have the flow table captured in compute node at the
right moment.

for instance, a compute side script may look like:

----
while :; do flow --match 10.47.255.238; sleep 0.2; done
----

Internet host side script can be:

----
while :; do curl -H 'Host:www.juniper.net' 101.101.101.1; sleep 3; done
----
//lynx -stdin --dump | cat; sleep 3; done

.telnet
we can also make use of `telnet` protocol. first we establish the TCP connection
toward the URL's corresponding IP, then and manually input the HTTP GET command
to trigger the HTTP request. doing this allow you some period of time before the
server end times out and tear down the TCP connection.

In this section we'll use the "telnet method".

=== from Internet Host to gateway router

==== curl `-v`

first let's start from the client. earlier we've used `curl` tool a lot to
trigger HTTP requests for our test. `curl` supports extensive options for
various features. we've seen `-H` option which specify the `host` field in a
HTTP request. this time for debugging purpose we use another useful option -
`-v` in `curl` command:

----
[root@cent-client ~]# curl -vH 'Host:www.juniper.net' 101.101.101.1
* About to connect() to 101.101.101.1 port 80 (#0)
*   Trying 101.101.101.1...
* Connected to 101.101.101.1 (101.101.101.1) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Accept: */*
> Host:www.juniper.net
>
* HTTP 1.0, assume close after body
< HTTP/1.0 200 OK
< Content-Type: text/html; charset=utf-8
< Content-Length: 359
< Server: Werkzeug/0.12.1 Python/2.7.12
< Date: Tue, 02 Jul 2019 16:50:46 GMT
* HTTP/1.0 connection set to keep alive!
< Connection: keep-alive
<

<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    Vwebserver-1-846c9ccb8b-g65dg</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
* Connection #0 to host 101.101.101.1 left intact
----

with this option, it prints more verbose information about the HTTP interaction:

* `>` lines are the messages content that `curl` sent out
* `<` lines are message content that it receives from remote. 

from the interaction we see:

* `curl` sent a HTTP `GET` with path `/` to the FIP `101.101.101.1`, with `Host`
  filled with "juniper" URL. 
* it gets the response with code `200 OK`, indicating the request has succeeded.
* there are a bunch of other headers in the response that are not important for
  our test so we can skip. 
* the rest part of the response is the HTML source code of a returned web page. 
* the connection is closed immediately afterward.

==== telnet

from Internet host, telnet to Ingress podIP `101.101.101.1` and port `80`:

----
[root@cent-client ~]# telnet 101.101.101.1 80
Trying 101.101.101.1...
Connected to 101.101.101.1.
Escape character is '^]'.
----

now the TCP connection is established - we'll check what is the other end
in a while. next we'll send GET command and host header, then hit return:

----
GET / HTTP/1.1
Host: www.juniper.net
----

one more return indicates the end of the request, and will trigger a response
from the server:

----
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 359
Server: Werkzeug/0.12.1 Python/2.7.12
Date: Mon, 02 Sep 2019 04:05:44 GMT
Connection: keep-alive


<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    rc-webserver-1-g65dg</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
----

from this moment, we can collect flow table in both (compute) nodes for later
analysis.

=== from gateway router to Ingress public FIP: MPLS over GRE
//of active haproxy node: MPLS over GRE

earlier in Ingress section we've seen gateway router's routing table, from the
prefix "next hop" we can find out that the packet will be sent to active haproxy
node `cent222` via MPLSoGRE tunnel. 
another way to demonstrate the forwarding
flow is to collect the flow table in node `cent222`. 

////
----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
......
Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)

   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)
----
* a user from Internet Host sends a http request by typing the URL
  "http://www.juniper.net" and hit enter
* DNS resolves the host to FIP address
* via default route Internet Host send HTTP request to gateway router's VRF
* gateway router learns the VIP prefix in VRF with next-hop pointing to the
  compute node running active haproxy, in this case node `cent222`

////

----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
Flow table(size 80609280, entries 629760)

Entries: Created 586803 Added 586861 Deleted 1308 Changed 1367Processed 586803
Used Overflow entries 0
(Created Flows/CPU: 147731 149458 144549 145065)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   114272<=>459264       15.15.15.2:58282                                    6 (2->2)
                         101.101.101.1:80
(Gen: 3, K(nh):89, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):61,
 Stats:2/112,  SPort 50985, TTL 0, Sinfo 192.168.0.204)

   459264<=>114272       10.47.255.238:80                                    6 (2->5)
                         15.15.15.2:58282
(Gen: 1, K(nh):89, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):89,
 Stats:1/74,  SPort 60289, TTL 0, Sinfo 8.0.0.0)

(vrouter-agent)[root@cent222 /]$ nh --get 61
Id:61         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:3316       Vrf:0
              Flags:Valid, MPLSoGRE, Etree Root,
              Oif:0 Len:14 Data:f0 1c 2d 41 90 00 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:192.168.0.204
----

////
(vrouter-agent)[root@cent222 /]$ nh --get 89
Id:89         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:7          Vrf:2
              Flags:Valid, Policy, Etree Root,
              EncapFmly:0806 Oif:8 Len:14
              Encap Data: 02 c0 0a c1 e6 6c 00 00 5e 00 01 00 08 00
////

this flow table rephrases the same fact as what we've seen from gateway router's
VRF table:

* the first flow entry displays the source and destination of the http request,
  it is coming from Internet host (`15.15.15.2`) and lands the FIP in current
  node `cent222`

* `S(nh):61` in is the next hop to the source of the request - the Internet
  host.  this is similiar concept like the reverse path forwarding(RPF). vrouter
  always maintains the path toward the source of the packet in the flow.

* `nh --get` command resolves the nexthop 61 with more details, we see a
  `MPLSoGRE` flag is set, `Sip` and `Dip` is the two end of the GRE tunnel,
  they are current node and gateway router's loopback IP respectively. 
  
overall this confirms the request packet from Internet host traverses gateway
router, and via MPLSoGRE tunnel it hit the Ingress external VIP `101.101.101.1`.
NAT will happen and we'll look into it next.

=== from Ingress public FIP to Ingress podIP: NAT
//on active haproxy node: NAT
////
----
(vrouter-agent)[root@cent222 /]$ flow --match 101.101.101.1
Flow table(size 80609280, entries 629760)

Entries: Created 1856648 Added 1856785 Deleted 3015 Changed 3234Processed 1856648 Used Overflow entries 0
(Created Flows/CPU: 467916 472342 457241 459149)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead
Listing flows matching ([101.101.101.1]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)

(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.238
......
Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)
----
////

to verify the NAT operation, we only need to dig a little bit more out of the
same flow output.

* the `Action` flag, `N(D)` in the first entry and `N(S)` in the second,
  indicate the two type of NAT operations:
  - destination NAT - `DNAT`, destination FIP `101.101.101.1` which is the
    external Ingress will be translated to the Ingress internal VIP
  - source NAT - `SNAT`, source IP which is the internal Ingress VIP
    `10.47.255.238` will be translated to the Ingress external VIP
  
* in vrouter flow, the second flow entry is also called a "reverse flow" of the
  first one. it is the flow entry vrouter uses to send returning packet towards
  Internet host.  from Ingress loadbalancer's perspective it only uses
  `10.47.255.248` assigned from the default pod network as its source IP, it
  does not knows anything about the FIP. same thing for the external Intenet host, it
  only knows how to reach the FIP and has no clues about the private
  Ingress internal VIP. it is vrouter that is doing the two way NAT translations
  in between.

in summary, what the flow table of active haprox node `cent222` tells is that on
receiving the packet destined to the Ingress FIP, vrouter on node `cent222`
performs NAT and translates destination FIP (`101.101.101.1`) to the Ingress's
internal VIP (`10.47.255.238`).  after that the packet lands the Ingress
loadbalancer's VRF, where active haproxy process is watching. next HTTP proxy
will happen.

=== from Ingress podIP to service IP: MPLS over UDP
//on active haproxy node 

now the packet lands in Ingress loadbalancer's VRF and it is in the frontend of
the haproxy.  what the haproxy supposes to do is:

* haproxy listening on the frontend IP (Ingress internal podIP/VIP) see the packet
* haproxy checks the ingress rule programmed in its config file, decides that
  the requests need to be proxied to service IP of `webservice-1`. 
* vrouter checks the Ingress loadbalancer's VRF table and sees the prefix of
  `webservice-1` IP is learned from a destination node `cent333`. between compute
  node the forwarding path is programmed with MPLSoUDP tunnel, so it pushes a
  MPLS label and send it through MPLS over UDP tunnel.

to verify haproxy process packet processing details, we capture packets on
the physical interface of node `cent222`, where the active haproxy process is
running.

//image::https://user-images.githubusercontent.com/2038044/60518123-e1ea9300-9cae-11e9-82ec-d341e32e42c8.png[]
image::https://user-images.githubusercontent.com/2038044/60539848-aadea680-9cdb-11e9-8896-c4824d17dd9d.png[]

from the wireshark screenshot, we see clearly that:

* the HTTP request packet is "forwarded" to the service IP, which is the other
  node `cent333`. that is why we see underlay destination IP of the request is
  `10.169.25.21`
* sending overlay packets between compute node requirs MPLSoUDP tunnel. 

everything is working as expected.

."forward" vs "proxy"

if you are observant enough, you should have noticed something "weird" in
this capture. questions are:

* shouldn't the source IP address be the Internet host's IP `15.15.15.2`,
  instead of loadbalancer's frontend IP? 
* is the packet "forwarded" at all?
* is the transaction within the same TCP session from Internet host, accrossing
  gateway router and loadbalancer node `cent222`, all the way up to the backend
  pod sitting in node 'cent333`?

The answers are NO. the haproxy in this test is doing layer 7 (application
layer) loadbalancing. what it does is:

* establish TCP connection with Internet host and keep monitoring the HTTP request. 
* whenever it see an request coming in, it checks its rule and initiates a brand
  new TCP connection to the corresponding backend
* it "copies" the original HTTP request from Internet host and "paste" into the
  new TCP connection with its backend. 

so, to put it precisely, the http request is "proxied", not "forwarded".

//capture on haproxy interface
//image::https://user-images.githubusercontent.com/2038044/60540296-d1e9a800-9cdc-11e9-8914-fbe4fc59ae60.png[]

=== from service IP to backend pod IP: NAT

at the moment we know the http request is "proxied" to haproxy's backend. that
backend is a kubernetes `service`, and to reach the `service` the request is
sent to node `cent333` where all backend pod is sitting.

on destination node `cent333`, when packet comes in from Ingress internal IP
`10.47.255.238` toward the service IP `10.99.225.17` of `webservice-1`, vrouter
again translates the service IP to the backend podIP `10.47.255.236`. the
translation is a NAT operation, pretty much the same as what we've seen many
times earlier, whenever FIP is involved. 

packet capture on the backend pod interface also reveals the packet interaction
between the Ingress intenral IP and backend podID.

----
$ tcpdump -ni tapeth0-baa392 -v
12:01:07.701956 IP (tos 0x0, ttl 63, id 32663, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [S], cksum 0xd88d (correct), seq 2129282145, win 29200, options [mss 1420,sackOK,TS val 515783670 ecr 0,nop,wscale 7], length 0
12:01:07.702012 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.236.http > 10.47.255.238.51968: Flags [S.], cksum 0x1468 (incorrect -> 0x8050), seq 3925744891, ack 2129282146, win 28960, options [mss 1460,sackOK,TS val 515781436 ecr 515783670,nop,wscale 7], length 0
12:01:07.702300 IP (tos 0x0, ttl 63, id 32664, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [.], cksum 0x1f57 (correct), ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 0
12:01:07.702304 IP (tos 0x0, ttl 63, id 32665, offset 0, flags [DF], proto TCP (6), length 159)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [P.], cksum 0x6fac (correct), seq 1:108, ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 107: HTTP, length: 107
        GET / HTTP/1.1
        User-Agent: curl/7.29.0
        Accept: */*
        Host:www.juniper.net
        X-Forwarded-For: 15.15.15.2

12:01:07.702336 IP (tos 0x0, ttl 64, id 12224, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.51968: Flags [.], cksum 0x1460 (incorrect -> 0x1eee), ack 108, win 227, options [nop,nop,TS val 515781436 ecr 515783671], length 0
12:01:07.711882 IP (tos 0x0, ttl 64, id 12225, offset 0, flags [DF], proto TCP (6), length 69)
    10.47.255.236.http > 10.47.255.238.51968: Flags [P.], cksum 0x1471 (incorrect -> 0x5f06), seq 1:18, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 17: HTTP, length: 17
        HTTP/1.0 200 OK
12:01:07.712032 IP (tos 0x0, ttl 64, id 12226, offset 0, flags [DF], proto TCP (6), length 550)
    10.47.255.236.http > 10.47.255.238.51968: Flags [FP.], cksum 0x1652 (incorrect -> 0x1964), seq 18:516, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 498: HTTP
12:01:07.712152 IP (tos 0x0, ttl 63, id 32666, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [.], cksum 0x1ec7 (correct), ack 18, win 229, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712192 IP (tos 0x0, ttl 63, id 32667, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [F.], cksum 0x1ccb (correct), seq 108, ack 517, win 237, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712202 IP (tos 0x0, ttl 64, id 12227, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.51968: Flags [.], cksum 0x1460 (incorrect -> 0x1cd5), ack 109, win 227, options [nop,nop,TS val 515781446 ecr 515783681], length 0
----

this may not be very convincing in one aspect. the packet capture shows the
communication is between loadbalancer IP and podIP, that part is fine. problem
is this is solely from the pod's perspective "after" the NAT operation. it does
not shows what happens right before NAT.

//TODO: add something here

here is the flow table we captured before it's gone.

----
evrouter-agent)[root@cent333 /]$ flow --match 10.47.255.238
Flow table(size 80609280, entries 629760)
Entries: Created 482 Added 482 Deleted 10 Changed 10Processed 482 Used Overflow entries 0
(Created Flows/CPU: 163 146 18 155)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   403188<=>462132       10.47.255.236:80                                    6 (2->4)
                         10.47.255.238:57760
(Gen: 1, K(nh):23, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):23,
 Stats:2/140,  SPort 52190, TTL 0, Sinfo 4.0.0.0)

   462132<=>403188       10.47.255.238:57760                                 6 (2->2)
                         10.99.225.17:8888
(Gen: 1, K(nh):23, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:3/271,  SPort 65421, TTL 0, Sinfo 10.169.25.20)
----

obviously the second entry is triggered by the incoming request. haproxy follows
its rules inheritated from our Ingress definition and dispatchs the request of
"juniper" URL to `webservice-1`, whose IP:port is `10.99.225.17:8888`. vrouter see
the service IP and knows that supposes to go to backend podIP `10.47.255.236`.
it does NAT between the two IPs. 

finally, pod sees the HTTP request and responds back with a web page.

////
----
$ tcpdump -ni tapeth0-baa392
23:37:29.754864 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [S], seq 1528773587, win 29200, options [mss 1420,sackOK,TS val 384765722 ecr 0,nop,wscale 7], length 0
23:37:29.754922 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [S.], seq 953745157, ack 1528773588, win 28960, options [mss 1460,sackOK,TS val 384763489 ecr 384765722,nop,wscale 7], length 0
23:37:29.755247 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 0
23:37:29.755253 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [P.], seq 1:71, ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 70: HTTP: GET / HTTP/1.1
23:37:29.755291 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 71, win 227, options [nop,nop,TS val 384763489 ecr 384765724], length 0
23:37:29.766886 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [P.], seq 1:18, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 17: HTTP: HTTP/1.0 200 OK
23:37:29.767032 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [FP.], seq 18:516, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 498: HTTP
23:37:29.767188 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 18, win 229, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767210 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [F.], seq 71, ack 517, win 237, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767218 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 72, win 227, options [nop,nop,TS val 384763501 ecr 384765736], length 0
----
////

=== returning traffic

on the reverse direction, podIP runs webserver and responds with it's web page.
the response follows the reverse path of the request:

* pod responds to loadbalancer frontend IP, across MPLSoUDP tunnel
* vrouter on node `cent333` perform source NAT, translating podIP into service IP
* respond reaches to active haproxy running on node `cent222`
* haproxy terminate the tcp connection with backend pod, "copies" the http
  response, and "paste" into its connection with the remote Internet host
* vrouter on node `cent222` perform source NAT, translating loadbalancer
  frontend IP to FIP
* response is sent to gateway router, which forwards to Internet host
* Internet host gets the response.

== contrail multiple interface pod

=== multiple interface pod introduction

in Kubernetes cluster, typically each pod only has one network interface (except
the `loopback` interface). In reality, there are scenarios where multiple
interfaces are required. e.g. a vnf(virtual network function) typically needs a
"left", "right" and optionally a "management" interface to do network fucntions.
a pod may requires a "data interface" to carry the data traffic, and a "management
interface" for the reachability detection. Service Providers also tend to keep the
management and tenant networks independent for isolation, and management purpose.
Multiple interfaces provide a way for containers to be connected to multiple devices
in multiple networks simultaneously.

=== contrail as a CNI

////
As you probably already know containers use namespaces to isolate resources and
rate limit their use. Linux’s network namespaces are used to glue container
processes and the host networking stack. Docker spawns a container in the
containers own network namespace and later on runs a veth pair (a cable with two
ends) between the container namespace and the host network stack
////

in container technology, A veth(Virtual ETHernet) pair is functioning pretty much
like a virtual "cabel", that can be used to create tunnels between network namespaces.
one end of it is "plugged" in the container and the other end is in the host or docker
bridge. 
//it can also be used to create a bridge to a physical network device in another
namespace.

A "CNI plugin" is the one who is responsible for inserting the network interface
(that is one end of the veth pair) into the container network namespace and it will
also makes all necessary changes on the host. e.g. attaching the other end of the veth
into a bridge, assigning IP, configuring routes, and so on.

//TODO: need redraw
.container and veth pair
image::https://user-images.githubusercontent.com/2038044/60554760-ee9ad580-9d06-11e9-9628-f01af759f6e1.png[]


there are many such CNI plugin implementations that are publicly available
today. contrail is one of them.  for a comprehensive list you can check
https://github.com/containernetworking/cni where contrail is also listed.

`multus-cni`, is another CNI plugin that "enables attaching multiple
network interfaces to pods". multipe-network support of `multus-cni` is
accomplished by Multus calling multiple other CNI plugins. because each plugin
will create its own network, multiple plugins make the pod be able to have
multiple networks. one of the main advantages that contrail provides, comparing
with `mutus-cni` and all other current implementations in the industry, is that
contrail by itself provides the ability to attach multiple network interfaces to
a kubernetes pod, without the need to call any other plugins. this brings
support to a truly "multi-homed" pod.

===  NetworkAttachmentDefinition CRD

////
Kubernetes supports a custom extension to represent networks in its object
model, through its `CustomResourceDefinition(CRD)` feature. This extension adds
support for a new kind of object called `NetworkAttachmentDefinition`, which
represents a network in Kubernetes data model.

in contail, a CRD object defines the template for a network object
`NetworkAttachmentDefinition`, which contains all information about each
network's specification, and tells Kubernetes API how to understand and expose
it. 
////

contrail CNI follows the Kubernetes Network `CRD`(Custom Resource Definition)
- NetworkAttachmentDefinition to provide a standardized method to specify the
configurations for additional network interfaces. there is no change to the 
standard kubernetes upstream APIs, making the implementation coming with the
most compatibilities.

in contrail setup the NetworkAttachmentDefinition CRD is created by
`contrail-kube-manager`(`KM`). when bootup, `KM` validates if a network CRD
`network-attachment-definitions.k8s.cni.cncf.io` is found in the Kubernetes API
server, and creates one if not yet.

here is a `CRD` object yaml:

----
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: network-attachment-definitions.k8s.cni.cncf.io
spec:
  group: k8s.cni.cncf.io
  version: v1
  scope: Namespaced
  names:
    plural: network-attachment-definitions
    singular: network-attachment-definition
    kind: NetworkAttachmentDefinition
    shortNames:
    - net-attach-def
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            config:
             type: string
----

in contrail kubernetes setup, the CRD has been created and can be verified:

----
$ kubectl get crd
NAME                                             CREATED AT
network-attachment-definitions.k8s.cni.cncf.io   2019-06-07T03:43:52Z
----

////
----
$ kubectl get crd -o yaml
apiVersion: v1
items:
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    creationTimestamp: 2019-06-07T03:43:52Z
    generation: 1
    name: network-attachment-definitions.k8s.cni.cncf.io
    resourceVersion: "1170"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/network-attachment-definitions.k8s.cni.cncf.io
    uid: 77f15393-88d6-11e9-a8b1-0050569e6cfc
  spec:
    additionalPrinterColumns:
    - JSONPath: .metadata.creationTimestamp
      description: |-
        CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.

        Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
      name: Age
      type: date
    group: k8s.cni.cncf.io
    names:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    scope: Namespaced
    version: v1
    versions:
    - name: v1
      served: true
      storage: true
  status:
    acceptedNames:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    conditions:
    - lastTransitionTime: 2019-06-07T03:43:52Z
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: null
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
    storedVersions:
    - v1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----
////

using this new kind `NetworkAttachmentDefinition` created from the above CRD,
now we have the ability to create `vitual-network` in contrail kubernetes
environments.

to create a virtual-network from kubernetes, use a yaml template like this:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: <network-name>
  namespace: <namespace-name>
  annotations:
    "opencontrail.org/cidr" : [<ip-subnet>]
    "opencontrail.org/ip_fabric_snat" : <True/False>
    "opencontrail.org/ip_fabric_forwarding" : <True/False>
spec:
  config: '{
    “cniVersion”: “0.3.0”,
    "type": "contrail-k8s-cni"
}'
----

like many other standard kubernetes object, basically you specify the VN name,
namespace under `metadata`, and `annotations` which is used to carry additional
information about a network. in contrail the following annotations are used in
`NetworkAttachmentDefinition` CRD to enable certain attributes for the
virtual-network:

* `opencontrail.org/cidr`: CIDR, which defines the subnet for a VN
* `opencontrail.org/ip_fabric_forwarding`: a flag to enable/disable `ip fabric
  forwarding` feature
* `opencontrail.org/ip_fabric_snat`: a flag to enable/disable `ip fabric snat`
  feature

****
In contrail, `ip-fabric-forwarding` feature enables ip fabric based forwarding
without tunneling for the VN. When two `ip_fabric_forwrding` enabled virtual
networks communicate with each other, overlay traffic will be forwarded directly
using the underlay. 

With the Contrail `ip-fabric-snat` feature, pods that are in the overlay can
reach the Internet without floating IPs or a logical-router. The
`ip-fabric-snat` feature uses compute node IP for creating a source NAT to reach
the required services.

`ip fabric forwarding` and `ip fabric snat` features are not covered in this
book.
****

alternatively, you can define a new VN by referring an existing VN:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: extns-network
  annotations:
    "opencontrail.org/network" : '{"domain":"default-domain", "project": "k8s-extns", "name":"k8s-extns-pod-network"}'
spec:
  config: '{
    “cniVersion”: “0.3.1”,
    "type": "contrail-k8s-cni"
}'
----

throughout this book we'll use the first template to define our VNs in all
examples.

=== multiple-interface pod

with multiple VNs created, we can "attach" (you may also say "plug", or
"insert") any of them into a pod, with a pod yaml file like this:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      { "name": "VN-a" },
      { "name": "VN-b" },
      { "namespace": "other-ns", "name": "VN-c" }
    ]'
spec:
  containers:
----

another valid format:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: 'VN-a,VN-b,other-ns/VN-c'
spec:
  containers:
----

you probably notice, pods in a namespace not only can refer to the networks
defined in local NS, but also networks created on other namespaces using their
fully scoped name. this is very useful - the same network does not has to be
duplicated again and again in every NS that needs it, it can be defined just one
time and then referred anywhere else.

=== test multi-interface pod

we've understood the basic theories and explored the various templates. now it's
time to look at a "working example" in the real world. we'll start from
creating two VNs, examining the VN objects, then create a pod and attach the 2
VNs into it. we'll conclude the test and this section by examining the pod
interfaces and connectivity with other pods sharing the same VNs.

////
now you may want to test these theories in your setup
starting from creating your own yaml files based on these templates. if this is
the first time you work on this, you will most likely run into all kinds of
small issues here and there.
////

here is a yaml file of two VNs: `vn-left-1` and `vn-right-1`

----
$ cat vn-left-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "10.10.10.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-left-1
spec:
  config: '{ 
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

----
$ cat vn-right-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "20.20.20.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-right-1
  #namespace: default
spec:
  config: '{
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

create both VNs:

----
$ kubectl apply -f vn-left-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-left-1 created

$ kubectl apply -f vn-right-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-right-1 created
----

examine the VNs:

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-left-1       3s
vn-right-1      10s
----

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io vn-left-1 -o yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"k8s.cni.cncf.io/v1","kind":"NetworkAttachmentDefinition","metadata":{"annotations":{"opencontrail.org/cidr":"10.10.10.0/24","opencontrail.org/ip_fabric_forwarding":"false"},"name":"vn-left-1","namespace":"ns-user-1"},"spec":{"config":"{ \"cniVersion\": \"0.3.0\", \"type\": \"contrail-k8s-cni\" }"}}
    opencontrail.org/cidr: 10.10.10.0/24
    opencontrail.org/ip_fabric_forwarding: "false"
  creationTimestamp: 2019-06-13T14:17:42Z
  generation: 1
  name: vn-left-1
  namespace: ns-user-1
  resourceVersion: "777874"
  selfLink: /apis/k8s.cni.cncf.io/v1/namespaces/ns-user-1/network-attachment-definitions/vn-left-1
  uid: 01f167ad-8de6-11e9-bbbf-0050569e6cfc
spec:
  config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

the VNs are created, as expected. it seems nothing much exciting here. However,
if you login to the contrail UI, you will see something "unexpected". 

//old GUI:
//image::https://user-images.githubusercontent.com/2038044/59985880-f5886080-9601-11e9-98c9-791fec2fbe55.png[]

.contrail command: "main-menu" -> "virtual networks"

image::https://user-images.githubusercontent.com/2038044/60283772-f78b4180-98d7-11e9-9358-1ed47aeeef57.png[]

NOTE: make sure you select a correct "project", in this case it is `k8s-default`.

you won't be able to find any VN with the exact name `vn-left-1` or `vn-right-1`
in the UI. instead, what you will find are two VNs named
`k8s-vn-left-1-pod-network` and `k8s-vn-right-1-pod-network` got created. 

there is nothing wrong here. What happened is whenever a VN get created from
kubernetes, contrail automatically adds kubernetes cluster name(by default
`k8s`) as a prefix to the VN name that you give in the network yaml file, and a
suffix `-pod-network` in the end. This makes sense because we know a VN can be
created by different methods.  with these extra keywords embeded in the name, it
is easier to tell how the VN was created(from kubernetes or from the UI
manually), what will it be used for, etc. also potential VN name conflicts can
be avoided across multiple kubernetes clusters.

here is yaml file of the `cirros` pod - a container version of the
classis cirros VM.

----
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
----

in pod annotations under metadata, we insert 2 VNs: `vn-left-1` and
`vn-right-1`. Now guess how many interfaces will the pod has on bootup?  you may
think it will be two because that is what we gave in the file. let's create the
pod and verify:

----
$ kubectl get pod -o wide
NAME    READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros  1/1    Running  0         20s  10.47.255.238  cent222  <none>

$ kubectl describe pod cirros
Name:               cirros
Namespace:          ns-user-1
Priority:           0
PriorityClassName:  <none>
Node:               cent222/10.85.188.20
Start Time:         Wed, 26 Jun 2019 12:51:30 -0400
Labels:             app=cirros
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.250",
                              "mac": "02:87:cf:6c:9a:98",
                              "name": "vn-left-1"
                          },
                          {
                              "ips": "10.47.255.238",
                              "mac": "02:87:98:cc:4e:98",
                              "name": "cluster-wide-default"
                          },
                          {
                              "ips": "20.20.20.1",
                              "mac": "02:87:f9:f9:88:98",
                              "name": "vn-right-1"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ 
                        { "name": "vn-left-1" }, { "name": "vn-right-1" } ]
                    kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"Pod","metadata":
                      {"annotations":{"k8s.v1.cni.cncf.io/networks":"[
                      { \"name\": \"vn-left-1\" }, { \"name\": \"vn-...
Status:             Running
IP:                 10.47.255.238
...<snipped>...
----

in `Annotations`, under `k8s.v1.cni.cncf.io/network-status` we see a list
`[...]`, which has 3 items each represented by a curly brace block `{}` of
key-value mappings. each curly brace block includes information about one
interface: the allocated IP, MAC and the VN it belongs to. so you will end up to
have 3 interfaces created in the pod instead of 2.  please notice the second
item which gives IP address `10.47.255.238`, that is the interface attached to
the "default pod network" named "cluster-wide-default", which is created by the
sytem. you can look at the default pod network as a "managment" network because
it is always "up and running" in every pod's network namespace. funtionally
it is no much different with the VN you create - except that you can't delete
it.

we can "login to" the pod, list the interfaces and verify the IP and MAC.

----
$ kubectl exec -it cirros sh
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
37: eth0@if38: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:47:06:d8:98 brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.238/12 scope global eth0
       valid_lft forever preferred_lft forever
39: eth1@if40: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:6b:a0:e2:98 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.250/24 scope global eth1
       valid_lft forever preferred_lft forever
41: eth2@if42: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:8e:8a:80:98 brd ff:ff:ff:ff:ff:ff
    inet 20.20.20.1/24 scope global eth2
       valid_lft forever preferred_lft forever
----

we see one lo interface and 3 interfaces plugged by contrail CNI, each with the
IP allocated from the corresponding VN. also you will notice the MAC addresses
match what we've seen in `kubectl describe` command output. 

NOTE: having the MAC address in the annotations could be useful under certain
cases. for example, in "service chaining" section, you will run into a scenario
where you have to use the MAC address to locate the proper interface, so that
you can assign the right podIP which kubernetes allocated from a VN. check
"service chaining" section for more details.

////
NOTE: having the MAC address in the annotations could be useful under certain
cases. for example, in "service chaining" section, you will run into a scenario
when you need to use the MAC address to locate the proper interface, before you
can even tell which interface should be configured with which podIP that
kubernetes allocated from a VN. check "service chaining" section for more
details.

- you login a pod and for some reason you lose the track
of interface to VN mapping (e.g., you manually changed/removed the IPs, or the
pod's application reset the IP, etc) you can count on the MAC address! later In
////

you will see multiple-interfaces pod again in sevice-chaining example later on.
in that example the pod will be based on Juniper CSRX image instead of a general
docker image. but the basic idea remains the same.

== service chaining with CSRX

=== contrail service chaining introduction

service chaining is the idea of forwarding traffic through multiple network
entity in a certain order, each network entity do specific function such as
firewall, IPS , NAT , LB , …,etc. the legacy way of doing service chaining would
use standalone HW appliances which made service chaining inflexible, expensive
and takes a long time to setup. Dynamic service chaining is where network
functions deployed as VM or Container and could be chained automatically in a
logical way. in the next example we use contrail for services chaining between
two PODs in two different networking using CSRX container L4-L7 firewall to
secure the traffic between these two networks as shown in the figure:

.service chaining
//image::https://user-images.githubusercontent.com/2038044/60268925-85a4ff00-98bb-11e9-94c3-219d41038642.png[]
image::https://user-images.githubusercontent.com/2038044/63706136-ad9dd980-c7fc-11e9-9d73-0b76bbcb6979.png[]

[NOTE]
====
- left and right networks are just a common name used for simplicity and
  expected the traffic to follow from left to right but you can use your own
  names 
- make sure to configure the network before you attached a POD to it otherwise
  POD would fail to be created 
====

=== create VNs

so let’s start create two networks using this YAML files 

----
# cat vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.10.10.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-left
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'

# cat vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.20.20.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-right
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

----
# kubectl create -f vn-left.yaml
# kubectl create -f vn-right.yaml
----

Verify using Kubectl 
 
----
# kubectl get network-attachment-definition
NAME       AGE
vn-left    19d
vn-right   17d

# kubectl describe network-attachment-definition
Name:         vn-left
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.10.10.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-25T20:28:22Z
  Generation:          1
  Resource Version:    83111
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-left
  UID:                 a44fe276-7f2b-11e9-9ff0-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>


Name:         vn-right
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.20.20.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-28T07:14:02Z
  Generation:          1
  Resource Version:    380427
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-right
  UID:                 2b8d394f-8118-11e9-b36d-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>
----

It’s a good practice to confirm these two networks are seen now in contrail
before proceeding.  From the Contrail UI, select Configure > Networking >
Networks > default-domain > k8s-default, As shown in the figure which focus on
left network

NOTE: using `default` namespace in the YAML file for a network will create
it in domain “default-domain” and project “K8s-default”

image::https://user-images.githubusercontent.com/2038044/60268927-863d9580-98bb-11e9-965a-b50f91d811d1.png[]

=== create client pods

Create two ubuntu Pods, one in each network using the annotation object

----
# cat left-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: left-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-left" }]'
spec:
  containers:
    - name: ubuntu-left-pod-sc
      image: virtualhops/ato-ubuntu:latest
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN


# cat right-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: right-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-right" }]'
spec:
  containers:
    - name: ubuntu-right-pod-sc
      image: virtualhops/ato-ubuntu:latest
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN

# kubectl create -f right-ubuntu-sc.yaml
# kubectl create -f left-ubuntu-sc.yaml


# kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
left-ubuntu-sc    1/1     Running   0          25h
right-ubuntu-sc   1/1     Running   0          25h

# kubectl describe pod 
Name:               left-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:20 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.1",
                              "mac": "02:7d:b1:09:00:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.47.255.249",
                              "mac": "02:7d:99:ff:62:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }]
Status:             Running
IP:                 10.47.255.249
Containers:
  ubuntu-left-pod-sc:
    Container ID:   docker://2f9a22568d844c68a1c4a45de4a81478958233052e08d4473742827482b244cd
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
   
...<snipped>...

Name:               right-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 04:09:18 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.20.20.1",
                              "mac": "02:89:cc:86:48:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.252",
                              "mac": "02:89:b0:8e:98:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-right" }]
Status:             Running
IP:                 10.47.255.252
Containers:
  ubuntu-right-pod-sc:
    Container ID:   docker://4e0b6fa085905be984517a11c3774517d01f481fa43aadd76a633ef15c58cbfe
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
  
 ...<snipped>...
----

=== create CSRX pod

create Juniper CSRX container that have one interface on the left network and
one interface on the right network using this YAML file 

----
# cat csrx1-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: csrx1-sc
  labels:
    app: webapp-sc
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left" },
       { "name": "vn-right" }
   ]'
spec:
  containers:
  - name: csrx1-sc
    image: csrx
    ports:
    - containerPort: 22
    imagePullPolicy: Never
    stdin: true
    tty: true
    securityContext:
      privileged: true

# kubectl create -f csrx1-sc.yaml
----

Confirm the interface placement in the correct network 

----
# kubectl describe pod csrx1-sc
Name:               csrx1-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:31 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.2",
                              "mac": "02:84:71:f4:f2:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.20.20.2",
                              "mac": "02:84:8b:4c:18:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.248",
                              "mac": "02:84:59:7e:54:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }, { "name": "vn-right" } ]
Status:             Running
IP:                 10.47.255.248
Containers:
  csrx1-sc:
    Container ID:   docker://82b7605172d937895269d76850d083b6dc6e278e41cb45b4cb8cee21283e4f17
    Image:          csrx
    Image ID:       docker://sha256:329e805012bdf081f4a15322f994e5e3116b31c90f108a19123cf52710c7617e

...<snipped>...

----

NOTE: each container has one interface belong to “cluster-wide-default” network
regardless the use of the annotations object because annotations object above
creates and put one extra interface in a specific network 

=== verify podIP

.verify podIP

Login to the left, right Pods and the CSRX to confirm the IP/MAC address 
 
----
# kubectl exec -it left-ubuntu-sc bash
root@left-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
13: eth0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:99:ff:62:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.249/12 scope global eth0
       valid_lft forever preferred_lft forever
15: eth1@if16: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:b1:09:00:8d brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.1/24 scope global eth1
       valid_lft forever preferred_lft forever



# kubectl exec -it right-ubuntu-sc bash
root@right-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
23: eth0@if24: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:b0:8e:98:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.252/12 scope global eth0
       valid_lft forever preferred_lft forever
25: eth1@if26: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:cc:86:48:8d brd ff:ff:ff:ff:ff:ff
    inet 10.20.20.1/24 scope global eth1
       valid_lft forever preferred_lft forever


# kubectl exec -it csrx1-sc cli
root@csrx1-sc>
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d
----

NOTE: unlike other PODs the CSRX didn’t acquire IP with DHCP and it start with
factory default configuration hence it need to be configured. 

NOTE: By default, CSRX eth0 is visible only from shell and used for management.
And when attaching networks, the first attach network is mapped to eth1 which is
GE-0/0/1 And the second attach is mapped to eth2 which is GE-0/0/0

.configure CSRX IP
Configure this basic setup on the CSRX, to assign the correct IP address use the
MAC/IP address mapping from the “ kubectl describe pod” command show output as
well configure default security policy to allow everything for now 

----
set interfaces ge-0/0/1 unit 0 family inet address 10.10.10.2/24
set interfaces ge-0/0/0 unit 0 family inet address 10.20.20.2/24

set security zones security-zone trust interfaces ge-0/0/0
set security zones security-zone untrust interfaces ge-0/0/1 
set security policies default-policy permit-all 
commit
----

verify the IP address assigned on the CSRX

----
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

  Logical interface ge-0/0/1.0 (Index 100)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.10.10.0/24, Local: 10.10.10.2

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d

  Logical interface ge-0/0/0.0 (Index 200)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.20.20.0/24, Local: 10.20.20.2
----

=== ping test

From the Left POD try to ping the left POD, ping would fail as there is no route 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 1999ms

root@left-ubuntu-sc:/# ip r
default via 10.47.255.254 dev eth0 
10.10.10.0/24 dev eth1  proto kernel  scope link  src 10.10.10.1 
10.32.0.0/12 dev eth0  proto kernel  scope link  src 10.47.255.249
----

Adding static route to the left and right PODs and try to ping again 

----
root@left-ubuntu-sc:/# ip r add 10.20.20.0/24 via 10.10.10.2

root@right-ubuntu-sc:/# ip r add 10.10.10.0/24 via 10.20.20.2

root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
----

Still ping failed, as we didn’t create the service chaining which will also take
care of the routing. let’s see what happen to our packets 

----
root@csrx1-sc# run show security flow session 
Total sessions: 0
----

No session on the CSRX.  

=== troubleshooting ping issue

Login to the compute node “cent22” that host this container to dump the traffic
using tshark and check the routing To get the interface linking the containers 

----
[root@cent22 ~]# vif -l
Vrouter Interface Table

Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
       Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
       D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
       Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
       Uuf=Unknown Unicast Flood, Vof=VLAN insert/strip offload, Df=Drop New Flows, L=MAC Learning Enabled
       Proxy=MAC Requests Proxied Always, Er=Etree Root, Mn=Mirror without Vlan Tag, Ig=Igmp Trap Enabled

...<snipped>...

vif0/3      OS: tapeth0-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.252
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10760  bytes:452800 errors:0
            TX packets:14239  bytes:598366 errors:0
            Drops:10744

vif0/4      OS: tapeth1-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.1
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13002  bytes:867603 errors:0
            TX packets:16435  bytes:1046981 errors:0
            Drops:10805

vif0/5      OS: tapeth0-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.249
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10933  bytes:459186 errors:0
            TX packets:14536  bytes:610512 errors:0
            Drops:10933

vif0/6      OS: tapeth1-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.1
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:12625  bytes:1102433 errors:0
            TX packets:15651  bytes:810689 errors:0
            Drops:10957

vif0/7      OS: tapeth0-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.248
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:20996  bytes:1230688 errors:0
            TX packets:27205  bytes:1142610 errors:0
            Drops:21226

vif0/8      OS: tapeth1-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.2
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13908  bytes:742243 errors:0
            TX packets:29023  bytes:1790589 errors:0
            Drops:10514

vif0/9      OS: tapeth2-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.2
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:16590  bytes:1053659 errors:0
            TX packets:31321  bytes:1635153 errors:0
            Drops:10421

...<snipped>...

----

Note that Vif0/3 and Vif0/4 are bounded with the right POD and both linked to
tapeth0-89a4e2 and tapeth1-89a4e2 respectively same goes for the left POD for
Vif0/5 and vif0/6 while vif0/7, vif 0/8 and vif0/9 are bound with CSRX1.| from
that you can also see the number of packets/bytes hits that interface as well
the VRF which is this interface belong in here VRF 3 is for the
default-cluster-network while VRF 6 for the left network and VRF 5 for the right
network in this figure you can see the interface mapping from the all
prospective (container, Linux , vr-agent) 

//image::https://user-images.githubusercontent.com/2038044/60268930-863d9580-98bb-11e9-9dc3-b0c5598ff528.png[]
image::https://user-images.githubusercontent.com/2038044/63706210-d3c37980-c7fc-11e9-92b8-e5de88e22e76.png[]

try to ping again from the left POD to the right POD and use tshark on the tap
interface for the right POD for further inspection 

----
[root@cent22 ~]# tshark -i tapeth1-89a4e2
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-89a4e2'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.254 (Request)
  2 0.000037656 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.253 (Request)
  3 1.379993896 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Who has 10.20.20.1?  Tell 10.20.20.253
----

Looks like the ping isn’t reaching the right POD at all , lets see on the CSRX
left network tap interface  

----
[root@cent22 ~]# tshark -i tapeth1-844f1c
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-844f1c'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Who has 0.255.255.252?  Tell 0.0.0.0
  2 0.201392098   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=410/39425, ttl=63
  3 0.201549430   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  4 1.201444156   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=411/39681, ttl=63
  5 1.201600074   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  6 1.394074095 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.254 (Request)
  7 1.394108344 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.253 (Request)
  8 2.201462515   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=412/39937, ttl=63
----

We can see the packet but there is nothing in the CSRX security prospective to
drop this packet

checking the routing table of the left network VRF by logging to the
`vrouter_vrouter-agent_1` docker in the compute node 

----
[root@cent22 ~]# docker ps | grep vrouter
9a737df53abe        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest   "/entrypoint.sh /usr…"   2 weeks ago         Up 47 hours                             vrouter_vrouter-agent_1
e25f1467403d        ci-repo.englab.juniper.net:5000/contrail-nodemgr:master-latest         "/entrypoint.sh /bin…"   2 weeks ago         Up 47 hours                             vrouter_nodemgr_1

[root@cent22 ~]# docker exec -it vrouter_vrouter-agent_1 bash
(vrouter-agent)[root@cent22 /]$ 
(vrouter-agent)[root@cent22 /]$ rt --dump 6 | grep 10.20.20.
(vrouter-agent)[root@cent22 /]$
----

Note that 6 is the routing table VRF of the left network, same would goes for
the right network VRF routing table there is missing route 

    (vrouter-agent)[root@cent22 /]$ rt --dump 5 | grep 10.10.10.
    (vrouter-agent)[root@cent22 /]$

So even if all the PODs are hosted on the same compute nodes, they can’t reach
each other. And if these PODs are hosted on different compute nodes then you
have a bigger problem to solve. Service chaining isn’t about adjusting the routes
on the containers but mainly about exchange routes between the vrouter-agent
between the compute nodes regardless of the location of the POD, as well adjust
that automatically if the POD moved to another compute node. Before we build
service chaining lets address an important concerns for network administrator
who are not fan of this kind of CLI troubleshooting, can we do the same
troubleshooting using contrail controller GUI? 

the answer is yes and lets do it.

From the Contrail Controller UI, select monitor > infrastructure > virtual
router then select the node the that host the POD , in our case “Cent22.local” 

image::https://user-images.githubusercontent.com/2038044/60268931-863d9580-98bb-11e9-9682-d330878fa386.png[]

as shown in the figure from the interface tab which is equivalent to running “
vif -l” command on the vrouter_vrouter-agent-1 container and even showing more
information notice the mapping between the instance ID and tap interface naming
where the first 6 character of the instance ID are always reflected in the tap
interface naming

to check the routing tables of each VRF move to the “routes” tab and select the
VRF you want to see

image::https://user-images.githubusercontent.com/2038044/60268935-86d62c00-98bb-11e9-8eaa-820578b11127.png[]

If we select the left network ( the name is longer as it include the domain ,
project ) we can confirm there is not 10.20.20.0/24 prefix from the right
network We can also check the mac address learned in the left network by
selecting L2 ( which is equvilant to “rt --dump 6 --family bridge” command 

image::https://user-images.githubusercontent.com/2038044/60268936-86d62c00-98bb-11e9-9050-ca104b278a1a.png[]

=== create service chaining

Now lets utilize the CSRX to service chaining using contrail command GUI

creating Service chaining is 4 steps make sure to do them in this order 

1. create Service template 
2. creating service instance based on the service template you created before
3. creating network policy and select the service instance you created before
4. apply this network policy on network   

NOTE: since contrail command GUI is the solution to provide a single point of
management for all environments, we will use it to build service changing but
you still can use the normal contrail controller GUI to build service changing
 
Login to contrail command GUI ( in our setup https://10.85.188.16:9091/) then select service > catalog > create 

image::https://user-images.githubusercontent.com/2038044/60268937-86d62c00-98bb-11e9-8744-b8213b5246ed.png[]
image::https://user-images.githubusercontent.com/2038044/60268938-876ec280-98bb-11e9-991b-a54dedadfbcd.png[]
 
insret a name of services template “myweb-CSRX-CS” in here then chose v2 ,
virtual machine ( no other option available) for service mode we will work with
In-network and firewall as service type  

image::https://user-images.githubusercontent.com/2038044/60268941-876ec280-98bb-11e9-8f68-5c49af9b06d1.png[]

Select interfaces management, left and right then click create
 
image::https://user-images.githubusercontent.com/2038044/60268942-876ec280-98bb-11e9-8c7c-ac2a95da9ab0.png[]

Now select deployment and click create to create the service instances

image::https://user-images.githubusercontent.com/2038044/60268943-876ec280-98bb-11e9-9cf8-12b240de0286.png[]

Insert a name for this service instance then select from the drop down menu the
name of the template you created before then chose the proper network from the
prospective of the CSRX being the instance (container in that case) that will do
the service chaining and click on port tuples to expand it 

image::https://user-images.githubusercontent.com/2038044/60268945-88075900-98bb-11e9-87fa-375337170b12.png[]

then for each of the three interface bound one interface of the CSRX then click create

NOTE: the name of the virtual machine interface isn’t shown in the drop down
menu instead the instance ID, you can identify that from the tap interface name
as we showed before.  In other word all you have to know is most 6 left
character for any interface belong to that container as all the interface in a
given instance ( VM or container)  share the same first characters from the left 
 
Before you procced make sure the status of the three interfaces are up and they
are showing the correct IP address of the CSRX instance 
 
image::https://user-images.githubusercontent.com/2038044/60268947-88075900-98bb-11e9-9b0a-ecf0c6a03e33.png[]

To create network policy go to overlay > network policies > create 

image::https://user-images.githubusercontent.com/2038044/60268948-88075900-98bb-11e9-88ba-f5f7a02161b0.png[]
 
Insert a name for your network policy then in the first rule add left network as source network and right network as destination with action pass 

image::https://user-images.githubusercontent.com/2038044/60268949-889fef80-98bb-11e9-844a-326b5d506038.png[]

Select advanced option to attached the service instance you create before and click create 

image::https://user-images.githubusercontent.com/2038044/60268951-889fef80-98bb-11e9-84c5-f354b3d8938e.png[]

To attach this network policy to network click virtual network and select the left network and edit 

image::https://user-images.githubusercontent.com/2038044/60268953-889fef80-98bb-11e9-8826-2626a76c3d4a.png[]

In network policies select the network policy you just created from the drop down menu then click save 
do the same for the right network

image::https://user-images.githubusercontent.com/2038044/60268955-89388600-98bb-11e9-9605-14fbc8d30fbe.png[]

=== verify service chaining

Now lets check the effect of this service changing on routing 
From the Contrail Controller module control node (http://10.85.188.16:8143 in
oursetup), select monitor > infrastructure > virtual router then select the node
the that host the POD , in our case “Cent22.local” then select the “routes” tab
and select the left VRF 
 
image::https://user-images.githubusercontent.com/2038044/60268956-89388600-98bb-11e9-9e82-7d5fbddf38f8.png[]

Now we can the right networks host routes has been leaked to the left network
(10.20.20.1/32 , 10.20.20.2/32 in this case) 

Now let’s try to ping the right pod from the left pod to see the session created on the CSRX 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
64 bytes from 10.20.20.1: icmp_seq=1 ttl=61 time=0.863 ms
64 bytes from 10.20.20.1: icmp_seq=2 ttl=61 time=0.290 ms
^C
--- 10.20.20.1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.290/0.576/0.863/0.287 ms

root@csrx1-sc# run show security flow session 
Session ID: 5378, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/2 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/2;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 

Session ID: 5379, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/3 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/3;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 
Total sessions: 2

----

=== security policy

Now let try to create security policy on the CSRX to allow only http and https

----
root@csrx1-sc# show security 
policies {
    traceoptions {
        file ayma;
        flag all;
    }
    from-zone trust to-zone untrust {
        policy only-http-s {
            match {
                source-address any;
                destination-address any;
                application [ junos-http junos-https ];
            }
            then {
                permit;
                log {
                    session-init;
                    session-close;
                }
            }
        }
        policy deny-ping {
            match {
                source-address any;
                destination-address any;
                application any;        
            }                           
            then {                      
                reject;                 
                log {                   
                    session-init;       
                    session-close;      
                }                       
            }                           
        }                               
    }                                   
    default-policy {                    
        deny-all;                       
    }                                   
}                                       
zones {                                 
    security-zone trust {               
        interfaces {                    
            ge-0/0/0.0;                 
        }                               
    }                                   
    security-zone untrust {             
        interfaces {                    
            ge-0/0/1.0;                 
        }                               
    }                                   
}
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2000ms
----

the ping failed as the policy on the CSRX drop it 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:04:01 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/8->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5394 N/A N/A -1
Jun 14 23:04:02 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/9->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5395 N/A N/A -1
Try to send http traffic from the left to the right POD and verify the session status on the CSRX
root@left-ubuntu-sc:/# wget 10.20.20.1
--2019-06-14 23:07:34--  http://10.20.20.1/
Connecting to 10.20.20.1:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 11510 (11K) [text/html]
Saving to: 'index.html.4'

100%[======================================>] 11,510      --.-K/s   in 0s      

2019-06-14 23:07:34 (278 MB/s) - 'index.html.4' saved [11510/11510]
----

And in the CSRX we can see the session creation 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Adding resolved unicast NH. dest: 10.20.20.1, proto v4 (peer initiated)
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Sending resolve request for stale ARP entry (b). NH: 5507 dest: 10.20.20.1
Jun 14 23:07:34 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CREATE: session created 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 N/A(N/A) ge-0/0/1.0 UNKNOWN UNKNOWN UNKNOWN N/A N/A -1
Jun 14 23:07:35 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CLOSE: session closed TCP FIN: 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 14(940) 12(12452) 2 UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 UNKNOWN N/A N/A -1
----

