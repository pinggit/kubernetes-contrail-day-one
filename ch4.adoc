// vim:set ft=asciidoc cc=80 tw=80:
= Chapter 4: Kubernetes and Contrail integration 
:toc: right
:toclevels: 3
//:toc-placement: preamble
:source-highlighter: pygments
:source-highlighter: coderay
:source-highlighter: prettify
:highlightjs-theme: googlecode
:coderay-linenums-mode: table
:coderay-linenums-mode: inline
:numbered:

This chapter demonstrates how kubernetes objects works in contrail setup.  we'll
start with a section about contrail kubernetes integration architecture, where
you'll learn object mapping between kubernetes and contrail. those objects are
NS, pod, service, ingress, and network policy. then we'll look into
implementation of each of them in detail. to understand `service` exposure we
also introduce floating IP, which is part of the contrail's service and Ingress
implementation. multiple interface pod is a highlight of contrail's advantages
as one of kubernetes network CNI over other implementations, so we cover that
also. in the end, we conclude this chapter by demonstration of a service chain
using Juniper CSRX container.

== Contrail-Kubernetes architecture 
=== Why contrail with Kubernetes ?

Now after we have seen the main concepts of Kubernetes in chapter 2 and 3, what
could be the gain in adding Contrail to standard Kubernetes deployment ?

in brief, Contrail offers common deployment for multiple environments
(OpenStack, Kubernetes, etc) as well it enriches Kubernetes networking and
security capabilities.

.multiple environment integration

When it comes to deployment for multiple environments, Yes containers is the
current trend to build applications, but don’t expect everyone to migrate
everything from VM to containers that fast (This is not to mention the nested
approach where containers are hosted in VM). if we add to the picture
workload fully or partially run in the public cloud, we end up feeling the
misery for network and security administrators where Kubernetes becomes just
one thing to manage Network and security. 

administrator in many organization manage individual orchestrator/manager for
each environment. OpenStack or VMware NSX for VM, Kubernetes or Mesos for
Containers, AWS console.  and here what contrail could put the network and
security administrators out of their misery is it provides dynamic end-to-end
networking policy and control for any cloud, any workload, and any deployment.

from a single user interface contrail translates abstract workflows into
specific policies, simplifying the orchestration of virtual overlay connectivity
across all environments by building and securing virtual networks that connect
BMS, VM and Containers located in private or public cloud. 

A very common way to deploy Kubernetes is to launch its POD in VMs orchestrated
by OpenStack. this is one of the many use cases of contrail doing its magic.  in
this book we won’t cover contrail integration with other environments as we
focus only in Kubernetes. But any feature that we explain in here could be
extended for other environments.

.kubernetes enrichment

Then what we mean by contrail enriching standard Kubernetes deployment?

kubernetes by itself does not implement the networking, networking is handled in
a plugin called "Container Network Interface" - `CNI`. there are a lot of
opensourced CNI plugin by different providers in the industry. on the other
hand, kubernetes does impose some very fundamental requirements for any CNI
implementations. the most important part of the requirement is that pods on a
node must be able to communicate with all pods on all nodes **without** NAT.
that indicates a "flat" pod network. 

contrail is one of such a CNI plugin. you can refer to
https://kubernetes.io/docs/concepts/cluster-administration/networking/
for more available CNI plugins.

with CNI plugins, 
Kubernetes offers flat network connectivity with some security feature confined
in a cluster, but Contrail could offer on top of that:

. namespaces and services customized isolations for segmentations and
  multi-tenancy
. service chaining
. distributed loadbalancing and firewall with extensive centralized flow and
  logs insight 
. rich security policy using tags that can extend to other environment
  (OpenStack, VMWare, BMS, AWS ,..,etc) 

In this chapter we will cover some of these aspects, but first let’s talk about
Kubernetes/contrail architecture and the object mapping 

=== contrail-kube-manager

A new components of contrail has been added called `contrail-Kube-manager`,
abbreviated as `KM`. what it does basically is to watch kubernetes apiserver
events, and translate kubernetes objects into Contrail controller object. the
following figure illustratesthe basic work flow:

.contrail kubernetes architecture

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/kubemanager.png[]
//image::https://raw.githubusercontent.com/pinggit/kubernetes-contrail-day-one/master/diagrams/kubemanager.png?token=AAPRSHE5SF522ETPA6NAUDK5D7PHS[]

image::https://github.com/aymanaborabh/kubernetes-contrail-day-one/blob/master/diagrams/kube-manager-chapter%204.png[]

=== kubernetes to contrail object mapping

So not much of change of the regular contrail that we have seen before and all
of that is happening behind the scene.
what we have to be aware of it before dealing with Kubernetes/contrail is the
object mapping. because contrail is single interface managing multiple
environments - as explained before – each environment has its own acronym and
terms hence the need for this mapping
 
For example, Namespace in Kubernetes are intended for segmentation between
multiple teams, or projects as if we are creating virtual cluster. In contrail
the similar concept would be named as project so when you create a namespace in
Kubernetes it will automatically create an equivalent project in contrail. more
on that will come later on for now kindly make yourself familiar with this list
of object mapping 

.contrail kubernetes object mapping

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/chapter%204%20contrail%20-%20k8s%20mapping.png[]

image::https://user-images.githubusercontent.com/2038044/60748774-6bc08780-9f5f-11e9-91ae-2ec496cab987.png[]

////

=== contrail-kube-manager

.contrail

image::https://user-images.githubusercontent.com/2038044/59642949-fb2f0380-9134-11e9-86d2-1035e5b901b7.png[]

.kubernetes
image::https://user-images.githubusercontent.com/2038044/59642835-94a9e580-9134-11e9-9053-80505cb1ba75.png[]

.contrail kubernetes
image::https://user-images.githubusercontent.com/2038044/59642699-1a796100-9134-11e9-8a58-fb529b329cba.png[]

////


== contrail namespaces and isolation

=== namespace introduction

In chapter3 you`ve read about `namespace` or `NS` in kubernetes, and how to use
a `quota` to apply some constraints to the resource utilization by a NS. in the
beginning of this chapter we've mentioned object mappings between kubernetes and
contrail. in this section we'll see how NS works in contrail environments and
how contrail extends the feature.

one analogy we`ve given when introducing `namespace` concept is openstack
`project`, or `tenant`. that is exactly how contrail is looking at it. whenever
a new `namespace` object is created, `contrail-kube-manager` (KM) gets noticed
about the object creation event and it will create the corresponding `project`
in contrail api database. to differentiate between multiple kubernetes NS
project in contrail, a kubernetes cluster name will be added to the kubernetes
NS or project name. the default kubernetes cluster name is `k8s`.  so if you
create a kubernetes NS `ns-user-1`, what you will end up to see in contrail GUI
will be: `k8s-ns-user-1`:

.contrail command: projects
image::https://user-images.githubusercontent.com/2038044/60316467-8fb91300-9938-11e9-9de6-429b56429868.png[]

****
the kubernetes `cluster name` is configurable, during deployment process only.
if you don't configure it `k8s` will be the default. once the cluster is
created, the name can not be changed anymore. to view the `cluster name`, go to
`contrail-kube-manager` (KM) docker and check its the configuration file.

.to locate the `KM` docker container
----
$ docker ps -a | grep  kubemanager
2260c7845964  ...snipped...  ago  Up  2  minutes  kubemanager_kubemanager_1
----

.to login to the `KM` container
----
$ docker exec -it kubemanager_kubemanager_1 bash
----

.find the `cluster_name` option
----
$ grep cluster /etc/contrail/contrail-kubernetes.conf
cluster_name=k8s        #<---
cluster_project={}
cluster_network={}
----

****

NOTE: in the rest part of this book we will refer all these terms `namespace`,
`NS`, `tenant`, `project` interchangeably.

=== Non-isolated NS

you are aware that kubernetes basic networking requirement is a "flat"/"NATless"
network - any pod can talk to any pod in any namespace, any cni providers should
ensure that. consequently in kubernetes by default all namespaces are **not**
isolated. It is just like a "shared tenant" in openstack.

.k8s-default-pod-network and k8s-default-service-network

To provide networking for all non-isolated namespace, there should be a
**common** VRF (virtual routing and forwarding table) or RI (routing instance).
in contrail kubernetes environment, two "default" VNs are pre-configured in k8s
default NS project, for pod and service respectively. correspondingly there are
2 VRFs each with same name as their correspondingly VN. 

the name of the two VNs/VRFs are in this format:

    <k8s-cluster-name>-<namespace name>-[pod|service]-network

so for `default` NS with a default cluster name `k8s`, the two VN/VRF names will
become:

* `k8s-default-pod-network`: pod VN/VRF, with the default subnet `10.32.0.0/12`
* `k8s-default-service-network`: service VN/VRF, with a default subnet `10.96.0.0/12`

NOTE: the default subnet for pod or service is configurable.

it is important to know that these 2 default VNs are **shared** between all of
the "non-isolated" namespaces. what that means is, they will be available for
any new non-isolated NS that you create, implicitly.  that is why in any
non-isolated NS and `default` NS, the communication between pods in the default
pod network works just fine. 

NOTE: this only applies to the "default" VNs. all new VNs that you create will
be isolated with other VNs, regardless of same or different NS. additional
configuration is needed to make pods belonging two different VNs to communicate
with each other.

for the isolated NS, however, it will be a different story.

=== Isolated NS 

in contrast, "isolated" namespace, will have its own default pod-network and
service-network, accordingly two new VRFs are also created for each "isolated"
namspace. The same flat-subnets `10.32.0.0/12` and `10.96.0.0/12` are shared by
the pod and service networks in the isolated namespaces. however since the
networks are with a different VRF, by default it is isolated with other NS.
pods launched in isolated NS can only talk to service and pods on the same
namespace. Additional configurations, e.g. policy, is required to make the pod
being able to reach the network outside of current namespace.

to illustrate this concept let's take an example. suppose you have 3 namespaces,
the `default` NS and two user NS: `ns-non-isolated` and `ns-isolated`.
in each NS you create one user VN: `vn-left-1`. you will end up to have
following VN/VRFs:

.NS default

* default-domain:k8s-default:k8s-default-pod-network
* default-domain:k8s-default:k8s-default-service-network
* default-domain:k8s-default:k8s-vn-left-1-pod-network

.NS ns-non-isolated

* default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network

.NS ns-isolated

* default-domain:k8s-ns-isolated:k8s-ns-isolated-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-service-network
* default-domain:k8s-ns-isolated:k8s-vn-left-1-pod-network

////
* default-domain:k8s-default:k8s-default-pod-network:k8s-default-pod-network
* default-domain:k8s-default:k8s-default-service-network:k8s-default-service-network
* default-domain:k8s-default:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
* default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-pod-network:k8s-ns-isolated-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-service-network:k8s-ns-isolated-service-network
* default-domain:k8s-ns-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
////

this can be illustrated in below diagram:

.NS and VN
image::https://user-images.githubusercontent.com/2038044/63223271-13e18700-c181-11e9-8fe4-987cf935a05b.png[]

here is the yaml file to create an isolated namespace:

----
$ cat ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation" : "true"
  name: ns-isolated
----

to create the NS:

----
kubectl create -f ns-isolated.yaml

$ kubectl get ns
NAME          STATUS    AGE
contrail      Active    8d
default       Active    8d
ns-isolated   Active    1d  #<---
kube-public   Active    8d
kube-system   Active    8d
----

the annotations under metadata are something additional comparing to standard
(non-isolated) k8s namespace, the value of `true` indicates this is an isolated
NS:

  annotations:
    "opencontrail.org/isolation" : "true"

this part of the definition is Juniper's extension. `contrail-kube-manager`
(`KM`) , reads the namespace `metadata` from `kube-apiserver`, parses the
information defined in the `annotations` object, and sees that the `isolation`
flag is set to `true`. it then creates the tenant with the correponding routing
instances(one for pod and one for service) instead of using the default ns
routing instances for the isolated namespace. fundamentally that is how the
"isolation" is implemented. 

in the following sections we'll verify how the routing isolation works.

=== communication between pod in different namespaces

create a non-isolated namespace and an isolated namespace:

----
$ cat ns-non-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ns-non-isolated

$ cat ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation": "true"
  name: ns-isolated

$ kubectl apply -f ns-non-isolated.yaml
namespace/ns-non-isolated created

$ kubectl apply -f ns-isolated.yaml
namespace/ns-isolated created

$ kubectl get ns | grep isolate
ns-isolated       Active   79s
ns-non-isolated   Active   73s
----

in both NS and the default NS, create a deployment to launch a pod:

----
$ kubectl apply -f deployment-cirros.yaml -n default
deployment.extensions/cirros created

$ kubectl apply -f deployment-cirros.yaml -n ns-non-isolated
deployment.extensions/cirros created

$ kubectl apply -f deployment-cirros.yaml -n ns-isolated
deployment.extensions/cirros created

$ kubectl get pod -o wide -n default
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-tjfn6  1/1    Running  0         13s  10.47.255.242  cent333  <none>

$ kubectl get pod -o wide -n ns-non-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-nrxq6  1/1    Running  0         23s  10.47.255.248  cent222  <none>

$ kubectl get pod -o wide -n ns-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222  <none>
----

ping between all pods in 3 namespaces

----
#default ns to non-isolated new ns: succeed
$ kubectl -n default exec -it cirros1-85fc7dd848-tjfn6 -- ping 10.47.255.248
PING 10.47.255.248 (10.47.255.248): 56 data bytes
64 bytes from 10.47.255.248: seq=0 ttl=63 time=1.600 ms
^C
--- 10.47.255.248 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.600/1.600/1.600 ms

#default ns to isolated new ns: fail
$ kubectl -n default exec -it cirros1-85fc7dd848-tjfn6 -- ping 10.47.255.239
PING 10.47.255.239 (10.47.255.239): 56 data bytes
^C
--- 10.47.255.239 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
----

the test result shows that, bidirectional communication between two non-isolated
namespaces (namespace `ns-non-isolated` and `default` in this case) works, but
traffic from non-isolated NS (`default` NS) toward isolated NS does not pass
through. what about traffic within the same isolated NS? 

with the power of the `deployment` we can quickly test it out: in isolated NS
`ns-isolated`, clone one more pod by `scale` the deployment with `replicas=2`
and ping between the 2 pods:

----
$ kubectl scale deployment cirros --replicas=2
$ kubectl get pod -o wide -n ns-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222  <none>
cirros-85fc7dd848-215k8  1/1    Running  0         8s   10.47.255.238  cent333  <none>

$ kubectl -n ns-isolated exec -it cirros-85fc7dd848-6l7j2 -- ping 10.47.255.238
PING 10.47.255.238 (10.47.255.238): 56 data bytes
64 bytes from 10.47.255.238: seq=0 ttl=63 time=1.470 ms
^C
--- 10.47.255.238 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.470/1.470/1.470 ms
----

the ping packet passes through now. to summarize the test results: 

* traffic is isolated between an isolated NS and all other tenant in the cluster
* traffic is not isolated in same NS 

NOTE: pod-level isolation can be achieved via kubernetes network policy, or
security groups in contrail. 
this will be covered later in this chapter.

== contrail floating IP

//(with type of loadBalancer or nodePort) 

=== overlay Internet access

we've discussed and tested the communication between pods in the same or
different NS. so far we've only tested it **inside** of the same cluster. what
about communication with devices **outside** of the cluster? you may already
know that in traditional (openstack) contrail environment, there are many ways
for the overlay entities (typically a VM) to access the Internet, the 3
frequently used methods among them are:

* floating IP
* SNAT
* logical router

however, the offical kubernetes solution is via `service` and `Ingress` objects.
which you've read about and got the idea in chapter 3. in contrail kubernetes
environment, floating IP is used in the service and Ingress implementation to
expose them to outside of the cluster.  later in this chapter we'll have a very
detail discussion for each of these two objects. befor that, in this section,
we'll review the "floating IP" basis and look at how it works with kubernetes.

=== floating IP introduction

`floating IP`, or `FIP` for short, is a "traditional" concept that contrail
supports since very early releases. Essentially it is an openstack concept to
"map" a VM IP, which is typically a private IP address, to a public IP (the
"floating IP" in this context) that is reachable from the outside of the
cluster. Internally the one to one mapping is implemented by NAT. whenever a
vrouter receives packets from outside of the cluster destined to the floating
IP, it will translate it to the VM's private IP and forward the packet to the
VM. similarly it will do the translation on reverse direction. Eventually both
VM and Internet host can talk to each other, and both can initiate the
communication.

NOTE: vrouter is a contrail forwarding plane resides in each compute node handles
workloads traffic

the figure below illustrates the basic work flow of FIP:

.Floating IP
//image::https://user-images.githubusercontent.com/2038044/60388331-be8cd180-9a7d-11e9-8ff7-c202ed9f7349.png[]
//image::https://user-images.githubusercontent.com/2038044/60556767-b8faea00-9d10-11e9-84bb-0e40e3edcc3d.png[]
//image::https://user-images.githubusercontent.com/2038044/60357106-b448d580-99a0-11e9-8ad2-31e15102b6bd.png[]
//image::https://user-images.githubusercontent.com/2038044/63227026-0ee7fc00-c1b0-11e9-8e59-d247ec8d7b2e.png[]
image::https://user-images.githubusercontent.com/2038044/63263460-a4d66200-c256-11e9-8d83-012ae4a8ab26.png[]

here are some highlights regarding FIP and FIP pool configuration:

* a FIP is associated with a VM's `port`, or a `VMI` (Virtual Machine
  Interface).
* a FIP is allocated from a `FIP pool`
* a FIP pool is created based on a virtual network(`FIP-VN`)
* the `FIP-VN` will be available to outside of the cluster, by setting matching
  `route-target` (`RT`) attributes of gateway routers VRF table . 
* when a gateway router sees a match with its route import policy in the RT,
  it will load the route into its VRF table. all remote clients connected to
  the VRF will be able to communicate with the FIP.

Regarding the FIP concept and role, there is nothing new in contrail kubernetes
environment. But the usage of floating IP has been extended in kubernetes
`service` and `ingress` object implementation, and it plays an important role
for accessing toward kubernetes `service` and `ingress` from external. 
you can check later sections in this chapter for more details on this.

=== creating FIP pool

creating a FIP pool is a 3 steps process:

* create a public FIP-VN, 
* set `RT` (route-target) for the VN so it can be advertised and imported into
  the gateway router's VRF.
* create a FIP pool based on the public FIP-VN

again this is nothing new but the same steps as with other contrail environment
without kubernetes. however, as you've learned in previous section, with
kubernetes integration a FIP-VN can now be created in a "kubernetes style":

.create a public FIP-VN named `vn-ns-default`

----
$ cat vn-ns-default.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "101.101.101.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-ns-default
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "contrail-k8s-cni"
  }'

$ kubectl apply -f vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default unchanged

$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-ns-default   22d
----

.set the `RT`

if you need the FIP to be reachable from Internet through gateway router, you'll
need to set a route-target to make the VN prefix getting imported in the gateway
router's VRF table. this step is necessary whenever Internet access is required.

.contrail command: setting RT
image::https://user-images.githubusercontent.com/2038044/60751261-b43c6d00-9f80-11e9-93c5-b06aeb642eb0.png[]

NOTE: the UI navigation path to set RT is:
contrail command(CC): main-menu > Overlay > "Virtual Networks" >
k8s-vn-ns-default-pod-network > Edit > "Routing, Bridging and Policies"

****
.what is "CC"?
contrail command (CC) is the new user interface (UI) coming with the
contrail 5.0.1. throughout this book we use both CC and old UI to demonstrate
most of lab studies. just keep in mind that in the future CC will be the only UI
and the "legacy" one will be deprecated. 

.what is CC's "main-menu"?
in CC, the functions and settings are groups in a a "main menu". it is also the
entry point from where you can navigate through different functions. 

.where to find CC's "main-menu"?
image::https://user-images.githubusercontent.com/2038044/60282872-ed684380-98d5-11e9-92f7-e1df07c5fecf.png[]

in order to get this menu, click on group name right next to the "contrail
command" logo on the upper left of the UI. in the above screen capture that
group is "Infrastructure", but regardless it can be any group, just click it and
you will get the main menu, then from there you can select and jump into all
other settings.
****

////
NOTE: in the later lab demo of `service` or `ingress`, you always need to set the
RT to the public VN whenever they need to be accessed from Internet host, 
////

.create a FIP pool based on the public VN

this is the final step. from contrail command UI, Create a floating IP pool
based on the public VN:

.contrail command: create a FIP pool
image::https://user-images.githubusercontent.com/2038044/60357727-6d5bdf80-99a2-11e9-90c1-98b037cb0c98.png[]

NOTE: the UI navigation path for this setting is: contrail-command: main-menu >
Overlay > Floating IP > Create

TIP: in contrail UI, you can also set the "external" flag in VN "Advanced"
options so that a FIP pool named "public" will automatically be created.

=== FIP pool scope

there are different ways you can refer an floating IP pool in contrail
kubernetes environment, and correspondingly the scope of the pools will also be
different. here are 3 possible levels with descending priority:

* object specific
* Namespace level
* global level

.object specific

this is the most specific level of scope. object specific FIP pool binds itself
only to the object that you specified, it does not affect any other objects in
the same NS or the cluster. E.g. you can specify a service object `web` to get
FIP from FIP pool `pool1`, a service object `dns` to get FIP from another FIP
pool `pool2`, etc.  This gives the most granular control of where the FIP will
be allocated from for an object, the cost is that you need to explicitly specify
it in your yaml file for every object.

.NS level

in a NS, a "lazy" way to give FIP is to define a "NS level" FIP pool, so that
all objects created in that NS will get FIP assignment from that pool. 
with NS level pool defined (e.g. `pool-ns-default`), there is no need to
specify the FIP-pool name in each object's yaml file any more. you can still
give a different pool name, say `my-webservie-pool` in an object `webservice` ,
in that case object `webservice` will get the FIP from `my-webservice-pool`
instead of from the NS level pool `pool-ns-default`, because the former is more
specific.

.global level

a "even lazier" method is to define a "global" level pool, which means
the scope will be the whole cluster, including all namespaces. 

you can combine all 3 methods to take advantages of the flexibility. here is a
practical example:

* define a global pool `pool-global-default`, so any objects in a NS that has no
  NS-level or object-level pool defined, will get a FIP from this pool
* for NS `dev`, define a FIP pool `pool-dev`, so all objects created in NS `dev`
  will by default get FIP from `poo-dev`
* for NS `sales`, define a FIP pool `pool-sales`, so all objects created in NS
  `sales` will by default get FIP from `poo-dev`
* for NS `test-only`, do NOT define any NS level pool, so by default objects
  created in it will get FIP from the `pool-global-default`
* when a service `dev-websevice` in NS `dev` needs a FIP from `pool-sales`
  instead of `pool-dev`, specify `pool-sales` in `dev-webservice` object yaml
  file will achieve this goal.

NOTE: Just keep in mind the rule of thumb - the most specific scope will always
prevail.

=== global level FIP pool

to specify a global level FIP pool, you need to give the full
qualified pool name (domain > project > network) in configuration file for the
`contrail-kube-manager` (`KM`) docker container. 

the configuration file is `/etc/contrail/common_kubemanager.env` in master node:

----
$ cat /etc/contrail/common_kubemanager.env
VROUTER_GATEWAY=10.169.25.1
CONTROLLER_NODES=10.85.188.19
KUBERNETES_API_NODES=10.85.188.19
RABBITMQ_NODE_PORT=5673
CLOUD_ORCHESTRATOR=kubernetes
KUBEMANAGER_NODES=10.85.188.19
CONTRAIL_VERSION=master-latest
KUBERNETES_API_SERVER=10.85.188.19
TTY=True
ANALYTICS_SNMP_ENABLE=True
STDIN_OPEN=True
ANALYTICS_ALARM_ENABLE=True
ANALYTICSDB_ENABLE=True
CONTROL_NODES=10.169.25.19
----

as you can see, this `.env` file contains important environmental parameters
about the setup. to specify a `global FIP pool`, add following line in it:

----
KUBERNETES_PUBLIC_FIP_POOL={'domain': 'default-domain','name': 'pool-global-default','network': 'vn-global-default','project': 'k8s-ns-user-1'}
----

it reads: the global FIP pool is called `pool-global-default`, and it
is defined based on a VN `vn-global-default` under project `k8s-ns-user-1`.
which indicates that the corresponding kubernetes namespace is `ns-user-1`.

now with that piece of configuration placed, you can "re-compose" the
`contrail-kube-manager` docker container to make the change take effect.
essentially you need to tear it down and then bring it back up:

----
$ cd /etc/contrail/kubemanager/
$ docker-compose down;docker-compose up -d
Stopping kubemanager_kubemanager_1 ... done
Removing kubemanager_kubemanager_1 ... done
Removing kubemanager_node-init_1   ... done
Creating kubemanager_node-init_1 ... done
Creating kubemanager_kubemanager_1 ... done
----

now the global FIP pool is specified for the cluster.

=== NS level FIP pool

the next FIP pool scope is in NS level. each NS can define its own FIP
pool.  same way as kubernetes annotations object is used to give a subnet to a
VN, it is also used to specify a FIP pool. the yaml file looks:

----
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    opencontrail.org/isolation: "true"
    opencontrail.org/fip-pool: "{'domain': 'default-domain', 'name': 'pool-ns-default', 'network': 'vn-ns-default', 'project': 'k8s-ns-user-1'}"
  name: ns-user-1
----

in this example, NS `ns-user-1` is given a NS level FIP pool named
`pool-ns-default`, and the corresponding VN is `vn-ns-default`. once the NS
`ns-user-1` is created with this yaml file, any new service which requires an
FIP, if not created with the object-specific pool name in its yaml file, will
get a FIP allocated from this pool. In practice, most NS (especially
those isolated NS) will need its own NS default pool so you will see this
type of configuration very often in field.

==== object level FIP pool

the last one is object-specific pool. here is an example:

----
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb-pool-public-1
  annotations:
    "opencontrail.org/fip-pool": "{'domain': 'default-domain','name': 'pool-public-1','network': 'vn-public-1','project': 'k8s-ns-user-1'}"
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer
----

in this example, service `service-web-lb-pool-public-1` will get an FIP from
pool `pool-public-1`, which is created based on VN `vn-public-1` under current
project `k8s-ns-user-1`. the corresponding kubernetes NS is `ns-user-1`.

=== creating FIP

once FIP pool is created and available, an FIP can be allocated from the FIP
pool for the objects that requires one. this can be done either manually, by
associating an FIP to a VMI (VM or pod interface), or automatically by contrail
system when it sees the need.

.manually association

you can manually create a FIP out of a FIP pool in contrail UI, and then
associate it with a pod VMI.

.create a FIP manually

.create FIP
image::https://user-images.githubusercontent.com/2038044/61014424-567b9c80-a355-11e9-832e-3a7f33d2590e.png[]

.associate a FIP in a pod interface
image::https://user-images.githubusercontent.com/2038044/61014684-aa3ab580-a356-11e9-92e7-882e21dd6657.png[]

NOTE: make sure the FIP pool is shared to the project where FIP is going to be
created.

.automatically assignment

in some cases a FIP will be created automatically. later in this book, you will
see examples when contrail automatically assigns a FIP and associate it to
some kubernetes objects, for example:

* LoadBalancer service object
* Ingress object

in fact, `ClusterIP` by itself is also implemented by FIP. we'll check out more
details later.

=== advertising FIP

once a FIP is associated to a pod interface, it will be advertised to the MP-BGP
peers, which are typically gateway routers.

following screenshot shows how to add/edit a BGP peer.

.contrail command: select "main-menu" > INFRASTRUCTURE: "Cluster" > "Advanced Options"
image::https://user-images.githubusercontent.com/2038044/61074698-4c55ae80-a3e6-11e9-81d5-5efa962cbdb5.png[]

.contrail command: select "BGP router" > "create"
image::https://user-images.githubusercontent.com/2038044/63260144-2bd30c80-c24e-11e9-973a-aa911e7d2ae1.png[]

.edit BGP peer parameters
image::https://user-images.githubusercontent.com/2038044/61074999-0cdb9200-a3e7-11e9-80a3-b180d6454267.png[]

input all the BGP peer information, don't forget to associate the controller(s),
which is shown next:

.associate the peer to a controller
image::https://user-images.githubusercontent.com/2038044/61075110-4d3b1000-a3e7-11e9-8eec-ece0304ce4d8.png[]

from the dropdown of `peer` under `Associated Peers`, select the controller(s)
to peer with this new BGP router that you are trying to add.  click `save` when
done. a new BGP peer with ROUTER TYPE "router" will pop up.

.a new BGP router in the BGP router list
//image::https://user-images.githubusercontent.com/2038044/61074880-be2df800-a3e6-11e9-82af-7e58ccd7e710.png[]
image::https://user-images.githubusercontent.com/2038044/61079058-1289a580-a3f0-11e9-93a7-85eb53397a32.png[]

now we've added a peer BGP router as type "router". for local BGP speaker which
is with type "control-node", we just need to double check the parameters by
clicking `edit` button. in our test we want to build MP-IBGP neighborship
between contrail controller and gateway router, so we make sure the ASN and
"Address Families" matches on both end.

.contrail controller BGP parameters: ASN
image::https://user-images.githubusercontent.com/2038044/61075264-94c19c00-a3e7-11e9-90bd-6006dad35ef0.png[]

now you can check BGP neighborship status in gateway router.

----
labroot@camaro> show bgp summary | match 10.169.25.19
10.169.25.19          60100       2235       2390       0      39    18:19:34 Establ
----

once the neighborship is "Established", BGP routes will be exchanged between the
two speakers, that is the time we'll see that the FIP assigned to the kubernetes
object is advertised by master node (`10.169.25.19`) and learned in the gateway
router.

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.2
Jul 11 01:18:31

k8s-test.inet.0: 8 destinations, 8 routes (8 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.2/32   *[BGP/170] 00:01:42, MED 200, localpref 100, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 47
----

the `detail` version of same command tells more: the FIP route is reflected from
the contrail controller, but "Protocol next hop" being the compute node
(`10.169.25.20`) indicates that the FIP is assigned to a compute node. 
one entity currently running in that compute node own the FIP.

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.2 detail | match "next hop"
Jul 11 01:19:18
                Next hop type: Indirect, Next hop index: 0
                Next hop type: Router, Next hop index: 1453
                Next hop: via gr-2/3/0.32771, selected
                Protocol next hop: 10.169.25.20
                Indirect next hop: 0x900e640 1048601 INH Session ID: 0x70f
----

//in this capture the next hop is on `10.169.25.20`, node `cent222`. 
the dynamic soft GRE configuration make the gateway router automatically create
a soft GRE tunnel interface:

----
labroot@camaro> show interfaces gr-2/3/0.32771
Jul 11 01:19:53
  Logical interface gr-2/3/0.32771 (Index 432) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000 Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` indicates GRE outer IP header, so the "tunnel" is built from
current gateway router whose BGP local address is `192.168.0.204`, to remote 
node `10.169.25.20`, in this case it's one of the contrail compute nodes.

the FIP advertisement process is illustrated in this figure below:

.FIP advertisement
//image::https://user-images.githubusercontent.com/2038044/63262377-c5e98380-c253-11e9-996f-27eecb0df931.png[]
image::https://user-images.githubusercontent.com/2038044/63263090-a4899700-c255-11e9-8e76-cbee47c2faae.png[]

== contrail services

in this section, we look at kubernetes `service` in contrail environment.
specifically, we'll focus on `clusterIP` and `loadbalancer` type of services
that is commonly used in practice. contrail uses its `loadbalancer` object to
implement these two type of services. we'll first review the concept of legacy
contrail neutron loadbalancer, then we'll look into the extended ECMP
loadbalancer object which is the object that these two type of`service` are
based on in contrail, for the rest part of this section we will explore how
`clusterIP` and `loadbalancer` service works in detail, each with a test we
build in our testbed.

=== kubernetes service introduction

service is the core object in kubernetes. in chapter 3 you've learned what is
kubernetes service and how to create a `service` object with yaml file.
functional-wise, a service is running as a layer 4 (transport layer) load
balancer that is sitting between clients and servers. client can be anything
"requesting" a service. server in our context is the backend pods "responding"
the request. the client only sees the "frontend" - a service IP and service port
exposed by a service, it does not (and no need to) care about which backend pods
(and with what "pod IP") actually responds the service request. inside of the
cluster, that `service IP`, also called `cluster IP`, is a kind of virtual IP
(`VIP`). 

NOTE: in contrail environment it is implemented through floating IP.

This design model is very powerful and efficient in one sense that, it covers
the fragility of the possible single point failure that may be caused by
failure of any individual pod providing the service, therefore making a
`service` much more robust from client's perspective.

////
`pod` is the one doing the real work, and in kubernetes it is very "cheap" to
launch pods as needed. in chapter 3 you'll learned how fast it is to scale a rc
and deployment to control numbers of running pods dynamically. However, the
nature of a kubernetes pod is "mortal". to understand that just think of if a
screw of a chair breaks for whatever reason, you won't bother to "repair" it but
instead you just grab a new one.
////

in contrail kubernetes integration environment, all 3 types of services are
supported:

* clusterIP
* nodePort
* loadbalancer

next we'll introduce how service is implemented in contrail environment.

=== contrail service: ECMP loadbalancer

in chapter 3 we've introduced kubernetes default implementation of service
through `kube-proxy`. in there we mentioned CNI providers can have its own
implementations.

in contrail, `nodePort` service is still implemented by `kube-proxy` module.
however, `clusterIP` and `loadbalancer` services are implemented by contrail's
`loadbalancer` (`LB`).  

before we dive into the details of a service loadbalancer, it will be good to
review the legacy neutron based loadbalancer concept in contrail. 

TIP: for brevity we'll sometimes also refer `loadbalancer` as `LB`.

==== contrail neutron loadbalancer object

contrail loadbalancer is an relatively "old" feature that is supported since version 1.x.
it enables the creation of a pool of VMs serving applications, sharing one
virtual-ip (`VIP`) as the frontend IP towards clients. 
this diagram below illustrates contrail loadbalancer and its components.

.contrail neutron loadbalancer
image::https://user-images.githubusercontent.com/2038044/60641740-1f5c3700-9dfb-11e9-962f-ed67836d8115.png[]

some highlights of this figure:

* the LB is created with VIP `20.1.1.1`. the LB is listening to some application
  layer (layber 4) ports. for each layber 4 port it is listening, a `LB
  listener` is also created
* all backend VMs together compose a `pool`
* each backend VM in the `pool`, also called a `member`, is allocated an IP from
  subnet 30.1.1.0/24. 
* a client only sees one frontend VIP `20.1.1.1`, representing the whole service

how it works:
* when LB sees a request coming, it does TCP connection proxying. what that
  means is it establishes the TCP connection with the client, extracts the
  clients' HTTP/HTTPS requests, creates a new TCP connection towards one of the
  backend VMs from the pool, and send the request in the new TCP connection.
* when LB gets its response from the VM, it close the connection towards the
  backend VM and deliver the response to the original TCP connection towards the
  client.

you see that this loadbalancer model is very similar to kubernetes service
concept:

* VIP is the "service IP" 
* backend VM becomes backend pods
* members are added by kubernetes instead of neutron

in fact, contrail re-uses a good part of this model in kubernetes service
implementation. to support service loadbalancing, contrail extends the
loadbalancer with a new driver, with it service will be implemented as "equal
cost multiple path"(ECMP) loadbalancer working in layer 4(transport layer) .
this is the primary difference comparing with the "proxy" mode that the old
neutron loadbalancer type does.

NOTE: ingress, on the other hand, is conceptually even closer with the old
neutron loadbalancer in the sense that both are layer 7 (application
layer) "proxy" based. more about ingress will be discussed in later section.

==== contrail sevice loadbalancer object

let's take a look at service loadbalancer and the related objects.

.service loadbalancer
//image::https://user-images.githubusercontent.com/2038044/60640833-0f425880-9df7-11e9-91e1-9b0830394aaa.png[]
//image::https://user-images.githubusercontent.com/2038044/60677600-f87c2000-9e4f-11e9-8032-7cffd5f35da7.png[]
//TODO: redraw, add color
image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]

highlights in this figure:

* Each service is represented by a `loadbalancer` object. 
* the loadbalancer object comes with a `loadbalancer_provider` property. for
  service implementation a new `loadbalancer_provider` type called `native` is
  implemented.  
* for each sevice port a `listener` object is created for the same service `loadbalancer`
* for each `listener` there will be a `pool` object
* the `pool` contains `members`, depending on number of backend pod one pool may
  has multiple `members`
* each member object in the pool will map to one of pod backend

this is how service works in contrail:
* when a `custerIP` or `loadbalancer` type of `service` is created, a
  `loadbalancer` object with `loadbalancer_provider` property set to `native` is
  created
* `loadbalancer` will have a "virtual IP" `VIP`, which is same as the `service
  IP` 
* The `service-ip`/`VIP` will be linked to each backend pod's interface
* the linkage from service-ip to multiple backend pods interface creates an ECMP
  next-hop in contrail, traffic will be loadbalanced from the source pod towards
  one of the backend pod directly. later we'll show the ECMP prefix in the pod's
  VRF table
* `contrail-kube-manager` listens `kube-apiserver` for any changes, based on
  pod list in `Endpoints` it will knows the most current backend pods, and
  update them as members in the pool .

the most important thing to understand in this diagram, as we've mentioned, is
that in contrast to the legancy neutron loadbalancer (and the ingress
loadbalancer which we'll discussed later), there is no application layer "proxy"
in this process. contrail service implementation is based on layer 4 (transport
layer) ECMP based loadbalancing. 

////
detail discussions of the LB and all surrounding objects are out
of the scope of this book.
////

NOTE: technically, the LB has `VIP` only, but it also has a reference toward VMI
object which again has a reference to the `instance-ip`. the `instance-ip` is
the same IP as `service-ip`. to avoid confusions we won't cover these level of
implementation details in this book.

////
# k8s-5.md
Till 4.1, service ip is allocated from cluster-network even for isolated
namespaces. So, service from one isolated namespaces can reach service from
another isolated namespace. Security groups in isolated namespace prevents
reachability from other namespaces which also prevents reachablity from outside
of the cluster. In order to provide reachablity to external entity, the security
group would be changed to allow all which defeats the isolation. 

To address this, two virtual-networks would be created in the isolated
namespaces. One is for pods(pod-network) and another one is for
services(service-network). Contrail network-policy would be created between
pod-network and service-network for the reachablity between pods and services.
Service uses the same service-ipam which will be a flat-subnet like pod-ipam. It
is applicable for default namespace as well. Since virtual-networks are isolated
by default in contrail, services from one isolated namespace can not reach
service from another isolated namespace.
////

////

=== contrail clusterIP service

the `clusterIP` type of service is the most simple one. it is the default mode
if the `ServiceType` is not given. 

clusterIP service is exposed on a `clusterIP` and a service port. when client
pods need to access the service it sends request toward this clusterIP and
service port. service "binds" itself to certain backend pods via label mapping
between the two objects. `endpoint` is created for each service as long as there
is at least one matching pod available to be its backend. this model works great
if all requests are coming from the same cluster. the nature of the clusterIP
limits the scope of this service to be only within the same cluster. overall by
default the clusterIP is not reachable from external. 

////

==== navigating the service loadbalancer objects

we've talked a lot about the contrail "loadbalancer object" and you may wonder
what exactly it looks like. now we'll dig a little big deeper to look at the
loadbalancers and the supporting objects: listener, pool, members.

in contrail setup you can pull the object data either from contrail UI, CLI
(`curl`) or third party UI tools based on restapi. in production depending on
which one is available and handy you can select your favorite. 

.explore loadbalancer object with `curl`

with `curl` tool you
just need a FQDN of the URL pointing to the object. 

e.g.: to find the loadbalancer object URL for the service
`service-web-clusterip` from loadbalancers list:

----
$ curl http://10.85.188.19:8082/loadbalancers | \
    python -mjson.tool | grep -C4 `service-web-clusterip`
        {
            "fq_name": [
                "default-domain",
                "k8s-ns-user-1",
                "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
            ],
            "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
            "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        },
----

now with one specific loadbalancer URL, you can pull the specific LB object
details:

----
$ curl \
    http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc \
    | python -mjson.tool
{
    "loadbalancer": {
        "annotations": {
            "key_value_pair": [
                {
                    "key": "namespace",
                    "value": "ns-user-1"
                },
                {
                    "key": "cluster",
                    "value": "k8s"
                },
                {
                    "key": "kind",
                    "value": "Service"
                },
                {
                    "key": "project",
                    "value": "k8s-ns-user-1"
                },
                {
                    "key": "name",
                    "value": "service-web-clusterip"
                },
                {
                    "key": "owner",
                    "value": "k8s"
                }
            ]
        },
        "display_name": "ns-user-1__service-web-clusterip",
        "fq_name": [
            "default-domain",
            "k8s-ns-user-1",
            "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        ],
        "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "id_perms": {
            ...<snipped>...
        },
        "loadbalancer_listener_back_refs": [    #<---
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/loadbalancer-listener/3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc-TCP-8888-3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
                ],
                "uuid": "3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
            }
        ],
        "loadbalancer_properties": {
            "admin_state": true,
            "operating_status": "ONLINE",
            "provisioning_status": "ACTIVE",
            "status": null,
            "vip_address": "10.105.139.153",    #<---
            "vip_subnet_id": null
        },
        "loadbalancer_provider": "native",      #<---
        "name": "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "parent_href": "http://10.85.188.19:8082/project/86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "parent_type": "project",
        "parent_uuid": "86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "perms2": {
            ...<snipped>...
        },
        "service_appliance_set_refs": [
            ...<snipped>...
        ],
        "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "virtual_machine_interface_refs": [
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/virtual-machine-interface/8d64176c-9fc7-491a-a44d-430e187d6b52",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "k8s__Service__service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
                ],
                "uuid": "8d64176c-9fc7-491a-a44d-430e187d6b52"
            }
        ]
    }
}
----

the output is very extensive and includes a whole bunch of details that may not
be of our interests at this moment. but it does tell something interesting:

* in "loadbalancer_properties", the LB use service IP as its VIP
* the LB is connected to a listener by a reference
* `loadbalancer_provider` attribute is `native`, this is a new extension to
  implement layer 4 (transport layer)  ECMP for kubernetes service

.explore LB from UI

in the rest part of the exploration to LB and its related objects, we'll use the
legacy contrail UI.

TIP: you can also use the new contrail command UI to do the same.

for each service there is a LB object, in the below capture it shows 2 LB
objects:

* `ns-user-1-service-web-clusterip`
* `ns-user-1-service-web-clusterip-mp`

.loadbalancer object list
image::https://user-images.githubusercontent.com/2038044/60685179-a0edac80-9e6f-11e9-98c1-e2db001df543.png[]

this indicates 2 services were created. the service loadbalancer object's name
is composed by connecting NS name with service name, hence we can tell the
2 service's name:

* `service-web-clusterip` 
* `service-web-clusterip-mp`

==== loadbalancer

click on the small triangle icon in left of the first loadbalancer object
`ns-user-1-service-web-clusterip` to expand it, then click on `advanced json
view` icon on the right, you will see the similar detail information as what
you've seen in `curl` capture. for example the `VIP`, `loadbalancer_provider`,
`loadbalancer_listener` object that refers it, etc. 

from here you can keep expanding the `loadbalancer_listener` object by clicking
the `+` character to see the detail information of it. you then see a
`loadbalancer_pool`, expand it again you will see `member`. you can repeat this
process to explore through the object data. by the reference all
of these objects are connected to each other and work together.

.loadbalancer
image::https://user-images.githubusercontent.com/2038044/60685370-bca58280-9e70-11e9-8030-2746766082c8.png[]

==== listener

click on the LB name and select "listener", then expand it and click on
`advanced json view` icon on the right, you will get the listener details. the
listener is listening on service port 8888, and it is referenced by a `pool`.

.listener
image::https://user-images.githubusercontent.com/2038044/60685556-b368e580-9e71-11e9-820f-47fb25aacee4.png[]

==== pool and member
just repeat the exploring process we will get down to the pool and two
`members` in it. the member is with a port of `80`, which maps to the container
targetPort in pod.

.pool
image::https://user-images.githubusercontent.com/2038044/60685626-15c1e600-9e72-11e9-8539-a24ea28b0bf3.png[]

.members
image::https://user-images.githubusercontent.com/2038044/60685682-6fc2ab80-9e72-11e9-804d-5eccd8e055df.png[]

next we'll examine the vrouter VRF table for the pod to show contrail service
loadbalancer ECMP operation details. in order to better understand the "1 to N"
mapping between loadbalancer and listener shown in the loadbalancer object
figure, we'll also give an example of a "multiple port service" in
our setup.  we'll conclude the ClusterIP service section by inspecting the
vrouter flow table to illustrate the service packet workflow.

=== service testbed

before starting our investigation, let's look at our testbed. in this book we
build a setup including the following devices, most of our case studies are
based on it:

* one cenos server running as k8s `master` and contrail controllers
* two cenos servers, each running as a k8s `node` and contrail vrouter
* one Juniper QFX switch running as the underlay "leaf"
* one Juniper MX router running as a gateway router, or a "spine"
* one centos server runs as an Internet host machine

the digaram is here:

//image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]
image::https://user-images.githubusercontent.com/2038044/63596670-bcc92100-c589-11e9-99f1-7340a24cc8fd.png[]


NOTE: To minimize the resource utilization, all "servers" are actually centos
virtual machines created by vmware ESXI hypervisor running in one physical HP
server. this is also the same testbed for ingress.

in appendix you will find all details about the setup. the prerequisites,
software/hardware specifications, sample configuration files, and installation
steps. following the steps you will be able to build a same setup in your lab.

=== contrail ClusterIP service

in chapter 3 we've demonstrated how to create and verify a clusterIP service. in
this section we'll revisit the lab and look at some important details about
contrail specific implementations. we'll continue and add a few more tests to
illustrate the contrail service loadbalancer implementation details.

==== ClusterIP as FIP

let's review what we got from service lab in chapter3:

----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE  SELECTOR
service-web-clusterip  ClusterIP  10.105.139.153  <none>       8888/TCP  45m  app=webserver
----

----
$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ...  IP             NODE     ...  LABELS
cirros                      1/1    Running  ...  10.47.255.237  cent222  ...  app=cirros
webserver-846c9ccb8b-g27kg  1/1    Running  ...  10.47.255.238  cent333  ...  app=webserver
----

////
----
$ kubectl get pod -o wide --show-labels
NAME                              READY STATUS   ... IP             NODE     ... LABELS
cirros                            1/1   Running  ... 10.47.255.237  cent222  ... app=cirros
webserver-846c9ccb8b-kvwvw 1/1   Running  ... 10.47.255.238  cent333  ... app=webserver
----
////

here we see one service is created, with one pod running as its backend. the
label in the pod matches to the SELECTOR in service. the pod name also indicates
this is a deploy-generated pod. later we can scale the deploy for ECMP case
study, for now we'll stick to one pod and examine the ClusterIP implementation
details.

in contrail, a `ClusterIP` is essentially implemented in the form of a FIP. once
a service is created, a FIP will be allocated from the service subnet and
associated to the backend pod VMI and its `fixed IP`. this FIP/clusterIP is also
acting as a "VIP" to the client pods inside of the cluster. 

NOTE: this holds true also for loadbalancer type of service, in that case
contrail will allocate a second FIP - the "EXTERNAL-IP" as the VIP, and the
external VIP is advertised outside of the cluster through gateway router.  you
will get more details about these later.

from UI we'll see the automatically allocated FIP as ClusterIP.

.ClusterIP as FIP
image::https://user-images.githubusercontent.com/2038044/60973473-57c9ac80-a2f6-11e9-81a7-df74349e9877.png[]

the FIP is also associated with the pod VMI and podIP, in this case the VMI is
representing the pod interface.

.pod interface
image::https://user-images.githubusercontent.com/2038044/60975990-df191f00-a2fa-11e9-9f81-e635c141c7e6.png[]

the interface can be expanded to display more details:

.pod interface detail
//image::https://user-images.githubusercontent.com/2038044/63632000-c6d83780-c5fc-11e9-92a6-6bed7f09a944.png[]
image::https://user-images.githubusercontent.com/2038044/63632051-87f6b180-c5fd-11e9-8695-9ec6fc7c88ca.png[]

expand the `fip_list`, we'll see more information below:

----
fip_list:  {
    list:  {
        FloatingIpSandeshList:  {
            ip_addr: 10.105.139.153
            vrf_name: default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
            installed: Y
            fixed_ip: 10.47.255.238
            direction: ingress
            port_map_enabled: true
            port_map:  {
                list:  {
                    SandeshPortMapping:  {
                    protocol: 6
                    port: 80
                    nat_port: 8888
                    }
                }
            }
        }
    }
}
----

service/clusterIP/FIP 10.105.139.153 maps to podIP/fixed_ip 10.47.255.238.  the
`port_map` tells that port `8888` is a `nat_port`, `6` is the protocol number so
it means protocol TCP. overall, clusterIP:port `10.105.139.153:8888` will be
translated to podIP:targetPort `10.47.255.238:80` and vice versa.

now you understand with FIP representing ClusterIP, NAT will happen in service.
later we'll examine NAT again in the flow table.

==== scale backend pods
in chapter 3 clusterIP service example, we have created a sevice and a backend
pod. to verify the ECMP, let's increase the replica to 2 to generate a second
backend pod. this is a more realistic and rebost model: each pod will now be
backing up each other to avoid a single point failure.

instead of using yaml file to manually create a new webserver pod, with the
"kubernetes spirit" in mind you should think of to `scale` a Deployment,
as what you`ve seen earlier in this book. in our service example we`ve been
using `Deployment` object to spawn our webserver pod on purpose:

----
$ kubectl scale deployment webserver --replicas=2
deployment.extensions/webserver scaled

$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ... IP             NODE     ... LABELS
cirros                      1/1    Running  ... 10.47.255.237  cent222  ... app=cirros
webserver-846c9ccb8b-7btnj  1/1    Running  ... 10.47.255.236  cent222  ... app=webserver
webserver-846c9ccb8b-g27kg  1/1    Running  ... 10.47.255.238  cent333  ... app=webserver

$ kubectl get svc -o wide
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service-web-clusterip   ClusterIP   10.105.139.153   <none>        8888/TCP   45m   app=webserver
----

immediately after you create a new webserver pod by scaling the deployment with
`replicas 2`, a new pod is launched.  we end up having 2 backend pods now, one
is running in same node `cent222` as the client cirros pod, or a "local" node
for cirros pod; the other one is running in the other node `cent333` - the
"remote" node from client pod's perspective.  and the `endpoint` objects get
updated to reflect the current set of backend pods behind the `service`.

----
$ kubectl get ep -o wide
NAME             ENDPOINTS                           AGE
service-web-lb   10.47.255.236:80,10.47.255.238:80   20m
----

NOTE: without `-o wide` option, only first endpoint will be displayed.

we go ahead and check the FIP again.

.ClusterIP as FIP (ECMP)
image::https://user-images.githubusercontent.com/2038044/60973157-b2163d80-a2f5-11e9-957a-438642355391.png[]

we see the same FIP, but now it is associated with two podIP, each representing
a seperate pod. 

==== ECMP routing table: control node perspective

first, to examine the ECMP, let's take a look at the routing table in the
controller's routing instance.

.control node routing instance table
image::https://user-images.githubusercontent.com/2038044/60966312-ee41a200-a2e5-11e9-8966-053f0bbc20ea.png[]

the routing instance (RI) has a full name with the following format:

    <DOMAIN>:<PROJECT>:<VN>:<RI>

in most cases RI inheritate the same name from it's VN, so in our case the
full IPv4 routing table has this name:
`default-domain:k8s-ns-user-1:k8s-ns-user-1-pod-network:k8s-ns-user-1-pod-network.inet.0`
the `.inet.0` indicate the routing table type is unicast IPv4. there are many
other tables which is not of our interests right now.

two routing entries with the same exact prefixes of the ClusterIP show up in the
routing table, with two different next hops, each pointing to a different node.
this gives a hint about the route propagation process: both nodes(compute) has
advertised the same clusterIP toward the master(contrail controller), to
indicate the presence of the running backend pods in itself. this route
propagation is via XMPP. master(contrail controller) then reflect the routes to
all other compute nodes.

==== ECMP routing table: compute node perspective

next, starting from the client pod node `cent222`, we'll look at the the pod's
VRF table to understand how the packets are forwarded towards the backend pods

.vrouter vrf table
image::https://user-images.githubusercontent.com/2038044/60680116-18174680-9e58-11e9-9235-48c152959df7.png[]

the most important part of the screenshot is the routing entry `Prefix:
10.105.139.153 / 32 (1 Route)`, it is our ClusterIP address. underneath the
prefix there is a statement `ECMP Composite sub nh count: 2`. this indicates the
prefix has multiple possible next hop to reach. now expand it by clicking the
small triangle icon in the left, you will be given a lot more details about this
prefix.

.vrouter ECMP nexthop
image::https://user-images.githubusercontent.com/2038044/60680345-ece12700-9e58-11e9-9793-2b609918e146.png[]

we won't cover all details in this outputs. the most important thing that is of
our focus is `nh_index: 87`, which is the next hop ID (`NHID`) for the clusterIP
prefix. from vrouter agent docker, we can further resolve the "Composite" NHID to 
the sub-NHs (member nexthops):

----
[2019-07-04 12:42:06]root@cent222:~
$ docker exec -it vrouter_vrouter-agent_1 nh --get 87
Id:87         Type:Composite      Fmly: AF_INET  Rid:0  Ref_cnt:2          Vrf:2
              Flags:Valid, Policy, Ecmp, Etree Root,
              Valid Hash Key Parameters: Proto,SrcIP,SrcPort,DstIp,DstPort
              Sub NH(label): 51(25) 37(59)              #<---

Id:51         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:18         Vrf:0
              Flags:Valid, MPLSoUDP, Etree Root,        #<---
              Oif:0 Len:14 Data:00 50 56 9e e6 66 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:10.169.25.21

Id:37         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:5          Vrf:2
              Flags:Valid, Etree Root,
              EncapFmly:0806 Oif:8 Len:14               #<---
              Encap Data: 02 30 51 c0 fc 9e 00 00 5e 00 01 00 08 00

$ vif --get 8
Vrouter Interface Table

Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
       Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
       D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
       Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
       Uuf=Unknown Unicast Flood, Vof=VLAN insert/strip offload, Df=Drop New Flows, L=MAC Learning Enabled
       Proxy=MAC Requests Proxied Always, Er=Etree Root, Mn=Mirror without Vlan Tag, Ig=Igmp Trap Enabled

vif0/8      OS: tapeth0-304431
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.236  #<---
            Vrf:2 Mcast Vrf:2 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:455  bytes:19110 errors:0
            TX packets:710  bytes:29820 errors:0
            Drops:455
----

TIP: don't forget to execute the vrouter command from within the vrouter docker.
doing it from the host directly won't work.

some important information to highlight from this capture:

* NHID 87 is an "ECMP composite nexthop"
* the ECMP nexthop contains 2 "sub" nexthops: nexthop 51 and nexthop 37, each representing a
  seperate path towards the backend pods
* nexthop 51 represents a MPLSoUDP tunnel, from current node `cent222`, with
  source IP being local fabric IP `10.169.25.20`, to the other node `cent333`
  whose fabric IP is `10.169.25.21`. if you recall where our two backend pods
  are located, this is the path to the "remote" node.
* nexthop 37 represents a "local" path, towards vif 0/8 (`Oif:8`), which is the
  local backend pod's interface. 
* the `vif --get 8` give the corresponding pod interface name of vif 0/8.

==== clusterIP service workflow

the clusterIP service's loadbalancer ECMP workflow is illustrated in this
figure:

.contrail service loadbalancer ECMP
//image::https://user-images.githubusercontent.com/2038044/60762382-97f60a00-a02c-11e9-81ad-b1f05d815571.png[]
image::https://user-images.githubusercontent.com/2038044/60762413-1ce12380-a02d-11e9-8cec-41d5e177bfb9.png[]

==== multiple port service

we've understood how the service layber 4 ECMP works and explored the LB
objects in lab. remember in the figure showing the LB and all relevant objects,
we showed that one LB may having 2 or more LB listeners. each listener has an
individual backend pool which has one or multiple member(s). 

.service loadbalancer
image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]

to understand the 1:N mapping between loadbalancer and listeners, we can use the
`multiple port service` as an example. let's look at the yaml file of it:

----
$ cat svc/service-web-clusterip-mp.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip-mp
spec:
  ports:
  - name: port1
    port: 8888
    targetPort: 80
  - name: port2
    port: 9999
    targetPort: 90
  selector:
    app: webserver
----

what we've added is another item in the `ports` list: a new service port `9999`
that maps to container's `targetPort` `90`. now with two port mappings we have
to give each mapping a name, `port1` and `port2` respectively.

NOTE: without a port `name` the multiple ports yaml file won't work.

now we apply the yaml file and a new service `service-web-clusterip-mp` with 2
ports is created:

----
$ kubectl apply -f svc/service-web-clusterip-mp.yaml
service/service-web-clusterip-mp created

$ kubectl get svc
NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)            AGE
service-web-clusterip     ClusterIP  10.105.139.153  <none>       8888/TCP           3h8m
service-web-clusterip-mp  ClusterIP  10.101.102.27   <none>       8888/TCP,9999/TCP  4s

$ kubectl get ep
NAME                       ENDPOINTS                           AGE
service-web-clusterip      10.47.255.238:80                    4h18m
service-web-clusterip-mp   10.47.255.238:80,10.47.255.238:90   69m
----

NOTE: to simply the case study we've scaled down the backend RC to one pod

it looks everything is ok, isn't it? the new service comes up with 2 service
ports exposed, `8888` is the old one we've tested in previous examples, and the
new `9999` port should work equally well.

turns out that is not the case.

service port 8888 works:

----
$ kubectl exec -it cirros -- curl 10.101.102.27:8888 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.238
                         Hostname = webserver-846c9ccb8b-g27kg
                                    [giphy]
----

service port 9999 doesn't work:

----
$ kubectl exec -it cirros -- curl 10.101.102.27:9999 | w3m -T text/html | cat
command terminated with exit code 7
curl: (7) Failed to connect to 10.101.102.27 port 9999: Connection refused
----

the request towards port 9999 is rejected. reason is the `targetPort` is not
running in pod container, so there is no way you will get a response from it.

----
$ kubectl exec -it webserver-846c9ccb8b-g27kg -- netstat -lnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/python
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   PID/Program name    Path
----

`readinessProbe` introduced in chater 3 is the official kubernetes tool to
detect this situation, so in case the pod is not "ready", it will be restarted
and you will catch the events.

to resolve this let's start a server in pod to listen on port `90`. 
one of the easiest way today to start a HTTP server is to use the
`SimpleHTTPServer` module coming with `python`. in our test we only need to
set the port to `90`

----
$ kubectl exec -it webserver-846c9ccb8b-g27kg -- python -m SimpleHTTPServer 90 
Serving HTTP on 0.0.0.0 port 90 ...                                    
----

NOTE: by default, python SimpleHTTPServer listens on port 8080.

now the `targetPort` is on, we can start the request towards service port `9999`
again from the cirros pod. this time it succeed and get the returned webpage
from python SimpleHTTPServer.

----
$ kubectl exec -it cirros -- curl 10.103.87.232:9999 | w3m -T text/html | cat
Directory listing for /

 ━━━━━━━━━━━━━━━━━━━━━

  • app.py
  • Dockerfile
  • file.txt
  • requirements.txt
  • static/

 ━━━━━━━━━━━━━━━━━━━━━
----

for each incoming request the `SimpleHTTPServer` logs one line output, with an
IP address showing where the request came from. in our case cirros client pod is
with the IP `10.47.255.237`.

----
10.47.255.237 - - [04/Jul/2019 23:49:44] "GET / HTTP/1.1" 200 -
----

==== the flow table

so far we've tested clusterIP service, and we see client request is sent towards
the service IP. in contrail environment `vrouter` is the module that does all of
the packet forwarding job. when the `vrouter` in client pod get the packet, it
looks up the corresponding `vrouter` VRF table for the pod, get the nexthop and
resolves to the correct egress interface and proper encapsulation. in our test
so far, the client and backend pods are in 2 different nodes, so the source
`vrouter` decides the packets need to be send in MPLSoUDP tunnel, towards the
node where backend pod is running. what interests us the most is:

* how the service IP and podIP is translated to each other? 
* is there a way to "capture and see" the two IPs in a flow, "before" and
  "after" the translations for comparison purpose?

the most "straightforward" method you would think of is to capture the packets,
then decode and see. doing that however, is not as easy as what you've expected.
first you need to capture the packet at different places:

* at the pod interface, this is after the address is translated, that part is
  easy
* the fabric interface, this is before packet is translated and reaches the pod
  interface. here the packets are with MPLSoUDP encapsulation since data plane
  packets are "tunneled" between nodes.

then you need to copy the pcap file out and load with wireshark to decode. you
probably also need to set up wireshark to recognize the MPLSoUDP encapsulation.

the easier way is to check the vrouter flow table which records IP and port
details about a traffic flow. in this test we will prepare a big file `file.txt`
in server pod and try to download it from the client pod. 

[TIP]
====
you may wonder why we don't simply use same curl test to pull the webpage, as
what we've done in early test. in theory that is fine.  the only problem is that
the TCP flow follows the TCP session. in our previous test with `curl`, the TCP
session starts and stops immediately after the webpage is retrieved, then the
vrouter clears the flow too. you won't be fast enough to capture the flow table
at the right moment. instead, downloading a big file will hold the TCP session -
as long as the file transfer is ongoing the session will remain, and we can take
time to investigate the flow. later on in `ingress` example we will demonstrate
a different method with a one-liner shell script.  

====

now in the cirros pod curl URL, instead of just give root path `/` to list the
files in folder, we try to pull the file: `file.txt`

----
$ kubectl exec -it cirros -- curl 10.103.87.232:9999/file.txt
----

in server pod we see the log indicating the file transfer starts:

----
10.47.255.237 - - [05/Jul/2019 00:41:21] "GET /file.txt HTTP/1.1" 200 -
----

now we have enough time to collect the flow table from both client and server
node, in the vrouter docker.

.client node flow table

----
(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1361 Added 1361 Deleted 442 Changed 443Processed 1361 Used Overflow entries 0
(Created Flows/CPU: 305 342 371 343)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    40100<=>340544       10.47.255.237:42332                                 6 (3)
                         10.103.87.232:9999
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):59,  Stats:7878/520046,
 SPort 65053, TTL 0, Sinfo 6.0.0.0)

   340544<=>40100        10.103.87.232:9999                                  6 (3)
                         10.47.255.237:42332
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):68,  Stats:142894/205180194,
 SPort 63010, TTL 0, Sinfo 10.169.25.21)
----

highlights in this output:

* cirros client starts TCP connection from its pod IP `10.47.255.237` and a
  rondom port, towards the service IP `10.103.87.232` and server port `9999`
* the flow TCP flag `SSrEEr` indicates the session is established bidirectionally.
* Action `F` means "forwarding". note that there is no special processing like
  `NAT` happening here. 

we can conclude, from client's perspective, it only see the service IP. it is
not aware of any backend pod IP at all.

.server node flow table

now look at flow table in server node vrouter docker:

----
(vrouter-agent)[root@cent333 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1116 Added 1116 Deleted 422 Changed 422Processed 1116 Used Overflow entries 0
(Created Flows/CPU: 377 319 76 344)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   238980<=>424192       10.47.255.238:90                                    6 (2->3)
                         10.47.255.237:42332
(Gen: 1, K(nh):24, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):24,
 Stats:8448/202185290,  SPort 62581, TTL 0, Sinfo 3.0.0.0)

   424192<=>238980       10.47.255.237:42332                                 6 (2->2)
                         10.103.87.232:9999
(Gen: 1, K(nh):24, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:8067/419582,  SPort 51018, TTL 0, Sinfo 10.169.25.20)
----

the second flow entry looks same as the one we just saw in client side capture.
traffic lands vrouter fabric interface from remote cirros client node, across
MPLSoUDP tunnel. destination IP and port are service IP and service port
respectively. it seems nothing special here.

however, the flow `Action` now is set to `N(DPd)`, not `F`. according to the
header lines in the `flow` command output, this means NAT, or specifically,
`DNAT` (Destination address translation) with `DPAT` (Destination port
translation). so both the service IP and service port will be translated, to
backend pod IP and port.

now look at the first flow entry. source IP `10.47.255.238` is the backend pod
IP and source port is python server port `90` opened in backend container .
obviously this is the returning traffic indicating the file downloading is still
ongoing. the `Action` is also NAT(`N`), but this time it is the reverse
operation - source NAT (`SNAT`) and source PAT(`SPAT`) will happen. vrouter will
translate backend's source IP source port to the service IP and port, before
putting it into the MPLSoUDP tunnel and return back to client node.

the complete end to end traffic flow is illustrated here:

//image::https://user-images.githubusercontent.com/2038044/60388198-f7c44200-9a7b-11e9-9b08-f34167b0a2b8.png[]
//image::https://user-images.githubusercontent.com/2038044/60762300-96c3dd80-a02a-11e9-8933-452d3ee074a4.png[]
image::https://user-images.githubusercontent.com/2038044/60763424-32147d00-a042-11e9-813a-a6aa3989c09d.png[]

=== contrail LoadBalancer service

in chapter 3 we've briefly talked about LoadBalancer service. in there we
mentioned if the goal is to expose the service to the external world outside of
the cluster, we just specify `ServiceType` as `LoadBalancer` in the service yaml
file. 

whenever a service of `type: LoadBalancer` get created, in contrail environment
what will happen is , not only a `clusterIP` will be allocated and exposed to
other pods within the cluster, but also a `floating ip` will be assigned to the
loadbalancer instance and exposed to the public world outside of the cluster. 

while the `clusterIP` is still acting as a `VIP` to the client **inside** of the
cluster, the `floating ip` will essentially act as a `VIP` facing those client
sitting **outside** of the cluster, for example, a remote Internet host which
sends request to the service across the gateway router. 

in this section we'll demonstrate how does the `LoadBalancer` type of service
works in our end to end lab setup, including the kubernetes cluster, fabric
switch, gateway router, and Internet host.

==== create loadbalancer service

let's look at the yaml file of a `LoadBalancer` service. it is same as ClusterIP
service except just one more line declaring the service `type`:

----
$ cat service-web-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer    #<---
----

create and verify the service:

----
$ kubectl apply -f service-web-lb.yaml
service/service-web-lb created

$ kubectl get svc -o wide
NAME            TYPE          CLUSTER-IP   EXTERNAL-IP      PORT(S)         AGE    SELECTOR
service-web-lb  LoadBalancer  10.96.89.48  101.101.101.252  8888:32653/TCP  10s    app=webserver
----

comparing with the `clusterIP` service type, this time in the "EXTERNAL-IP"
column there is an IP allocated. if you remember what we've covered in the
"floating IP pool" section, you should understand this "EXTERNAL-IP" is actually
another `FIP`, allocated from the `NS FIP pool` - we did not give any specific
FIP pool information in the service object yaml file, so based on the algorithm
`NS default FIP pool` will be used automatically. 

==== examine VRF table in gateway router

the `route-target` community setting in the FIP VN makes it reachable by the
Internet host, so effectively our service is now also exposed to the Internet
,instead of only to pods inside of the cluster. Examining the gateway router's
VRF table reveals this:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24
Jun 19 03:56:11

k8s-test.inet.0: 23 destinations, 40 routes (23 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:01:11, MED 100, localpref 200, from 10.169.25.19
                      AS path: ?, validation-state: unverified
                    > via gr-2/2/0.32771, Push 40
----

the FIP host route is learned by gateway router, from contrail controller - more
specifically, contrail control node, which acts as a standard MP-BGP VPN `RR`
reflects routes between compute nodes and the gateway router. A further look
at the detail version of the same route displays more information about this
process:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24 detail
Jun 20 11:45:42

k8s-test.inet.0: 23 destinations, 41 routes (23 active, 0 holddown, 0 hidden)
101.101.101.252/32 (2 entries, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:9
                ......
                Source: 10.169.25.19                    #<---
                Next hop type: Router, Next hop index: 1266
                Next hop: via gr-2/2/0.32771, selected  #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                Indirect next hop: 0x900c660 1048574 INH Session ID: 0x690
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 10:15:38   Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1020006
                    unknown type 8084 value eac4:1030001
                Import Accepted
                VPN Label: 44
                Localpref: 200
                Router ID: 10.169.25.19
                Primary Routing Table bgp.l3vpn.0
----

* the `source` indicates from which BGP peer the route is learned,
  `10.169.25.19` is the contrail controller (and kubernetes master) in our lab
* `protocol next hop` tells who generates the route. `10.169.25.20` is node
`cent222` where the backend webserver pod is running
* `gr-2/2/0.32771` represents the (MPLS over) GRE tunnel between node `cent222`
and the gateway router.

to summarize, the FIP given to the service as its external ip is advertised to
gateway router, and get loaded in the router's VRF table. when Internet host
sends a request to the FIP, through MPLSoGRE tunnel the gateway router will
forward it to the compute node where backend pod is locating.

the packet flow is illustrated in this figure:

.service workflow
image::https://user-images.githubusercontent.com/2038044/60563159-a7254100-9d28-11e9-94ca-934b8f870b1e.png[]

////
* you create a `FIP pool` from a public VN, with route-target the VN is
  advertised to the remote gateway router via MP-BGP 
* you create a pod with a label `app: webserver`, kubernetes decides the pod
  will be created in node `cent222`. via XMPP the node publish the pod IP
* you create a loadbalancer type of service with `service port` and label
  selector `app=webserver`.  kubernetes allocates a service IP.
* kubernetes finds the pod with the matching label and update the `endpoint`
  with the pod IP and port information. 
* contrail create a loadbalancer instance and assign a FIP to it. contrail also
  associate that FIP with the pod interface, so there will be one to one NAT
  operation between the FIP and podIP.
* via XMPP, node `cent222` advertises the podIP and FIP to contrail controller
  `cent111`, which then advertises only the FIP to the gateway router. at this
  moment the gateway learns the nexthop of the FIP is `cent222`, so it generate
  a soft GRE tunnel toward `cent222`.
* when gateway router see a request coming from Internet toward the FIP, through
  the MPLS over GRE tunnel it will send the request to the node `cent222`
* vrouter in the node sees the packets destined to the FIP, it will perform NAT
  so the packets will be sent to the right backend pod.
////

==== verify the loadbalancer service

To verify the end to end service access from Internet host to the backend pod, 
we will login to the Internet host desktop and launch a browser, with URL
pointing to `http://101.101.101.252:8888`. 

TIP: just to keep in mind that the request has to be sent to the public **FIP**,
not the **service IP**(**clusterIP**) or backend **podIP** which is only
reachable from inside of the cluster!

this is the returned web page:

image::https://user-images.githubusercontent.com/2038044/60388669-ea5e8600-9a82-11e9-87b9-30a98572f7bb.png[]

****
in our testbed we installed a centos server as an Internet host. as with any
linux distribution, if you need to login the "GUI", you need to install Xwindow
or linux desktop applications and set it up properly. also you need a web
browser if it does not come with the server.
****

To simplify the test, you can also ssh into the Internet host and test it with
`curl` tool:

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | w3m -T text/html | cat
             Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = webserver-846c9ccb8b-vl6zs
   [giphy.gif]
----

the kubernetes service is available from Internet!

==== loadbalancer service ECMP

so far you've seen how loadbalancer type of service is exposed to the Internet
and how the FIP did the "trick". in ClusterIP service section, you've also seen
how the service loadbalancer ECMP works. what you haven't seen yet is how does
the "ECMP" processing works under loadbalancer type of service. To demonstrate
this again we scale the RC to generate one more backend pod behind the
`service`. 

----
$ kubectl scale rc rc-webserver --replicas=2
replicationcontroller/rc-webserver scaled

$ kubectl get pod -l app=webserver -o wide
NAME                READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
webserver-846c9ccb8b-r9zdt  1/1    Running  0         25m  10.47.255.238  cent333  <none>
webserver-846c9ccb8b-xkjpw  1/1    Running  0         23s  10.47.255.236  cent222  <none>
----

here is the question: with 2 pods on different node as backend now, from the
gatway router's perspective when it get the service request, which node it will
choose to forward the traffic to? let`s check the gateway router`s VRF table
again:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32
Jun 30 00:27:03

k8s-test.inet.0: 24 destinations, 46 routes (24 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 26
                    [BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 26
----

the same FIP prefix is imported as we've seen in previous example, except that
now the same route is learned twice and an additional MPLSoGRE tunnel is
created. previously in ClusterIP service example we use `detail` option in `show
route` command to find the tunnel endpoints, this time we examine the soft GRE
`gr-` interface to find the same:

----
labroot@camaro> show interfaces gr-2/2/0.32771
Jun 30 00:56:01
  Logical interface gr-2/2/0.32771 (Index 392) (SNMP ifIndex 1801)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.21:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None

labroot@camaro> show interfaces gr-2/3/0.32771
  Logical interface gr-2/3/0.32771 (Index 393) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 11
    Output packets: 11
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` of `gr-` interface indicates the two end points of a GRE tunnel:

* `10.169.25.20:192.168.0.204`: tunnel between node `cent222` and gateway router
* `10.169.25.21:192.168.0.204`: tunnel between node `cent333` and gateway router

We end up to have 2 tunnels in the gateway router, each pointing to a different
node where a backend pod is running. now we believe the router will perform
ECMP load balancing between the two GRE tunnel, whenever it got service request
toward the same FIP. let's check it out.

==== verify the loadbalancer service ECMP

to verify the ECMP we'll just pull the webpage a few more time and we expect to
see both podIP displayed eventually.

turns out this never happens!

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = webserver-846c9ccb8b-xkjpw
----

the only webpage we got is from the first backend pod `10.47.255.236`,
`webserver-846c9ccb8b-xkjpw`, running in node `cent222`. the other one never show up.
so the expected ECMP does not happen yet. when we examine the route again with
`detail` or `extensive` keyword we find the root cause:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32 detail | match state
Jun 30 00:48:29
                State: <Secondary Active Int Ext ProtectionCand>
                Validation State: unverified
                State: <Secondary NotBest Int Ext ProtectionCand>
                Validation State: unverified
----

from that we realize that, even if the router learned the same prefix from both
node, only one is `Active` and the other one won't take effect because it is
`NotBest`. therefore, the second route and the corresponding GRE interface
`gr-2/2/0.32771` will never get loaded into the forwarding table:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 00:53:12
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination         Type  RtRef  Next  hop      Type  Index  NhRef  Netif
101.101.101.252/32  user  0      indr  1048597  2
                                Push 26     1272     2 gr-2/3/0.32771
----

this is the default Junos BGP path selection behavior and detail discussion of
it is out of the scope of this book. 

NOTE: for Junos BGP path selection algorithm, check this link:
https://www.juniper.net/documentation/en_US/junos/topics/topic-map/bgp-path-selection.html

the solution is to enable the `multipath vpn-unequal-cost` knob under the VRF:

----
labroot@camaro# set routing-instances k8s-test routing-options multipath vpn-unequal-cost
----

now check the VRF table again:

----
labroot@camaro# run show route table k8s-test.inet.0 101.101.101.252/32
Jun 26 20:09:21

k8s-test.inet.0: 27 destinations, 54 routes (27 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 @[BGP/170] 00:00:04, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/1/0.32771, Push 72
                    [BGP/170] 00:00:52, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 52
                   #[Multipath/255] 00:00:04, metric 100, metric2 0
                       via gr-2/1/0.32771, Push 72
                     > via gr-2/2/0.32771, Push 52
----

a `Multipath` with both GRE interface will be added under the FIP prefix, the
forwarding table reflects the same:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 01:12:36
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination        Type RtRef Next hop    Type Index    NhRef Netif
101.101.101.252/32 user     0             ulst  1048601     2
                                          indr  1048597     2
                                         Push 26     1272     2 gr-2/3/0.32771
                                          indr  1048600     2
                                         Push 26     1277     2 gr-2/2/0.32771
----

now try to pull the webpage from Internet host multiple times with `curl` or web
browser, we see the random result - both backend pod get the request and
responses back.

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = webserver-846c9ccb8b-xkjpw

[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = webserver-846c9ccb8b-r9zdt
----

==== loadbalancer service workflow

the end to end packet flow is illustrated here:

.loadbalancer service ECMP
image::https://user-images.githubusercontent.com/2038044/60763675-8e2dd000-a047-11e9-91a6-5fb1319517dc.png[]

== contrail ingress

in chapter 3 we've learned that Ingress maps URLs to services with `rules`. this
makes this Ingress section a little bit easier. we don't need to explain
everything that happens in Ingress. instead, we can focus on the Ingress
external IP exposure and service mapping. the rest part of the story is all
about service to backend mapping which we've examined a lot. we'll also
introduce how does contrail integrate with ingress, then we'll demonstrate with
an "end to end" lab and verify in details to understand exactly how ingress
works in contrail.

=== contrail ingress loadbalancer

like contrail's service implementation, contrail Ingress, is also implemented
through loadbalancer, but with a different `loadbalancer_provider` attribute,
accordingly `contrail-svc-monitor` component takes different actions to
implement `Ingress` in contrail environment.

Remember in "Contrail-Kubernetes architecture" section we gave the "object
mapping" between kubernetes and contrail. in that section you've learned kubernetes
`service` maps to `ECMP loadbalancer (native)` and `Ingress` maps to `Haproxy
loadbalancer`. 

in `service` section when we were exploring the loadbalancer and
the relevant objects (listener, pool, and member), we noticed the loadbalancer's
`loadbalancer_provider` attribute is with a type `native`. 

        "loadbalancer_provider": "native",

in this section we'll see `loadbalancer_provider` has a different value for 
Ingress's `loadbalancer`. we'll also look into the similarities and differences
between `service` loadbalancer and `Ingress` loadbalancer.

=== contrail ingress workflow

When an Ingress is configured in contrail kubernetes environment, the event will
be noticed by other system components, and a lot of actions will be triggered.
the deep level implementation is out of the scope of this book, but in a high
level here is the workflow:

. `contrail-kube-manager` keeps listening to the events of`kube-apiserver` 
. user creates an `ingress` object (rules)
. `contrail-kube-manager` gets the event from `kube-apiserver`
. `contrail-kube-manager` creates a `loadbalancer` object in contrail
  DB, and set `loadbalancer_provider` attribute differently for `service`
  and `Ingress`:
  - for `service` it is set to `native`.
  - for `Ingress` it is set to `opencontrail`.
. `contrail-service-monitor` component sees the `loadbalancer` creation event,
  based on `loadbalancer_provider` value, it actions differently. 
  - if the `loadbalancer_provider` is `native`, indicating this is a `service` object,
    it will do ECMP processing. we've learned this in previous section.
  - if the `loadbalancer_provider` is `opencontrail`, indicating this is an `Ingress`
    object, `contrail-svc-monitor` invokes haproxy processes.

as you can see, contrail implements `Ingress` with haproxy loadbalancer, this is
what you've read in the section of "contrail kubernetes object mapping".
specifically for Ingress:

* for `Ingress`, overall two haproxy processes will be created
* for each `Ingress` object, one loadbalancer will be created
* the two haproxy processes work in "active-standby" mode:
  - one compute node runs the "active" haproxy process
  - the other compute node runs the "standby" haproxy process
* both `haproxy` processes are programmed with appropriate configuration, based
  on the rules defined in Ingress object.

=== contrail Ingress traffic flow

Service request, as a type of `overlay` traffic, may come from two sources
depending on who initiates the request:

* internal request: requests coming from another pod inside of the cluster
* external request: requests coming from an Internet host outside of the cluster

the only difference between the two, is how the traffic hit the "active"
haproxy. 

an Ingress will be allocated 2 IPs: 

* cluster-internal virtual IP
* external virtual IP, contrail implement this with FIP

here is the traffic flow for service request process:

* for internal request it hits Ingress's "internal" VIP directly. 
* for external request it first hits Ingress's "external" VIP - the FIP, which
  is the one exposed to external, and that is the time when NAT starts to play
  as we've explained in `FIP` section. 
* traffic is then forwarded to internal Ingress VIP after NAT process.  
* from this moment on, both type of requests is processed exactly the same way:
  the requests will be "proxied" to the node where backend pods are located and
  reaches the pods eventually. 
* In the case that the backend pods are running in a different compute node than
  the one running active haproxy, a MPLS over UDP tunnel is created between the
  two compute node.

here is the end to end service request flow when accessing from a pod in the
cluster:

//TODO: lost the drawing, need to redraw, and give text explain
.Ingress traffic flow: access from internal
image::https://user-images.githubusercontent.com/2038044/61061849-0b9c6c00-a3cb-11e9-8788-cb1c1dedafc4.png[]

here is the end to end service request flow when accessing from Internet host:

.Ingress traffic flow: access from external
//image::https://user-images.githubusercontent.com/2038044/60410376-09017180-9b96-11e9-927e-4cf1d98f2cef.png[]
//image::https://user-images.githubusercontent.com/2038044/61061268-eb1fe200-a3c9-11e9-9d36-191955b766e1.png[]
image::https://user-images.githubusercontent.com/2038044/61061427-3f2ac680-a3ca-11e9-9364-f11bea477319.png[]


////
. the "haproxy driver" will create a service instance (SI) with
  `haproxy-loadbalancer` type of template applied.
. the SI will has a "port tuple" linked to a linux netns VM
. the linux netns VM VM has its VMI, and a reference to an instance-ip
. `contrail-svc-monitor` launches the HAProxy process, with appropriate
  configuration, based on the ingress rules defined in the yaml file.

Whenever an ingress is configured in kubernetes, `contrail-kube-manager` that is
watching the kube-apiserver get the events and creates an loadbalancer object in
contrail-controller.  `contrail-svc-monitor` component listens for the load
balancer objects and takes a different action based on its
`loadbalancer_provider` attribute. when it sees `loadbalancer_provider`
attribute being `opencontrail`, it launches two haproxy processes, each in a
seperate compute node. both `haproxy` processes are programmed with appropriate
configuration based on the ingress rules you defined. the two haproxy processes
work in "active-standby" mode. 

contrail Ingress is also implemented through loadbalancer (like service), but
Ingress's loadbalancer is with a different `loadbalancer_provider` attribute,
which makes `contrail-svc-monitor` takes a different action than what it does
for service. now it is the time to tell that the `loadbalancer_provider` is
`opencontrail`, and accordingly the `contrail-svc-monitor` action is to launch a
haproxy process running with ingress rules in its configuration file. this
basically explains what we see now.

that is only a high level overview about the contrail's implementation of
ingress. 
in fact that for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will generate a service-instance (SI).
next we'll explore the objects in a little bit more details.

this is how it works:

////

contrail supports all 3 types of ingress:

* http-based single-service ingress, 
* simple-fanout ingress
* name-based virtual hosting ingress.

we'll look into each type of ingress.

=== Ingress testbed

in our lab we use the same testbed as what we use for `service` test:

.Ingress testbed
image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]

.Ingress test preparation

in order to create and test Ingress, we will also need to create the following
objects in our setup.

. NS: `ns-user-1`
. FIP-VN: `vn-ns-default`
. FIP-pool: `pool-ns-default`
. client pod: `cirros`

what will hold all of our test objects is the `ns-user-1` NS/project, which
refers to a NS default pool `pool-ns-default` that is to be created manually.
the NS default pool is based on a VN `vn-ns-default` that has subnet
`101.101.101/24`.  that is why later on you will see FIP for Ingress get
assigned from this subnet. a pod `cirros` is needed to start the internal
HTTP request towards the Ingress.

////
----
$ kubectl apply -f ns/ns-user-1-default-pool.yaml
namespace/ns-user-1 created
$ kubectl apply -f vn/vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default created
$ kubectl apply -f pod/pod-cirros.yaml
pod/cirros created
$ kubectl apply -f ingress/ingress-simple-fanout.yaml
ingress.extensions/ingress-sf created
----
////

after these three objects are created, we can proceed to the next step of
creating each type of Ingress object.

=== single service Ingress

single service Ingress is the most basic form of Ingress. it does not define any
rules and its main usage is to expose service and proxy all incoming service
request to the same "single" backend service.

    www.juniper.net --|                 |
    www.cisco.com   --|  101.101.101.1  |-> webservice
    www.google.com  --|                 |

to demonstrate `single service` type of Ingress, the objects that we need to
create are:

* an `Ingress` object that defines the backend service
* a backend service object
* at least one backend pod for the service

additionally, a "client" pod is needed to test the ingress from inside of the
cluster. we can use the same `cirros` pod we've used in earlier examples as
cluster-internal client.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `ingress` definition

in our single service ingress test lab, we want to achieve this goal:

* request toward any URLs will be directed to `webservice-1` with `servicePort`
  8888

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-ss
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
----

this does not look anything fancy. basically in this "single service" Ingress
there is nothing else but a reference to one "single service" `webserver-1` as
its "backend". all HTTP request will be dispatched to this service, and from
there the request will reach a backend pod. next we'll look at the backend
service.

==== backend `service` definition

we can use exactly the same service as introduced in `service` example. 

----
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
----

NOTE: the service `type` is optional. with `Ingress`, `service` does not need to
be exposed to external directly anymore. therefore `LoadBalancer` type of
service is not required. 

==== backend `pod` definition

same as in `service` example, we can use exactly the same `webserver` rc to
launch backend pods:

----
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-webserver
  labels:
    app: webserver
spec:
  replicas: 1
  selector:
    app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

==== an "all in one" yaml file

as usual, we can create an individual yaml file for each of the objects. but
considering in `Ingress`, these objects always need to be created and removed
together, it is better to "merge" all yaml files into one. yaml syntax supports
this by using a "document delimitor", a `---` line between each object
definition. the benefits are:

* you can create all objects in the yaml file in one go, using just one `kubectl apply`
  command with the yaml file name
* similarly, if anything goes wrong and you need to clean up, you can delete
  all objects created with the yaml file in one `kubectl delete` command
* whenever needed, you can still delete/update each individual objects
  independently, by giving the object name

NOTE: imaging during test process you may need to create and delete all objects
as a whole very often, grouping multiple objects in one yaml file is a very
convenient and recommended method.

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-ss
spec:
  backend:
    serviceName: service-web-clusterip
    servicePort: 8888
---
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  #type: LoadBalancer
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-webserver
  labels:
    app: webserver
spec:
  replicas: 1
  selector:
    app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

////
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
////

==== deploy the single service Ingress

before applying the yaml file to get all objects created, let's take a quick
look at our two nodes, see if there is any `haproxy` process running already:

----
$ ps aux | grep haproxy
$ 
----

So the answer is no. haproxy will be created only after we created `Ingress` and
the corresponding loadbalancer object is seen by `contrail-service-monitor`.
we'll check this again after we create an `Ingress`.

----
$ kubectl apply -f ingress/ingress-single-service.yaml
ingress.extensions/ingress-ss created
service/service-web-clusterip created
replicationcontroller/rc-webserver created
----

the Ingress, one service and one RC object are now created.

==== ingress objects

let's start to look at the Ingress object.

----
$ kubectl get ingresses.extensions -o wide
NAME         HOSTS   ADDRESS                       PORTS   AGE
ingress-ss   *       10.47.255.238,101.101.101.1   80      29m

$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1", "kind":"Ingress",
        "metadata":{"annotations":{},"name":"ingress-ss","namespace":"ns-user-1"},
        "spec":{"backend":{"serviceName":"webservice-1", "servicePort":80}}}
    creationTimestamp: 2019-07-18T04:06:29Z
    generation: 1
    name: ingress-ss
    namespace: ns-user-1
    resourceVersion: "845969"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-ss
    uid: 6b48bd8f-a911-11e9-8112-0050569e6cfc
  spec:
    backend:
      serviceName: webservice-1
      servicePort: 80
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

as expected, the backend service is applied to the Ingress properly. there is no
explicit rules defined to map a certain URL to a different service - all
HTTP requests will be dispatched to the same backend service. 

what may confuse you is the two IP addresses shown here: 

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

we've seen these two subnets in service examples:

* 10.47.255.x is an cluster-internal `podIP` allocated from the pod's default subnet
* 101.101.101.x is the `FIP` associated with an internal IP.

but the question is why an Ingress has a `podIP` and `FIP`?

////
this is the IP addresses allocated to the haproxy "virtual machine". 
what is a haproxy "virtual machine" anyway? isn't it just a process running in
the compute node? or, does compute node spawned some hidden VMs behind the
scene? 
////

let's hold the answer for now and continue to check service and pod
object created from the all-in-one yaml file. we'll come back to this question
shortly.

.service objects
----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE  SELECTOR
service-web-clusterip  ClusterIP  10.97.226.91  <none>       8888/TCP  28m  app=webserver
----

the service is also created and allocated a clusterIP. we've seen
this before and it looks nothing special. now look at the pods:

.backend and client pod
----
$ kubectl get pod -o wide --show-labels
NAME                READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE           LABELS
cirros              1/1    Running  0         18d  10.47.255.237  cent222  <none>     app=cirros
webserver-846c9ccb8b-9nfdx  1/1    Running  0         33m  10.47.255.236  cent333  <none>     app=webserver
----

everything looks fine. there is a backend pod running for the service. we have
learned how selector and label works in service-pod associations so there is
nothing new here. next we'll examine the haproxy and try to make some sense out
of the 2 IPs allocated to Ingress object.

==== haproxy processes

earlier before the Ingress is created, we were looking for haproxy process in
node but could not see anything. let's check it again and see if any magic
happens:

.node `cent222`

----
$ ps aux | grep haproxy
188  23465  0.0  0.0  55440  852  ?  Ss  00:58  0:00  haproxy  
  -f  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf  
  -p  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid  
  -sf  23447
----

.node `cent333`

----
$ ps aux | grep haproxy
188   16335  0.0  0.0  55440  2892  ?  Ss  00:58  0:00  haproxy  
  -f  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf  
  -p  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid  
  -sf  16317
----

right after ingress got created, we see a haproxy process created in each of our
two nodes!

remember earlier when we talk about ingress contrail implementation, we've said
contrail Ingress is also implemented through loadbalancer (like service), but
Ingress's loadbalancer is with a different `loadbalancer_provider` attribute,
which makes `contrail-svc-monitor` takes a different action than what it does
for service. the `loadbalancer_provider` for Ingress loadbalancer is
`opencontrail`, and accordingly the action that `contrail-svc-monitor` take on
it is to launch a haproxy process running with ingress rules programmed in its
configuration file.  this basically explains the haproxy processes we see in the
2 nodes.

////
that is only a high level overview about the contrail's implementation of
ingress. in fact the for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will generate a service-instance (SI).
next we'll explore the objects in a little bit more details.

----
$ ps aux | grep haproxy
188      16335  0.0  0.0  55440  2892 ?        Ss   00:58   0:00 haproxy -f /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf -p /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid -sf 16317
root     18937  0.0  0.0 112716   984 pts/0    S+   01:21   0:00 grep --color=auto haproxy

$ pstree -lnaps 16335
systemd,1 --switched-root --system --deserialize 22
  └─dockerd,6268
      └─docker-containe,6756 --config /var/run/docker/containerd/containerd.toml
          └─docker-containe,3114 -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/5e9c1c3a14cf7e2d5dca2512784b227808890b4d260c9badef9b8aab8aaaa76b -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc
              └─haproxy,16335 -f /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf -p /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid -sf 16317

$ docker ps -a | grep 5e9c
5e9c1c3a14cf        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest         "/entrypoint.sh /usr…"   5 weeks ago         Up 2 weeks                                   vrouter_vrouter-agent_1

$ docker exec -it vrouter_vrouter-agent_1 ps ef
  PID TTY      STAT   TIME COMMAND
17141 pts/0    Ssl+ 233:02 /usr/bin/python /usr/bin/contrail-nodemgr --nodetype=
16837 pts/0    Ss     0:00 -bash USER=root LOGNAME=root HOME=/root PATH=/usr/loc
17090 pts/0    T      0:00  \_ less _fzf_orig_completion_tee=complete -F %s tee
17279 pts/0    T      0:00  \_ less _fzf_orig_completion_tee=complete -F %s tee
19281 pts/0    Sl+    0:00  \_ docker exec -it vrouter_vrouter-agent_1 ps ef _fz
19297 pts/2    Rs+    0:00 ps ef PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/
16646 pts/1    Ss+    0:00 bash PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/u
 3130 pts/0    Ss+    0:00 /bin/bash /entrypoint.sh /usr/bin/contrail-vrouter-ag
 3307 pts/0    Sl+  358:06  \_ /usr/bin/contrail-vrouter-agent HOSTNAME=cent333
 5675 tty1     Ss+    0:00 /sbin/agetty --noclear tty1 linux LANG= PATH=/usr/loc

$ docker exec -it vrouter_vrouter-agent_1 ps 16335
  PID TTY      STAT   TIME COMMAND
16335 ?        Ss     0:00 haproxy -f /var/lib/contrail/loadbalancer/haproxy/5be
----
////


==== Ingress loadbalancer objects

////
.contrail object: SI, port tuple, VMI
image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]
////

we've talked about Ingress loadbalancers for a while. in `service` section,
we've looked into service loadbalancer object in UI and learned more details
about the object data structrure. now after we created Ingress object, let's
check the loadbalancers object again and see what Ingress brings in.

.loadbalancers (configuration > Networking > Floating IPs)
//image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]
//image::https://user-images.githubusercontent.com/2038044/61432850-aa5f2600-a8ff-11e9-9d9f-932a386bf81f.png[]
image::https://user-images.githubusercontent.com/2038044/61433696-ff03a080-a901-11e9-96c1-3dfd4886c322.png[]

2 loadbalancers are generated after we applied the all-in-one yaml file.

* loadbalancer `ns-user-1__ingress-ss` for Ingress `ingress-ss`
* loadbalancer `ns-user-1__webservice-clusterip` for service `webserver-clusterip`

we've learned the service loadbalancer object previously, if you expand the
service you will see more details, but nothing would surprise us now.

.service loadbalancer object (click the triangle in the left of the loadbalancer name)
//image::https://user-images.githubusercontent.com/2038044/61049744-409cc480-a3b3-11e9-8a8e-5cdff7e6a931.png[]
//image::https://user-images.githubusercontent.com/2038044/61050199-64143f00-a3b4-11e9-9f7d-339775a3ae0e.png[]
image::https://user-images.githubusercontent.com/2038044/61433906-8bae5e80-a902-11e9-8039-fc5c5414c15a.png[]

as expected, the service loadbalancer has a ClusterIP, and a listener object
that is listening on port 8888. one thing we want to highlight here is the
`loadbalancer_provider`. here the value is "native", so the action
`contrail-svc-monitor` takes is layer 4 (application layer) ECMP process, which
we've explored a lot in service section. now let's expand Ingress loadbalancer
and look at the details.

.ingress loadbalancer object 
//image::https://user-images.githubusercontent.com/2038044/61021789-f98ddf80-a370-11e9-9cce-30a0c2671bc2.png[]
image::https://user-images.githubusercontent.com/2038044/61434308-97e6eb80-a903-11e9-8a34-58f4bbaddf30.png[]

NOTE: in order to see the detail parameters of an object, click the triangle in
the left of the loadbalancer name to expand it, then click on the "Advanced JSON
view" icon on the right up in the expanded view)

some highlights in the figure:

* `loadbalancer_provider` is `opencontrail`
* Ingress loadbalancer has a reference to a `service-instance` (SI) object
* `SI` object has a property `ha_mode` set to `active-standby`
* `SI` object has an interface with IP `10.47.255.238`

at this moment,  we can explain the Ingress IP `10.47.255.248` seen in ingress.
it is:

* an cluster-internal IP address allocated from the default pod network to
  the `SI` which the Ingress loadbalancer refers to.  
* the frontend IP that the Ingress loadbalancer will listen for HTTP requests, 
* the internal IP that the public FIP `101.101.101.1` maps to with NAT. 

TIP: in this book we'll refer this private IP with different names
interchangeably: "Ingress Internal IP", "Ingress internal VIP", "Ingress private
IP", "Ingress loadbalancer interface IP", etc.  to differentiate it from the
Ingress public FIP, we can also name it as "Ingress podIP". 
//similarly we'll refer the Ingress public FIP as "Ingress external IP"

Now we understand the purpose of these two IPs:

* Ingress `podIP` is the VIP facing inside of the cluster. To reach Ingress from
  inside of the cluster, requests coming from other pods will have their
  destination IP set to Ingress `podIP`.
* Ingress `FIP` is VIP facing outside world. To reach Ingress from outside of
  the cluster, requests coming from Internet hosts need to have their
  destinations IP set to Ingress FIP.  when node receives traffic destined to
  the Ingress FIP from outside of the cluster, vrouter will translate it into
  the Ingress `podIP`

NOTE: the detail Ingress loadbalancer object implementation refers to SI, and SI
again includes other data structure or reference to other objects (port-tuple,
linux netns, VMI, etc). overall it is more complicated and involves more details
than what we've covered and it is hard to put everything in this book. we've
tailored the details into a high level overview so that important concepts like
haproxy and the two Ingress IPes are more understandable.

Once packet arrives to the Ingress `podIP`, from internal or external, Ingress
loadbalancer will do TCP proxy operation through haproxy process, and dispatch
the traffic towards the service and eventually to the backend pod.

we've seen the haproxy process is running, to examine more details of this proxy
operation, next we can further check its configuration file and running
parameters.
//configuration file and confirm the ingress rules are programmed properly.

==== `haproxy.conf` file

in each (compute) node, under `/var/lib/contrail/loadbalancer/` folder there
will be a `haproxy` subfolder. the file structure looks like this:

----
├── 5be035d8-a918-11e9-8112-0050569e6cfc.conf
└── haproxy
    └── 5be035d8-a918-11e9-8112-0050569e6cfc
        ├── haproxy.conf
        ├── haproxy.pid
        └── haproxy.sock
----

you can check either `8fd3e8ea-9539-11e9-9e54-0050569e6cfc` or
`haproxy/haproxy.conf` file for the same haproxy configuration:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ......
        ulimit-n 200000
        maxconn 65000
        ......
        stats socket
        /var/lib/contrail/loadbalancer/haproxy/6b48bd8f-a911-11e9-8112-0050569e6cfc/haproxy.sock
            mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend f3a7a6a6-5c6d-4f78-81fb-86f6f1b361cf
        option tcplog
        bind 10.47.255.238:80                                   #<---
        mode http                                               #<---
        option forwardfor
        default_backend b45fb570-bec5-4208-93c9-ba58c3a55936    #<---

backend b45fb570-bec5-4208-93c9-ba58c3a55936                    #<---
        mode http                                               #<---
        balance roundrobin
        option forwardfor
        server 4c3031bb-e2bb-4727-a1c7-95afc580bc77 10.111.216.190:80 weight 1
                                                    ^^^^^^^^^^^^^^^^^
----

the configuration is simple, and here is the illustration of it:

.single service ingress 
image::https://user-images.githubusercontent.com/2038044/61689786-db6f9a00-acf5-11e9-9625-4ba1e570d354.png[]

* the haproxy `frontend` represents the "frontend" of an Ingress, facing clients
* the haproxy `backend` represents the "backend" of an Ingress, facing services.
* the haproxy `frontend` defines a `bind` to the Ingress podIP and `mode` of
  `http`. these knobs indicate what the frontend is listening.  
* the haproxy `backend` section defines the `server`, which is backend `service`
  in our case. it has a format of `serviceIP:servicePort`, which is the exact
  `service` object we've created using the all-in-one yaml file.
* the `default_backend` in `frontend` section defines which backend is the
  "default": it will be used when a haproxy receives a URL request that has no
  explicit match anywhere else in the `frontend` section. in this case the
  `default_backend` refers to the only `backend` service `10.111.216.190:80`.
  this is due to the fact that there is no `rules` defined in `single service
  Ingress`, so all HTTP requests will go to the same default_backend service,
  regardless of what URL the user types in.

NOTE: later in `simple fan-out Ingress` and `name-based virtual hosting Ingress`
examples, we will see another type of configuration statement
`use_backend...if...` that can be used to force each URL to go to a different
backend.

through this configuration, the haproxy implemented our single service Ingress.

==== gateway router VRF table

we've explored a lot inside of the cluster. now let's look at the gateway
router's VRF table. 

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61

----

Same as in service example, from outside of the cluster, only FIP is visible.
`detail` version of it conveys more information:

----
labroot@camaro> show route table k8s-test 101.101.101.1 detail

k8s-test 24 destinations, 49 routes (24 active, 0 holddown, 0 hidden)
101.101.101.1/32 (1 entry, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:5     #<---
                ......
                Source: 10.169.25.19
                Next hop: via gr-2/2/0.32771, selected
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                Indirect next hop: 0x900d320 1048597 INH Session ID: 0x6f9
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 34         Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1040000
                Import Accepted
                VPN Label: 61
                Localpref: 200                          #<---
                Router ID: 10.169.25.19
----

//TODO: add diagram

* through XMPP, vrouter advertises the FIP prefix to contrail controller.
  at least 2 pieces of information from the output indicates who represents the
  FIP in this example - node `cent222`:
  - `Protocol next hop` being `10.169.25.20`
  - `Route Distinguisher` being `10.169.25.20:5`
* through MP-BGP, contrail controller "reflects" the FIP prefix to the gateway
  router, `Source: 10.169.25.19` indicates this fact.

so it looks `cent222` is "selected" to be the active haproxy node, and the other
node `cent333` is the standby one. therefore you should expect service request
coming from Internet host goes to node `cent222` first. of course, the overlay
traffic will be carried in MPLS over GRE tunnel, same as what you've seen from
service example. 

the FIP advertisement towards gateway router is exactly the same in all types of
Ingresses.

[NOTE]
==== 
another fact that we've skipped on purpose is the different "local
preference" value used by the active and standby node when advertising FIP
prefix. saying that will involve other complex topics like the active node
selection algorithm and so on. but it is worth a brief introduction:

both nodes have loadbalancer and haproxy running so both will advertise the FIP
prefix `101.101.101.1` to gateway router. however, they are advertised with
different local preference value. the "Active" node advertise with a value of
`200` and the "standby" node with `100`. contrail controller have both routes
from the 2 nodes, but only the "winning" one will be advertised to the gateway
router. that is why the "other" BGP route is dropped and only the one is
displayed.  `Localpref` being `200` proves it is coming from the active compute
node.
====

==== verify ingress: from internal

we've explored a lot about ingress configuration and objects examination, now it
is time to verify the test result. since the `Ingress` serves both inside and
outside of the cluster, our verification will start from the cirros pod
inside of cluster, then from the Internet host outside of it.

.from inside of cluster

----
$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]

$ kubectl exec -it cirros -- \
curl -H 'Host:www.google.com' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]
$ kubectl exec -it cirros -- \
curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
                                    [giphy]
----

we still use the `curl` command to trigger HTTP requests towards the ingress's
private IP. the return proves our `Ingress` works: requests towards
different URLs are all proxied to the same backend pods, through the
default backend services `service-web-clusterip`. 

in the fourth request we didn't give a URL via `-H`, `curl` will fill `host`
with the request IP address, `10.47.255.238` in this test, again it goes to the
same backend pod and get the same returned response.

NOTE: The `-H` option is important in Ingress test with `curl`. it carries the
full URL in HTTP payload that the Ingress loadbalancer is waiting for. without
it the HTTP header will carry `Host: 10.47.255.238`, which has no matching rule,
so it will be treated same as with a unknown URL.

==== verify ingress: from external (Internet host)

the more exciting part of the test is to visit the URLs from external. overall
we hope `Ingress` meant to expose services to the Internet host, even though it
does not have to. 

to make sure the URL resolves to the right FIP address, we need to update
`/etc/hosts` file by adding one line in the end - you don't want to just end up
with a nice webpage from Juniper/cisco's official website as your test result.

----
# echo "101.101.101.1  www.juniper.net www.cisco.com www.google.com" >> /etc/hosts
# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
101.101.101.1  www.juniper.net www.cisco.com www.google.com     #<---
----

now, from internet host's "desktop", we launch chrome browser, and input one of
the 3 URLs: `www.juniper.net`, `www.cisco.com` or `www.google.com`. By keep
refreshing the pages we can confirm all HTTP request is returned by the same
backend pod.

//image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]
//TODO: add figure

same result can be seen from `curl` also. the command is exactly the same as
what we've seen when testing from a pod, except this time we send requests to
Ingress external FIP, instead of the Ingress internal podIP.

from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = webserver-846c9ccb8b-2-m2272

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]
----

everything works!  next we'll look at the second Ingress type "simple fan-out
Ingress".  it is better to clean up everything before moving forward.  now we
can take advantage of the all-in-one yaml file - everything can be cleared with
the same all-in-one yaml file:

----
$ kubectl delete -f ingress/ingress-single-service.yaml
ingress.extensions "ingress-ss" deleted
service "webservice-1" deleted
replicationcontroller "webserver-846c9ccb8b-1" deleted
----

=== simple fan-out Ingress

//TODO: adjust the TOC, default backend
both simple fan-out Ingress and name-based virtual host Ingress support "URL
routing", the only difference is the former is based on `path` and the
latter is based on `host`.

with simple fan-out Ingress, based on the URL path and rules, an ingress
loadbalancer directs traffic to different backend services.

    www.juniper.net/qa --|                 |-> webservice-1
                         |  101.101.101.1  |
    www.juniper.net/dev -|                 |-> webservice-2

to demonstrate `simple fan-out` type of Ingress, the objects that we need to
create are:

* an `Ingress` object: defines the rules, mapping 2 paths to 2
  backend services
* 2 backend services objects
* each service requires at least one pod as backend

we use the same `cirros` pod as cluster-internal client we've used in earlier
examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////


==== `ingress` definition

in our simple fan-out Ingress test lab, we want to achieve these goals for URL
`www.juniper.net`:

* request toward path `/dev` will be directed to a service `webservice-1`
  with `servicePort` 8888
* request toward path `/qa` will be directed to a service `webservice-2`
  with `servicePort` 8888

here is the corresponding yaml file to implement these goals:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-sf
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: webservice-2
          servicePort: 8888
----

in contrast to "single service Ingress", in "fan-out" `Ingress` object (and
"name-based virtual host Ingress") we see "rules" defined - here it is the
mappings from multiple "paths" to different backend services. 

==== backend `service` definition

since we defined 2 rules each for a `path`, we need two services also. we 
can "clone" the previous service in `single service Ingress` example and
just change the service's name and selector to generate the second service.
e.g.: this is definition of `webservice-1` and `webservice-2` service.

----
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
----

----
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
  #type: LoadBalancer
----

==== backend `pod` definition

because we have 2 backend services now, apparently we also need at least two
backend pods each with a label matching to a service.  we can clone the previous
RC into two and just change the name and label of the second RC.

this is the definition of the RCs.

.RC for webserver-1
----
apiVersion: v1
kind: ReplicationController
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----


.RC for webserver-2
----
apiVersion: v1
kind: ReplicationController
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

==== deploy simple fan-out Ingress

same as in `single service Ingress`, we put everything together to get an
"all-in-one" yaml file to test simple fan-out Ingress:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-sf
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: webservice-2
          servicePort: 8888
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
  #type: LoadBalancer
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

.`apply` the all-in-one yaml file to create all objects

----
$ kubectl apply -f ingress/ingress-simple-fanout.yaml
ingress.extensions/ingress-sf created
service/webservice-1 created
service/webservice-2 created
deployment.extensions/webserver-1 created
deployment.extensions/webserver-2 created
----

the Ingress, two services and two RC objects are now created.

==== ingress objects and ingress loadbalancer

let's look at the kubernetes objects created from the all-in-one yaml file:

.ingress objects
//TODO: test this out and capture the data
----
$ kubectl get ingresses.extensions
NAME        HOSTS            ADDRESS                      PORTS  AGE
ingress-sf  www.juniper.net  10.47.255.238,101.101.101.1  80     7s

$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ingress-sf","namespace":"ns-user-1"},"spec":{"rules":[{"host":"www.juniper.net","http":{"paths":[{"backend":{"serviceName":"webservice-1","servicePort":8888},"path":"/dev"},{"backend":{"serviceName":"webservice-2","servicePort":8888},"path":"/qa"}]}}]}}
    creationTimestamp: 2019-08-13T06:00:28Z
    generation: 1
    name: ingress-sf
    namespace: ns-user-1
    resourceVersion: "860530"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-sf
    uid: a6e801fd-bd8f-11e9-9072-0050569e6cfc
  spec:
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: webservice-1
            servicePort: 8888
          path: /dev
        - backend:
            serviceName: webservice-2
            servicePort: 8888
          path: /qa
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

the "rules" are defined properly, within each rule there is a mapping from a
`path` to the corresponding service. we see same Ingress internal podIP and
external FIP as we've seen in "single service Ingress" example:

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

therefore from gateway router's perspective, there is no differences between all
types of Ingress. in all cases a FIP will be allocated to the Ingress and it is
advertised to the gateway router:

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61
----

now check backend services and pods:

.service objects
----
$ kubectl get svc -o wide
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE    SELECTOR
webservice-1  ClusterIP  10.111.234.187  <none>       8888/TCP  4m46s  app=webserver-1
webservice-2  ClusterIP  10.97.77.82     <none>       8888/TCP  4m46s  app=webserver-2
----

////
.the real capture
----
$ kubectl get svc -o wide
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   SELECTOR
webservice-1   ClusterIP   10.96.51.227    <none>        8888/TCP   85m   app=webserver-1
webservice-2   ClusterIP   10.100.156.38   <none>        8888/TCP   85m   app=webserver-2
----
////

.backend and client pod
----
$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE   IP              NODE      NOMINATED NODE
cirros                 1/1     Running   1          44d   10.47.255.237   cent222   <none>
webserver-846c9ccb8b-1-wns77   1/1     Running   0          13m   10.47.255.236   cent333   <none>
webserver-846c9ccb8b-2-t75d8   1/1     Running   0          13m   10.47.255.235   cent333   <none>

$ kubectl get pod -o wide -l app=webserver-1
NAME                   READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE
webserver-846c9ccb8b-1-wns77   1/1     Running   0          156m   10.47.255.236   cent333   <none>

$ kubectl get pod -o wide -l app=webserver-2
NAME                   READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE
webserver-846c9ccb8b-2-t75d8   1/1     Running   0          156m   10.47.255.235   cent333   <none>
----

two services are created, each with a different clusterIP allocated.
for each service there is a backend pod. later when we verify Ingress from
client we'll see these podIPs in the returned web pages.

.contrail Ingress loadbalancer object
comparing with `single service Ingress`, only difference is one more `service`
loadbalancer:

.loadbalancers (configuration > Networking > Floating IPs)
//image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]
//image::https://user-images.githubusercontent.com/2038044/61432850-aa5f2600-a8ff-11e9-9d9f-932a386bf81f.png[]
//image::https://user-images.githubusercontent.com/2038044/61433696-ff03a080-a901-11e9-96c1-3dfd4886c322.png[]
image::https://user-images.githubusercontent.com/2038044/61790253-befe5b00-ade4-11e9-8a97-40c7d924b7e6.png[]

totally 3 loadbalancers are generated in this test:

* loadbalancer `ns-user-1__ingress-sf` for Ingress `ingress-sf`
* loadbalancer `ns-user-1__webservice-1` for service `webserver-1`
* loadbalancer `ns-user-1__webservice-2` for service `webserver-2`

we won't explore the details of each objects again this time since we've
investigated the key parameters of `service` and `Ingress` loadbalancers in
`single service Ingress` - there is really nothing new here.

==== haproxy process and haproxy.cfg file

in "single service Ingress" example, we've demonstrated the two haproxy
processes invoked by `contrail-svc-monitor` when it sees `loadbalancer` appears
with `loadbalancer_provider` being `opencontrail`. when we removed the "single
service Ingress", because there is no more Ingress left in the cluster, the two
haproxy processes will be killed. 
now with a new Ingress creation, two new haproxy processes are invoked again:

.node `cent222`

----
$ ps aux | grep haproxy
188   29706  0.0  0.0  55572   2940  ?      Ss  04:04  0:00  haproxy  
    -f /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.conf 
    -p /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.pid 
    -sf  29688
----

.node `cent333`

----
[root@b4s42 ~]# ps aux | grep haproxy
188   1936  0.0  0.0  55572   896  ?      Ss  04:04  0:00  haproxy  
    -f /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.conf
    -p /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.pid  
    -sf  1864
----

what interests us is how the simple fan-out Ingress "rules" are programmed in
the haproxy.conf file:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583
$ cat haproxy.conf
global
    daemon
    user haproxy
    group haproxy
    log /var/log/contrail/lbaas/haproxy.log.sock local0
    log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
    tune.ssl.default-dh-param 2048
    ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:......
    ulimit-n 200000
    maxconn 65000
    stats socket
        /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.sock
        mode 0666 level user

defaults
    log global
    retries 3
    option redispatch
    timeout connect 5000
    timeout client 300000
    timeout server 300000

frontend acd9cb38-30a7-4eb1-bb2e-f7691e312625
    option tcplog
    bind 10.47.255.238:80
    mode http
    option forwardfor
    acl 020e371c-e222-400f-b71f-5909c93132de_host hdr(host) -i www.juniper.net
    acl 020e371c-e222-400f-b71f-5909c93132de_path path /qa
    use_backend 020e371c-e222-400f-b71f-5909c93132de if
        020e371c-e222-400f-b71f-5909c93132de_host
        020e371c-e222-400f-b71f-5909c93132de_path


    acl 46f7e7da-0769-4672-b916-21fdd15b9fad_host hdr(host) -i www.juniper.net
    acl 46f7e7da-0769-4672-b916-21fdd15b9fad_path path /dev
    use_backend 46f7e7da-0769-4672-b916-21fdd15b9fad if
        46f7e7da-0769-4672-b916-21fdd15b9fad_host
        46f7e7da-0769-4672-b916-21fdd15b9fad_path


backend 020e371c-e222-400f-b71f-5909c93132de
    mode http
    balance roundrobin
    option forwardfor
    server c13b0d0d-6e4a-4830-bb46-2377ba4caf23 10.97.77.82:8888 weight 1

backend 46f7e7da-0769-4672-b916-21fdd15b9fad
    mode http
    balance roundrobin
    option forwardfor
    server d58689c2-9e59-494b-bffd-fb7a62b4e17f 10.111.234.187:8888 weight 1
----

NOTE: the config file is slightly formatted to make it fit to a page width. 
//also we moved the default backend to the end.

the configuration looks a little bit more complicated than the one for `single
service Ingress`, but the most important part of it doesn't really look hard to
read. 

* the haproxy `frontend` section: it now defines URLs. each is represented by
  two `acl` statement, one for `host`, and the other for `path`.  `host` is the
  domain name and the `path` is what follows the `host` in the URL string. here
  for `simple fan-out Ingress` there are one host `www.juniper.net` with two
  different paths: `\dev` and `\qa`.
* the haproxy `backend` section: now we see 2 of them.  for each `path` there is
  a dedicated service.
* `use_backend` command in `frontend` section: this statement declares the
  ingress rules: `if` the URL request includes a specified `path` that
  matches to what is programmed in one of the two ACLs, "use" the corresponding
  "backend" to forward the traffic. 
  
for example, `acl 020e371c-e222-400f-b71f-5909c93132de_path path /qa` defines
path `/qa`. if the URL request contains such a path, haproxy will use backend
`020e371c-e222-400f-b71f-5909c93132de`, which you can find in `backend` section,
is a UUID referring to service `server d58689c2-9e59-494b-bffd-fb7a62b4e17f
10.111.234.187:8888 weight 1`

////
* `default_backend` defines which backend is the "default": it will be used when
  a haproxy receives a URL request that is other than the two defined one
////

the configuration file can be illustrated in this figure:

.simple fan-out service
//image::https://user-images.githubusercontent.com/2038044/61764125-0f09fd00-ada6-11e9-8a30-61e3a2ed2db3.png[]
//image::https://user-images.githubusercontent.com/2038044/61790000-1cde7300-ade4-11e9-9a78-d1f5c13c7046.png[]
image::https://user-images.githubusercontent.com/2038044/62929112-c8496a80-bd87-11e9-8b53-fdc85ca959d0.png[]

with this proxy.conf file, the haproxy implements our simple fan-out Ingress:

* if the full URL composes `host` "www.juniper.net" and `path` "/dev" , request
  will be dispatched to `webservice-1` (`10.111.234.187:8888`)
* if the full URL composes `host` "www.juniper.net" and `path` "/qa" , request
  will be dispatched to `webservice-2` (`10.97.77.82:8888`)
* for any other URLs the request will be dropped because there is no
  corresponding backend service defined for it.

NOTE: in practice we often need a "default" backend service to process those
HTTP request with non-specified URLs. later in `name-based virtual hosting
Ingress` section we'll introduce how to implement this.

==== verify ingress: from internal

===== problem of old method

we'll compose similar curl commands to test `simple fan-out Ingress`. this time
we give different paths and see how Ingress distribute them.

----
$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.238/dev
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server.  If you entered the URL
manually please check your spelling and try again.</p>
----

it doesn't work!

the reason is our webserver coming with the backend pods does not support the
paths `dev` or `qa` in the URL. checking its code reveals this issue:

----
$ kubectl exec -it webserver-846c9ccb8b-1-wns77 bash
root@webserver-846c9ccb8b-1-wns77:/app# ls
Dockerfile  app.py  requirements.txt  static
root@webserver-846c9ccb8b-1-wns77:/app# cat app.py
# Simple Web-Server
from flask import Flask
import subprocess
app = Flask(__name__)
def workers():
    cmd_ip = 'ifconfig | sed -n 2p | cut -d ":" -f2 | cut -d " " -f1 | tr -d "\n"'
    cmd_hostname = 'hostname | tr -d "\n"'
    ip_addr = str(subprocess.check_output(cmd_ip, shell=True))
    hostname = str(subprocess.check_output(cmd_hostname, shell=True))
    return '''
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b> pod</h2><br><h3>IP address = ''' + ip_addr + '''<br>Hostname = ''' + hostname + '''</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
'''
@app.route('/')                 #<---
def root():
    return workers()
@app.route('/contrail')         #<---
def contrail():
    return workers()
if __name__ == '__main__':
    app.run(debug=True,host='0.0.0.0', port=80)
----

===== workaround

there are several ways to workaround the problem:

* change the current server code and restart the server
* create a new server that supports URLs with the 2 paths

in this section we will demonstrate how to use a python module to create a HTTP
server, and make it to serve the "path" that we give in the URL:

* in each backend pods we'll create a new HTTP server.
* the server is created from python module `SimpleHTTPServer` coming with
  image of each pods.
* to avoid confliction, the new HTTP servers will listen on a different port
  other than default port `80`, which had been used by the existing server
  process.
* accordingly, we update the service's `targetPort` (from `80` to `90`) in order to
  deliver the request to the new HTTP server. 
  
NOTE: don't forget to delete the old services and create the new ones with the
updated parameters.

===== create web pages

in order to make the webserver responding to the URL with given paths, we can
create some web pages with file names being same as the `path`: `dev`, `qa`,
`abc` and etc. to make the test to return consistent output with the one
returned by the old server, we simply use the same template for our web page.
make sure to change the IP address and Hostname to the value of the pod that
this file is going to be copied to. for example pod `webserver-1-846c9ccb8b-wns77` is
with IP `10.47.255.236`, so we generate a file `dev` for this pod:

.generate a webpage `dev`
----
cat <<EOF > dev
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    webserver-1-846c9ccb8b-wns77</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
EOF
----

.copy the webpage into the backend pod

----
$ kubectl cp dev webserver-846c9ccb8b-1-wns77:/app/dev
$ kubectl cp dev webserver-846c9ccb8b-1-wns77:/app/qa
$ kubectl cp dev webserver-846c9ccb8b-1-wns77:/app/abc
$ kubectl exec -it webserver-846c9ccb8b-1-wns77 -- ls -lt
total 24
-rw-r--r--. 1 root root 364 Aug 13 06:26 abc    #<---
-rw-r--r--. 1 root root 364 Aug 13 06:26 dev    #<---
-rw-r--r--. 1 root root 364 Aug 13 06:26 qa     #<---
-rw-r--r--. 1 root root 937 Apr 21  2017 app.py
drwxr-xr-x. 2 root root  23 Apr 21  2017 static
-rw-r--r--. 1 root root 254 Apr  6  2017 Dockerfile
-rw-r--r--. 1 root root   6 Apr  6  2017 requirements.txt
----

similarly, just change the IP and Hostname of this webpage to the vaule of the
other backend pod and then copy into it:

//$ sed 's/10.47.255.236/10.47.255.235/g; s/webserver-846c9ccb8b-wns77/webserver-846c9ccb8b-2-t75d8/g' dev > dev2

----
$ sed 's/10.47.255.236/10.47.255.235/g' dev > temp
$ sed 's/webserver-1-846c9ccb8b-wns77/webserver-2-846c9ccb8b-t75d8/g' temp > dev2
$ kubectl cp dev2 webserver-846c9ccb8b-2-t75d8:/app/dev
$ kubectl cp dev2 webserver-846c9ccb8b-2-t75d8:/app/qa
$ kubectl cp dev2 webserver-846c9ccb8b-2-t75d8:/app/abc
$ kubectl exec -it webserver-846c9ccb8b-2-t75d8 -- ls -lt
total 24
-rw-r--r--. 1 root root 366 Aug 13 06:39 abc    #<---
-rw-r--r--. 1 root root 366 Aug 13 06:39 dev    #<---
-rw-r--r--. 1 root root 366 Aug 13 06:39 qa     #<---
-rw-r--r--. 1 root root 937 Apr 21  2017 app.py
drwxr-xr-x. 2 root root  23 Apr 21  2017 static
-rw-r--r--. 1 root root 254 Apr  6  2017 Dockerfile
-rw-r--r--. 1 root root   6 Apr  6  2017 requirements.txt
----

TIP: we create a page `abc` that has no corresponding path to test the scenario
when incoming HTTP request contains an "unknown" path.

===== start new web server

for each pod open a seperate terminal, then start a web server with python
module `SimpleHTTPServer`, listening on port 90:

----
$ kubectl exec -it webserver-846c9ccb8b-1-wns77 -- python -m SimpleHTTPServer 90
Serving HTTP on 0.0.0.0 port 90 ...
$ kubectl exec -it webserver-846c9ccb8b-2-t75d8 -- python -m SimpleHTTPServer 90
Serving HTTP on 0.0.0.0 port 90 ...
----

////
create some files with name being same as the `paths`: `dev`, `qa`, and `abc`.

----
kubectl exec -it webserver-846c9ccb8b-1-s2zn9 -- bash -c "echo webserver-846c9ccb8b-1-s2zn9:10.47.255.249 > dev"
kubectl exec -it webserver-846c9ccb8b-1-s2zn9 -- bash -c "echo webserver-846c9ccb8b-1-s2zn9:10.47.255.249 > qa"
kubectl exec -it webserver-846c9ccb8b-1-s2zn9 -- bash -c "echo webserver-846c9ccb8b-1-s2zn9:10.47.255.249 > abc"
kubectl exec -it webserver-846c9ccb8b-2-k9x26 -- bash -c "echo webserver-846c9ccb8b-2-k9x26:10.47.255.248 > dev"
kubectl exec -it webserver-846c9ccb8b-2-k9x26 -- bash -c "echo webserver-846c9ccb8b-2-k9x26:10.47.255.248 > qa"
kubectl exec -it webserver-846c9ccb8b-2-k9x26 -- bash -c "echo webserver-846c9ccb8b-2-k9x26:10.47.255.248 > abc"
----

////

===== the new all-in-one yaml file

the last step before testing our Ingress is to update the previous services
`targetPort` 80 in the "all-in-one" yaml file with the new HTTP server's port
90. delete the current services and apply the updated all-in-one yaml file
again:

.updated yaml file ingress-simple-fanout2.yaml
----
$ diff ingress-simple-fanout.yaml ingress-simple-fanout2.yaml
26c26
<     targetPort: 80
---
>     targetPort: 90
38c38
<     targetPort: 80
---
>     targetPort: 90
----

.delete the old services and apply yaml file again
----
$ kubectl delete svc/webservice-1
service "webservice-1" deleted

$ kubectl delete svc/webservice-2
service "webservice-2" deleted

$ kubectl apply -f ingress/ingress-simple-fanout2.yaml
ingress.extensions/ingress-sf unchanged
service/webservice-1 created    #<---
service/webservice-2 created    #<---
deployment.extensions/webserver-1 unchanged
deployment.extensions/webserver-2 unchanged
----

===== test the URL with different paths

----
$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/dev | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-wns77
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/qa | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = webserver-846c9ccb8b-2-t75d8
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/abc | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.
----

////
----
$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.238/dev
webserver-846c9ccb8b-1-s2zn9:10.47.255.249

$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.250/qa
webserver-846c9ccb8b-2-k9x26:10.47.255.248

$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.250/abc
<html><body><h1>503 Service Unavailable</h1>
No server is available to handle this request.
</body></html>

$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.250
<html><body><h1>503 Service Unavailable</h1>
No server is available to handle this request.
</body></html>

----
////

we still use the `curl` command to trigger HTTP requests towards the ingress's
loadbalancer IP. the return proves our `Ingress` works: the 2 requests towards
"/qa" and "/dev" paths are proxied to 2 different backend pods, through 2
backend services `webservice-1` and `webservice-2` respectively. 

the third request with a path `abc` is a "unknown" URL which does not have a
matching service in `Ingress` configuration, so it goes to the default backend
service - `service-1`.  same for the fourth and fifth requests, without a path,
or with a different Host, the URL become unknown to our Ingress so it won't be
served.

////
same rule applies to the fourth request. without given a URL via `-H`, `curl`
will fill `host` with the request IP address, `10.47.255.238` in this test, and
since that "URL" does not have a defined backend service so the default backend
service will be used. 

in our test, we have just one backend pod for each
service, so the podIP in returned webpage tells who is who. except in the second
test the returned podIP `10.47.255.235` represent `webservice-2`, all other three
tests returns podIP for `webservice-1`, as expected.
////

==== verify ingress: from external (Internet host)

to test simple fan-out Ingress from outside of the cluster, again we can 
use `curl` CLI.

////
with chrome, this time we launch two chrome page side by side, and input URLs
`www.juniper.net/qa` and `www.juniper.net/dev`. By keep refreshing the 2 pages
we can confirm "qa" page is always returned by RC `rc-webserver-1` pod
`10.47.255.236`, "dev" page is always returned by RC `rc-webserver-2` pod
`10.47.255.235`. 

//TODO: capture
image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

same result can be seen from `curl` also. 
////

the command is the same as what we've seen when initiating the HTTP request from
inside of a pod, except this time we are initiating from an Internet host. we
will send the HTTP requests to the Ingress's public FIP, instead of its internal
podIP.

from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1/qa | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = webserver-846c9ccb8b-2-t75d8
                                    [giphy]

$ curl -H 'Host:www.juniper.net' 101.101.101.1/dev | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-wns77
                                    [giphy]

$ curl -H 'Host:www.juniper.net' 101.101.101.1/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ curl -H 'Host:www.juniper.net' 101.101.101.1/abc | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ curl -H 'Host:www.cisco.com' 101.101.101.1/dev | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.
----

everything works as expected!

=== name-based virtual hosting Ingress

Name-based virtual hosts support routing HTTP traffic to multiple host names at
the same IP address. based on the URL and rules, an ingress loadbalancer directs
traffic to different backend services, and each service direct traffic to its
backend pod. 

    www.juniper.net --|                 |-> webservice-1
                      |  101.101.101.1  |
    www.cisco.com   --|                 |-> webservice-2

to demonstrate `virtual host` type of Ingress, the objects that we need to
create are:

* an `Ingress` object: the rules, mapping 2 URLs to 2 backend services
* 2 backend services objects
* each service requires at least one pod as backend

again we use the same `cirros` pod as cluster-internal HTTP client we've used in
earlier examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `ingress` definition

in our virtual host ingress test lab, we define the following rules:

* request toward URL `www.juniper.net` will be directed to a service `webservice-1`
  with `servicePort` 8888
* request toward URL `www.cisco.com` will be directed to a service `webservice-2`
  with `servicePort` 8888
* request toward any URLs other than these 2, will be directed to `webservice-1`
  with `servicePort` 8888

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: webservice-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: webservice-2
              servicePort: 8888
            path: /
----

.backend `service` and `pod` definition

same exact service and RC definition that were used in simple fan-out Ingress
can be used here.

==== an "all in one" yaml file

----
$ cat ingress/ingress-test.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: webservice-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: webservice-2
              servicePort: 8888
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
---
apiVersion: v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

.`apply` the all-in-one yaml file

----
$ kubectl apply -f ingress/ingress-virtual-host-test.yaml
ingress.extensions/ingress-vh created
service/webservice-1 created
service/webservice-2 created
deployment.extensions/webserver-1 created
deployment.extensions/webserver-2 created
----

the Ingress, two services and two RC objects are now created.

==== examine ingress objects

let's start to look at the Ingress object.

----
$ kubectl get ingresses.extensions -o wide
NAME        HOSTS                          ADDRESS                      PORTS  AGE
ingress-vh  www.juniper.net,www.cisco.com  10.47.255.248,101.101.101.1  80     8m27s
----

the internal and external Ingress IP remains the same, but this time we see 2
hosts, each representing a domain name.

----
$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    ......
    generation: 1
    name: ingress-vh
    namespace: ns-user-1
    resourceVersion: "830991"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-vh
    uid: 8fd3e8ea-9539-11e9-9e54-0050569e6cfc
  spec:
    backend:
      serviceName: webservice-1
      servicePort: 8888
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: webservice-1
            servicePort: 8888
          path: /
    - host: www.cisco.net
      http:
        paths:
        - backend:
            serviceName: webservice-2
            servicePort: 8888
          path: /
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

the rules are defined properly, within each rule there is a mapping from a
`host` to the corresponding service. 

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

the services, pods and FIP prefix advertisement to gateway router are all
exactly the same as those in simple fan-out Ingress so we'll skip them.

==== exploring Ingress loadbalancer objects

//TODO: should skip if simple fan-out Ingress can capture this.
////
.contrail object: SI, port tuple, VMI
image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]
////

loadbalancers created in this test is almost the same as the ones created in
simple fan-out Ingress test:

.loadbalancers
image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]

3 loadbalancers are generated after we applied the all-in-one yaml file.

* 1 for Ingress
* 2 for services

next we can check haproxy configuration file for `name-based virtual host
Ingress`.

==== examine `haproxy.conf` file

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
        ulimit-n 200000
        maxconn 65000
        stats socket /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.sock mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend acf8b96d-b322-4bc2-aa8e-0611baa43b9f

        option tcplog
        bind 10.47.255.248:80                   #<---Ingress loadbalancer podIP
        mode http
        option forwardfor

        #map www.juniper.net to backend "xxx4e6a681ec8e6", which maps to "webservice-1"
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host hdr(host) -i www.juniper.net
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path path /
        use_backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6 if
            77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host
            77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path

        #map URL www.cisco.net to backend "xxx44d1ca50a92f", which maps to "webservice-2"
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host hdr(host) -i www.cisco.net
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path path /
        use_backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f if
            1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host
            1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path

        #map other URLs, to default backend "xxx4e6a681ec8e6"
        default_backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46

backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6    #<---webservice-1
        mode http
        balance roundrobin
        option forwardfor
        server 33339e1c-5011-4f2e-a276-f8dd37c2cc51 10.101.158.92:8888 weight 1

backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f    #<---webservice-2
        mode http
        balance roundrobin
        option forwardfor
        server aa0cde60-2526-4437-b943-6f4eaa04bb05 10.104.4.232:8888 weight 1

backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46    #<---default
        mode http
        balance roundrobin
        option forwardfor
        server e8384ee4-7270-4272-b765-61488e1d3e9c 10.101.158.92:8888 weight 1
----

NOTE: the config file is slightly formatted to make it fit to a page width. also
we moved the default backend to the end.

here are the highlights:

* the haproxy `frontend` section defines each URL, or `host`, and its path. here the
  2 hosts are `www.juniper.net` and `www.cisco.com`. `path` is what follows the
  host part in the URL string, in our case both are `/`. 
* the haproxy `backend` section defines the `server`, which is `service` in our
  case. it has a format of `serviceIP:servicePort`, which is the exact `service`
  object we've created using the all-in-one yaml file.
* `use_backend` command in `frontend` section declares the ingress rules: `if` the
  request includes a specified URL and path, "use" the corresponding "backend"
  to forward the traffic
* `default_backend` defines which backend is the "default": it will be used when
  a haproxy receives a URL request that is other than the two defined one

through this configuration, the haproxy implemented our ingress:

////
.haproxy frontend:

* 10.47.255.248:80 is the frontend IP and port facing clients

.haproxy backend:
////

* `www.juniper.net` and `/` composes the full URL, request will be dispatched to
  `webservice-1` (`10.101.158.92:8888`)
* `www.cisco.net` and `/` composes the full URL, request will be dispatched to
   `webservice-2` (`10.104.4.232:8888`)
* other URLs goes to default backend which is service `webservice-1`

==== verify ingress: from internal

we've explored a lot about ingress configuration and objects examination, now it
is time to verify the test result. since the `Ingress` serves both inside and
outside of the cluster, our verification will start from the cirros pod
inside of cluster, then from the Internet host outside of it.

.from inside of cluster

----
$ kubectl exec -it cirros -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = webserver-846c9ccb8b-2-m2272

$ kubectl exec -it cirros -- \
    curl -H 'Host:www.google.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]

$ kubectl exec -it cirros -- \
    curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]
----

we still use the `curl` command to trigger HTTP requests towards the ingress's
loadbalancer IP. the return proves our `Ingress` works: the 2 requests towards
"juniper" and "cisco" URL is proxied to 2 different backend pods, through 2
backend services `webservice-1` and `webservice-2` respectively. the third request
towards "google" is a "unknown" URL which does not have a matching service in
`Ingress` configuration, so it goes to the default backend service -
`webservice-1`. 

same rule applies to the fourth request. without given a URL via `-H`, `curl`
will fill `host` with the request IP address, `10.47.255.238` in this test, and
since that "URL" does not have a defined backend service so the default backend
service will be used. in our test, for each service we use backend pods spawned
by same RC , so the podIP in returned webpage tells who is who. except in the
second test the returned podIP `10.47.255.235` represent `webservice-2`, all
other three tests returns podIP for `webservice-1`, as expected.

NOTE: The `-H` option is important in Ingress test with `curl`. it carries the
full URL in HTTP payload that the loadbalancer is waiting for. without it the
HTTP header will carry `Host: 10.47.255.238`, which has no matching rule, so it
will be treated same as with a unknown URL.

==== verify ingress: from external (Internet host)

the more exciting part of the test is to visit the URLs from external. overall
we hope `Ingress` meant to provide the "extra power" to expose services to the
Internet host, even though it does not have to. 

to make sure the URL resolves to the right FIP address, we need to update
`/etc/hosts` file by adding one line in the end - you don't want to just end up
with a nice webpage from Juniper/cisco official website as your test result.

----
# echo "101.101.101.1  www.juniper.net www.cisco.com www.google.com" >> /etc/hosts
# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
101.101.101.1  www.juniper.net www.cisco.com www.google.com     #<---
----

now, from internet host's "desktop", we launch two chrome page side by side, and
input URLs `www.juniper.net` and `www.cisco.com`. By keep refreshing the 2 pages
we can confirm "juniper" page is always returned by Deployment `webserver-1` pod
`10.47.255.236`, "cisco" page is always returned by Deployment `webserver-2` pod
`10.47.255.235`. we launch a third chrome page and input `www.google.com`, we
see "google" page is returned by the same pod serving "Juniper" URL.

image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

same result can be seen from `curl` also. the command is exactly the same as
what we've seen when testing from a pod, except this time we send requests to
Ingress FIP, instead of the Ingress internal podIP.

from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = webserver-846c9ccb8b-2-m2272

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = webserver-846c9ccb8b-1-g65dg
                                    [giphy]
----

everything works!

=== contrail ingress packet flow

let's examine the external service request flow step by step in our lab setup,
starting from Internet host, through gateway router, haproxy, to the destination
pod. you've seen this figure:

.Ingress traffic flow: access from external
image::https://user-images.githubusercontent.com/2038044/61061427-3f2ac680-a3ca-11e9-9364-f11bea477319.png[]

earlier we use gateway router's VRF routing table to show the packet next
hops, this time we'll introduce more useful tools available in node and client
host. 

after understanding these tools you will be able to troubleshoot both the
routing and forwarding plane problems in contrail kubernetes environment. 

the tools are:

* curl with debug option
* tcpdump packet capture on tap interface and fabric interface
* vrouter flow table with prefix filter
* vrouter VRF routing table
* shell script

//we'll then conclude the ingress section by a figure showing the end to end flow.

==== Internet Host -> gateway router

----
[root@cent-client ~]# curl -vH 'Host:www.juniper.net' 101.101.101.1
* About to connect() to 101.101.101.1 port 80 (#0)
*   Trying 101.101.101.1...
* Connected to 101.101.101.1 (101.101.101.1) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Accept: */*
> Host:www.juniper.net
>
* HTTP 1.0, assume close after body
< HTTP/1.0 200 OK
< Content-Type: text/html; charset=utf-8
< Content-Length: 359
< Server: Werkzeug/0.12.1 Python/2.7.12
< Date: Tue, 02 Jul 2019 16:50:46 GMT
* HTTP/1.0 connection set to keep alive!
< Connection: keep-alive
<

<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    webserver-846c9ccb8b-1-g65dg</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
* Connection #0 to host 101.101.101.1 left intact
----

first we use `-v` option in `curl` command. with this option it prints more
verbose information about the HTTP interaction. `>` lines are the messages
content that `curl` sent out, and `<` lines are message content that it receives
from remote. from the interaction we see `curl` sent a HTTP `GET` with path `/`
to the FIP 101.101.101.1, with `Host` filled with "juniper" URL. it gets the
response with code `200 OK`, indicating the request has succeeded. there are a
bunch of other headers in the response that are not important for our test so we
can skip. the rest part of the response is the HTML source code of a returned
web page. the connection is closed immediately afterward.

NOTE: the `curl` tool implementation will always close the TCP session right
after the HTTP request is responded. although this is a safe and clean behavior
in practice, it may bring some difficulties to our illustration. we prefer the
connection to remain open for a while so we can take time to investigate some
details during our analysis. there are some methods to workaround that.  e.g.
in "service" section you've seen we install a large file in the webserver and
try to pull it from curl, that way the file transfer process "holds" the TCP
session. in this test we will show another method.
//how to use a small shell script to workaround this problem.

==== gateway router -> active haproxy node: MPLS over GRE

we've seen gateway router's routing table earlier, so we already know the packet
will be sent to active haproxy node `cent222` via MPLSoGRE tunnel. to
demonstrate the forwarding flow, this time we collect the flow table in node
`cent222`. 

NOTE: with a filter `--match 15.15.15.2`. only flow entries with Internet Host
IP is printed.

////
----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
......
Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)

   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)
----
* a user from Internet Host sends a http request by typing the URL
  "http://www.juniper.net" and hit enter
* DNS resolves the host to FIP address
* via default route Internet Host send HTTP request to gateway router's VRF
* gateway router learns the VIP prefix in VRF with next-hop pointing to the
  compute node running active haproxy, in this case node `cent222`

////

----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
Flow table(size 80609280, entries 629760)

Entries: Created 586803 Added 586861 Deleted 1308 Changed 1367Processed 586803 Used Overflow entries 0
(Created Flows/CPU: 147731 149458 144549 145065)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   114272<=>459264       15.15.15.2:58282                                    6 (2->2)
                         101.101.101.1:80
(Gen: 3, K(nh):89, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):61,
 Stats:2/112,  SPort 50985, TTL 0, Sinfo 192.168.0.204)

   459264<=>114272       10.47.255.238:80                                    6 (2->5)
                         15.15.15.2:58282
(Gen: 1, K(nh):89, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):89,
 Stats:1/74,  SPort 60289, TTL 0, Sinfo 8.0.0.0)

(vrouter-agent)[root@cent222 /]$ nh --get 61
Id:61         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:3316       Vrf:0
              Flags:Valid, MPLSoGRE, Etree Root,
              Oif:0 Len:14 Data:f0 1c 2d 41 90 00 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:192.168.0.204
----
////
(vrouter-agent)[root@cent222 /]$ nh --get 89
Id:89         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:7          Vrf:2
              Flags:Valid, Policy, Etree Root,
              EncapFmly:0806 Oif:8 Len:14
              Encap Data: 02 c0 0a c1 e6 6c 00 00 5e 00 01 00 08 00
////

this rephrases the same fact as what we've seen from gateway router's VRF table:

* the first flow entry displays the source and destination of the http request,
  it is coming from Internet host (`15.15.15.2`) and lands the FIP in current
  node `cent222`

* `S(nh):61` in is the next hop to the source of the request - the Internet
  host.  this is similiar concept like the reverse path forwarding(RPF). vrouter
  always maintains the path toward the source of the packet in the flow.

* `nh --get` command resolves the nexthop 61 with more details, we see a
  `MPLSoGRE` flag is set, `Sip` and `Dip` is the two end of the GRE tunnel,
  they are current node and gateway router's loopback IP respectively. 
  
overall this confirms the request packet from Internet host traverses gateway
router, and via MPLSoGRE tunnel it hit the Ingress external VIP `101.101.101.1`.
NAT will happen and we'll look into it next.

==== FIP -> loadbalancer IP: NAT
////
----
(vrouter-agent)[root@cent222 /]$ flow --match 101.101.101.1
Flow table(size 80609280, entries 629760)

Entries: Created 1856648 Added 1856785 Deleted 3015 Changed 3234Processed 1856648 Used Overflow entries 0
(Created Flows/CPU: 467916 472342 457241 459149)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead
Listing flows matching ([101.101.101.1]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)

(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.238
......
Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)
----
////

to verify the NAT operation, we only need to dig a little bit more out of the
same flow output.

* the `Action` flag, `N(D)` in the first entry and `N(S)` in the second,
  indicate the two type of NAT operations:
  - destination NAT - `DNAT`, destination FIP `101.101.101.1` which is the
    external Ingress will be translated to the Ingress internal VIP
  - source NAT - `SNAT`, source IP which is the internal Ingress VIP
    `10.47.255.238` will be translated to the Ingress external VIP
  
* in vrouter flow, the second flow entry is also called a "reverse flow" of the
  first one. it is the flow entry vrouter uses to send returning packet towards
  Internet host.  from Ingress loadbalancer's perspective it only uses
  `10.47.255.248` assigned from the default pod network as its source IP, it
  does not knows anything about the FIP. same thing for the external Intenet host, it
  only knows how to reach the FIP and has no clues about the private
  Ingress internal VIP. it is vrouter that is doing the two way NAT translations
  in between.

in summary, what the flow table of active haprox node `cent222` tells is that on
receiving the packet destined to the Ingress FIP, vrouter on node `cent222`
performs NAT and translates destination FIP (`101.101.101.1`) to the Ingress's
internal VIP (`10.47.255.238`).  after that the packet lands the Ingress
loadbalancer's VRF, where active haproxy process is watching. next HTTP proxy
will happen.

==== active haproxy -> service IP: MPLS over UDP

now the packet lands in Ingress loadbalancer's VRF and it is in the frontend of
the haproxy.  what the haproxy supposes to do is:

* haproxy listening on the frontend IP (Ingress internal podIP/VIP) see the packet
* haproxy checks the ingress rule programmed in its config file, decides that
  the requests need to be proxied to service IP of `webservice-1`. 
* vrouter checks the Ingress loadbalancer's VRF table and sees the prefix of
  `webservice-1` IP is learned from a destination node `cent333`. between compute
  node the forwarding path is programmed with MPLSoUDP tunnel, so it pushes a
  MPLS label and send it through MPLS over UDP tunnel.

to verify haproxy process packet processing details, we capture packets on
the physical interface of node `cent222`, where the active haproxy process is
running.

//image::https://user-images.githubusercontent.com/2038044/60518123-e1ea9300-9cae-11e9-82ec-d341e32e42c8.png[]
image::https://user-images.githubusercontent.com/2038044/60539848-aadea680-9cdb-11e9-8896-c4824d17dd9d.png[]

from the wireshark screenshot, we see clearly that:

* the HTTP request packet is "forwarded" to the service IP, which is the other
  node `cent333`. that is why we see underlay destination IP of the request is
  `10.169.25.21`
* sending overlay packets between compute node requirs MPLSoUDP tunnel. 

everything is working as expected.

==== "forward" vs "proxy"

if you are observant enough, you should have noticed something "weird" in
this capture. questions are:

* shouldn't the source IP address be the Internet host's IP `15.15.15.2`,
  instead of loadbalancer's frontend IP? 
* is the packet "forwarded" at all?
* is the transaction within the same TCP session from Internet host, accrossing
  gateway router and loadbalancer node `cent222`, all the way up to the backend
  pod sitting in node 'cent333`?

The answers are NO. the haproxy in this test is doing layer 7 (application
layer) loadbalancing. what it does is:

* establish TCP connection with Internet host and keep monitoring the HTTP request. 
* whenever it see an request coming in, it checks its rule and initiates a brand
  new TCP connection to the corresponding backend
* it "copies" the original HTTP request from Internet host and "paste" into the
  new TCP connection with its backend. 

so, to put it precisely, the http request is "proxied", not "forwarded".

//capture on haproxy interface
//image::https://user-images.githubusercontent.com/2038044/60540296-d1e9a800-9cdc-11e9-8914-fbe4fc59ae60.png[]

==== service IP -> backend pod IP: NAT

at the moment we know the http request is "proxied" to haproxy's backend. that
backend is a kubernetes `service`, and to reach the `service` the request is
sent to node `cent333` where all backend pod is sitting.

on destination node `cent333`, when packet comes in from Ingress internal IP
`10.47.255.238` toward the service IP `10.99.225.17` of `webservice-1`, vrouter
again translates the service IP to the backend podIP `10.47.255.236`. the
translation is a NAT operation, pretty much the same as what we've seen many
times earlier, whenever FIP is involved. 

packet capture on the backend pod interface also reveals the packet interaction
between the Ingress intenral IP and backend podID.

----
$ tcpdump -ni tapeth0-baa392 -v
12:01:07.701956 IP (tos 0x0, ttl 63, id 32663, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [S], cksum 0xd88d (correct), seq 2129282145, win 29200, options [mss 1420,sackOK,TS val 515783670 ecr 0,nop,wscale 7], length 0
12:01:07.702012 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.236.http > 10.47.255.238.51968: Flags [S.], cksum 0x1468 (incorrect -> 0x8050), seq 3925744891, ack 2129282146, win 28960, options [mss 1460,sackOK,TS val 515781436 ecr 515783670,nop,wscale 7], length 0
12:01:07.702300 IP (tos 0x0, ttl 63, id 32664, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [.], cksum 0x1f57 (correct), ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 0
12:01:07.702304 IP (tos 0x0, ttl 63, id 32665, offset 0, flags [DF], proto TCP (6), length 159)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [P.], cksum 0x6fac (correct), seq 1:108, ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 107: HTTP, length: 107
        GET / HTTP/1.1
        User-Agent: curl/7.29.0
        Accept: */*
        Host:www.juniper.net
        X-Forwarded-For: 15.15.15.2

12:01:07.702336 IP (tos 0x0, ttl 64, id 12224, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.51968: Flags [.], cksum 0x1460 (incorrect -> 0x1eee), ack 108, win 227, options [nop,nop,TS val 515781436 ecr 515783671], length 0
12:01:07.711882 IP (tos 0x0, ttl 64, id 12225, offset 0, flags [DF], proto TCP (6), length 69)
    10.47.255.236.http > 10.47.255.238.51968: Flags [P.], cksum 0x1471 (incorrect -> 0x5f06), seq 1:18, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 17: HTTP, length: 17
        HTTP/1.0 200 OK
12:01:07.712032 IP (tos 0x0, ttl 64, id 12226, offset 0, flags [DF], proto TCP (6), length 550)
    10.47.255.236.http > 10.47.255.238.51968: Flags [FP.], cksum 0x1652 (incorrect -> 0x1964), seq 18:516, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 498: HTTP
12:01:07.712152 IP (tos 0x0, ttl 63, id 32666, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [.], cksum 0x1ec7 (correct), ack 18, win 229, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712192 IP (tos 0x0, ttl 63, id 32667, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [F.], cksum 0x1ccb (correct), seq 108, ack 517, win 237, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712202 IP (tos 0x0, ttl 64, id 12227, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.51968: Flags [.], cksum 0x1460 (incorrect -> 0x1cd5), ack 109, win 227, options [nop,nop,TS val 515781446 ecr 515783681], length 0
----

this may not be very convincing in one aspect. the packet capture shows the
communication is between loadbalancer IP and podIP, that part is fine. problem
is this is solely from the pod's perspective "after" the NAT operation. it does
not shows what happens right before NAT.

we can rely on the flow table again, as what we've seen on node `cent222` where
vrouter does NAT between FIP and loadbalancer IP. the problem is, as we
mentioned earlier, `curl` get its job done pretty fast, it open the session,
send request, get response, then close it. in fast this process is too fast to
be captured in the flow table. as soon as you hit "enter" to curl command,
everything is done in less than 2 or even 1 second. by the time you type in
flow command, everything is done and you end up with empty table. we could
change the server's behavior - let it send a big file to hold the session, or
we can repeat the test to take some chance. in the client terminal with a
one-liner script we can repeat the test over and over, then in compute terminal
using same script we can keep dumping the flow table. over the time we will
have the chance to capture the flow table at the right moment. then we stop the script
and investigate the table.

compute side script:

----
while :; do flow --match 10.47.255.238; sleep 0.2; done
----

Internet host side script:

----
while :; do curl -H 'Host:www.juniper.net' 101.101.101.1 | \
lynx -stdin --dump | cat; sleep 3; done
----

here is the flow table we captured before it's gone.

----
evrouter-agent)[root@cent333 /]$ flow --match 10.47.255.238
Flow table(size 80609280, entries 629760)
Entries: Created 482 Added 482 Deleted 10 Changed 10Processed 482 Used Overflow entries 0
(Created Flows/CPU: 163 146 18 155)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   403188<=>462132       10.47.255.236:80                                    6 (2->4)
                         10.47.255.238:57760
(Gen: 1, K(nh):23, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):23,
 Stats:2/140,  SPort 52190, TTL 0, Sinfo 4.0.0.0)

   462132<=>403188       10.47.255.238:57760                                 6 (2->2)
                         10.99.225.17:8888
(Gen: 1, K(nh):23, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:3/271,  SPort 65421, TTL 0, Sinfo 10.169.25.20)
----

obviously the second entry is triggered by the incoming request. haproxy follows
its rules inheritated from our Ingress definition and dispatchs the request of
"juniper" URL to `webservice-1`, whose IP:port is `10.99.225.17:8888`. vrouter see
the service IP and knows that supposes to go to backend podIP `10.47.255.236`.
it does NAT between the two IPs. 

finally, pod sees the HTTP request and responds back with a web page.

////
----
$ tcpdump -ni tapeth0-baa392
23:37:29.754864 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [S], seq 1528773587, win 29200, options [mss 1420,sackOK,TS val 384765722 ecr 0,nop,wscale 7], length 0
23:37:29.754922 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [S.], seq 953745157, ack 1528773588, win 28960, options [mss 1460,sackOK,TS val 384763489 ecr 384765722,nop,wscale 7], length 0
23:37:29.755247 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 0
23:37:29.755253 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [P.], seq 1:71, ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 70: HTTP: GET / HTTP/1.1
23:37:29.755291 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 71, win 227, options [nop,nop,TS val 384763489 ecr 384765724], length 0
23:37:29.766886 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [P.], seq 1:18, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 17: HTTP: HTTP/1.0 200 OK
23:37:29.767032 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [FP.], seq 18:516, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 498: HTTP
23:37:29.767188 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 18, win 229, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767210 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [F.], seq 71, ack 517, win 237, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767218 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 72, win 227, options [nop,nop,TS val 384763501 ecr 384765736], length 0
----
////

==== returning traffic

on the reverse direction, podIP runs webserver and responds with it's web page.
the response follows the reverse path of the request:

* pod responds to loadbalancer frontend IP, across MPLSoUDP tunnel
* vrouter on node `cent333` perform source NAT, translating podIP into service IP
* respond reaches to active haproxy running on node `cent222`
* haproxy terminate the tcp connection with backend pod, "copies" the http
  response, and "paste" into its connection with the remote Internet host
* vrouter on node `cent222` perform source NAT, translating loadbalancer
  frontend IP to FIP
* response is sent to gateway router, which forwards to Internet host
* Internet host gets the response.

== contrail multiple interface pod

=== multiple interface pod introduction

in Kubernetes cluster, typically each pod only has one network interface (except
the `loopback` interface). In reality, there are scenarios where multiple
interfaces are required. e.g. in contrail solution service chain model, a
service instance typically needs a "left", "right" and optionally a "management"
interface to manipulate the service traffic. a pod may requires a "data
interface" to carry the service traffic, and a "management interface" for the
reachability detection. Service Providers also tend to keep the management and
tenant networks independent for isolation, and management purpose. Multiple
interfaces provide a way for containers to be connected to multiple devices in
multiple networks simultaneously.

=== contrail as a CNI

////
As you probably already know containers use namespaces to isolate resources and
rate limit their use. Linux’s network namespaces are used to glue container
processes and the host networking stack. Docker spawns a container in the
containers own network namespace and later on runs a veth pair (a cable with two
ends) between the container namespace and the host network stack
////

in container technology, A virtual network device pair abstraction (the `veth`)
is functioning pretty much like a virtual "cabel", that can be used to create
tunnels between network namespaces. one end of it is "plugged" in the container
and the other end is in the host. it can also be used to create a bridge to a
physical network device in another namespace.

A "CNI plugin" is the one who is responsible for inserting the network
interface, that is one end of the veth pair, into the container network
namespace.  it will also makes all necessary changes on the host. e.g. attaching
the other end of the veth into a bridge, assigning IP, configuring routes, and
so on.

//TODO: need redraw
.container and veth pair
image::https://user-images.githubusercontent.com/2038044/60554760-ee9ad580-9d06-11e9-9628-f01af759f6e1.png[]


there are many such CNI plugin implementations that are publicly available
today. contrail is one of them.  for a comprehensive list you can check
https://github.com/containernetworking/cni where contrail is also listed. for
example, `multus-cni`, is another CNI plugin that "enables attaching multiple
network interfaces to pods". `multus-cni` 's multipe-network support is
accomplished by Multus calling multiple other CNI plugins. because each plugin
will create its own network, multiple plugins make the pod be able to have
multiple networks. one of the main advantages that contrail provides, comparing
with `mutus-cni` and all other current implementations in the industry, is that
contrail by itself provides the ability to attach multiple network interfaces to
a kubernetes pod, without the need to call any other plugins. this brings
support to a truly "multi-homed" pod.

contrail CNI follows the Kubernetes Network `CRD` (Custom Resource Definition)
Standard to provide a standardized method to specify the configurations for
additional network interfaces. there is no change to the standard kubernetes
upstream APIs, making the implementation coming with the most compatibilities.

=== CRD and `contrail-kube-manager`

////
Kubernetes supports a custom extension to represent networks in its object
model, through its `CustomResourceDefinition(CRD)` feature. This extension adds
support for a new kind of object called `NetworkAttachmentDefinition`, which
represents a network in Kubernetes data model.
////

in contail, a CRD object defines the template for a network object
`NetworkAttachmentDefinition`, which contains all information about each
network's specification, and tells Kubernetes API how to understand and expose
it. in contrail setup the CRD is created by `contrail-kube-manager` (`KM`),
running as a docker container typically. `KM` interfaces with Kubernetes API
server and converts objects from kube-apiserver to Contrail config-api server.

when bootup, `KM` validates if a network CRD
`network-attachment-definitions.k8s.cni.cncf.io` is found in the Kubernetes API
server, and creates one if not yet.

here is a `CRD` object template:

----
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: network-attachment-definitions.k8s.cni.cncf.io
spec:
  group: k8s.cni.cncf.io
  version: v1
  scope: Namespaced
  names:
    plural: network-attachment-definitions
    singular: network-attachment-definition
    kind: NetworkAttachmentDefinition
    shortNames:
    - net-attach-def
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            config:
             type: string
----

in contrail kubernetes setup, the CRD has been created and can be verified:

----
$ kubectl get crd
NAME                                             CREATED AT
network-attachment-definitions.k8s.cni.cncf.io   2019-06-07T03:43:52Z
----

////
----
$ kubectl get crd -o yaml
apiVersion: v1
items:
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    creationTimestamp: 2019-06-07T03:43:52Z
    generation: 1
    name: network-attachment-definitions.k8s.cni.cncf.io
    resourceVersion: "1170"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/network-attachment-definitions.k8s.cni.cncf.io
    uid: 77f15393-88d6-11e9-a8b1-0050569e6cfc
  spec:
    additionalPrinterColumns:
    - JSONPath: .metadata.creationTimestamp
      description: |-
        CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.

        Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
      name: Age
      type: date
    group: k8s.cni.cncf.io
    names:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    scope: Namespaced
    version: v1
    versions:
    - name: v1
      served: true
      storage: true
  status:
    acceptedNames:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    conditions:
    - lastTransitionTime: 2019-06-07T03:43:52Z
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: null
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
    storedVersions:
    - v1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----
////

with this CRD object present, we have the ability to create a
`NetworkAttachmentDefinition` object as our virtual-network.

=== `NetworkAttachmentDefinition` object

to create a virtual-network from kubernetes, use a yaml template like this:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: <network-name>
  namespace: <namespace-name>
  annotations:
    "opencontrail.org/cidr" : [<ip-subnet>]
    "opencontrail.org/ip_fabric_snat" : <True/False>
    "opencontrail.org/ip_fabric_forwarding" : <True/False>
spec:
  config: '{
    “cniVersion”: “0.3.0”,
    "type": "contrail-k8s-cni"
}'
----

through `kind: NetworkAttachmentDefinition` which is created by CRD, we can
define new VNs. like many other standard kubernetes object, basically you
specify the VN name, namespace under `metadata`, and `annotations` which
is used to carry additional information about a network. in contrail
`NetworkAttachmentDefinition` implementation, the `annotations` bring:

* `opencontrail.org/cidr`: CIDR, which defines the subnet for a VN
* `opencontrail.org/ip_fabric_forwarding`: a flag to enable/disable `ip fabric
  forwarding` feature
* `opencontrail.org/ip_fabric_snat`: a flag to enable/disable `ip fabric snat`
  feature

****
with the contrail `ip-fabric-forwarding` feature, A VN can be marked for IP
fabric based forwarding without tunneling. When two virtual networks with this
type of configuration communicate with each other, overlay traffic will be
forwarded directly using the underlay. 

With the Contrail `ip-fabric-snat` feature, pods that are in the overlay can
reach the Internet without floating IPs or a logical-router. The
`ip-fabric-snat` feature uses compute node IP for creating a source NAT to reach
the required services and is applicable only to pod networks. 

`ip fabric forwarding` and `ip fabric snap` features are not covered in this
book.
****

alternatively, you can define a new VN by referring an existing VN:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: extns-network
  annotations:
    "opencontrail.org/network" : '{"domain":"default-domain", "project": "k8s-extns", "name":"k8s-extns-pod-network"}'
spec:
  config: '{
    “cniVersion”: “0.3.1”,
    "type": "contrail-k8s-cni"
}'
----

throughout this book we'll use the first template to define our VNs in all
examples.

=== multiple-interface pod

with multiple VNs created, we can "attach" (you may also say "plug", or
"insert") any of them into a pod, with a pod yaml file like this:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      { "name": "VN-a" },
      { "name": "VN-b" },
      { "name": "other-ns/VN-c" }
    ]'
spec:
  containers:
----

another valid format:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: 'VN-a,VN-b,other-ns/VN-c'
spec:
  containers:
----

you probably notice, pods in a namespace not only can refer to the networks
defined in local NS, but also networks created on other namespaces using their
fully scoped name. this is very useful - the same network does not has to be
duplicated again and again in every NS that needs it, it can be defined just one
time and then referred anywhere else.

=== test multi-interface pod

we've understood the basic theories and explored the various templates. now it's
time to look at a "working example" in the real world. we'll start from
creating two VNs, examining the VN objects, then create a pod and attach the 2
VNs into it. we'll conclude the test and this section by examining the pod
interfaces and connectivity with other pods sharing the same VNs.

////
now you may want to test these theories in your setup
starting from creating your own yaml files based on these templates. if this is
the first time you work on this, you will most likely run into all kinds of
small issues here and there.
////

here is a yaml file of two VNs: `vn-left-1` and `vn-right-1`

----
$ cat vn-left-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "10.10.10.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-left-1
spec:
  config: '{ 
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

----
$ cat vn-right-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "20.20.20.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-right-1
  #namespace: default
spec:
  config: '{
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

create both VNs:

----
$ kubectl apply -f vn-left-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-left-1 created

$ kubectl apply -f vn-right-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-right-1 created
----

examine the VNs:

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-left-1       3s
vn-right-1      10s
----

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io vn-left-1 -o yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"k8s.cni.cncf.io/v1","kind":"NetworkAttachmentDefinition","metadata":{"annotations":{"opencontrail.org/cidr":"10.10.10.0/24","opencontrail.org/ip_fabric_forwarding":"false"},"name":"vn-left-1","namespace":"ns-user-1"},"spec":{"config":"{ \"cniVersion\": \"0.3.0\", \"type\": \"contrail-k8s-cni\" }"}}
    opencontrail.org/cidr: 10.10.10.0/24
    opencontrail.org/ip_fabric_forwarding: "false"
  creationTimestamp: 2019-06-13T14:17:42Z
  generation: 1
  name: vn-left-1
  namespace: ns-user-1
  resourceVersion: "777874"
  selfLink: /apis/k8s.cni.cncf.io/v1/namespaces/ns-user-1/network-attachment-definitions/vn-left-1
  uid: 01f167ad-8de6-11e9-bbbf-0050569e6cfc
spec:
  config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

the VNs are created, as expected. it seems nothing much exciting here. However,
if you login to the contrail UI, you will see something "unexpected". 

//old GUI:
//image::https://user-images.githubusercontent.com/2038044/59985880-f5886080-9601-11e9-98c9-791fec2fbe55.png[]

.contrail command: "main-menu" -> "virtual networks"

image::https://user-images.githubusercontent.com/2038044/60283772-f78b4180-98d7-11e9-9358-1ed47aeeef57.png[]

NOTE: make sure you select a correct "project", in this case it is `k8s-default`.

you won't be able to find any VN with the exact name `vn-left-1` or `vn-right-1`
in the UI. instead, what you will find are two VNs named
`k8s-vn-left-1-pod-network` and `k8s-vn-right-1-pod-network` got created. 

there is nothing wrong here. What happened is whenever a VN get created from
kubernetes, contrail automatically adds a prefix `k8s-` to the VN name that you
give in the network yaml file, and a suffix `-pod-network` in the end. This
makes sense because we know a VN can be created by different methods. with these
extra keywords embeded in the name, it is easier to tell how the VN was created
(from kubernetes or from the UI manually), what will it be used for, and also
potential VN name confliction is avoided.

here is yaml file of the `cirros` pod - a container version of the
classis cirros VM.

----
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
----

in pod annotations under metadata, we insert 2 VNs: `vn-left-1` and
`vn-right-1`. Now guess how many interfaces will the pod has on bootup?  you may
think it will be two because that is what we gave in the file. let's create the
pod and verify:

----
$ kubectl get pod -o wide
NAME    READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros  1/1    Running  0         20s  10.47.255.238  cent222  <none>

$ kubectl describe pod cirros
Name:               cirros
Namespace:          ns-user-1
Priority:           0
PriorityClassName:  <none>
Node:               cent222/10.85.188.20
Start Time:         Wed, 26 Jun 2019 12:51:30 -0400
Labels:             app=cirros
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.250",
                              "mac": "02:87:cf:6c:9a:98",
                              "name": "vn-left-1"
                          },
                          {
                              "ips": "10.47.255.238",
                              "mac": "02:87:98:cc:4e:98",
                              "name": "cluster-wide-default"
                          },
                          {
                              "ips": "20.20.20.1",
                              "mac": "02:87:f9:f9:88:98",
                              "name": "vn-right-1"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ 
                        { "name": "vn-left-1" }, { "name": "vn-right-1" } ]
                    kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"Pod","metadata":
                      {"annotations":{"k8s.v1.cni.cncf.io/networks":"[
                      { \"name\": \"vn-left-1\" }, { \"name\": \"vn-...
Status:             Running
IP:                 10.47.255.238
...<snipped>...
----

in `Annotations`, under `k8s.v1.cni.cncf.io/network-status` we see a list
`[...]`, which has 3 items each represented by a curly brace block `{}` of
key-value mappings. each curly brace block includes information about one
interface: the allocated IP, MAC and the VN it belongs to. so you will end up to
have 3 interfaces created in the pod instead of 2.  please notice the second
item which gives IP address `10.47.255.238`, that is the interface attached to
the "default pod network" named "cluster-wide-default", which is created by the
sytem. you can look at the default pod network as a "managment" network because
it is always "up and running" in every pod's network namespace. funtionally
it is no much different with the VN you create - except that you can't delete
it.

we can "login to" the pod, list the interfaces and verify the IP and MAC.

----
$ kubectl exec -it cirros sh
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
37: eth0@if38: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:47:06:d8:98 brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.238/12 scope global eth0
       valid_lft forever preferred_lft forever
39: eth1@if40: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:6b:a0:e2:98 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.250/24 scope global eth1
       valid_lft forever preferred_lft forever
41: eth2@if42: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:8e:8a:80:98 brd ff:ff:ff:ff:ff:ff
    inet 20.20.20.1/24 scope global eth2
       valid_lft forever preferred_lft forever
----

we see one lo interface and 3 interfaces plugged by contrail CNI, each with the
IP allocated from the corresponding VN. also you will notice the MAC addresses
match what we've seen in `kubectl describe` command output. 

NOTE: having the MAC address in the annotations may be very important under
certain cases - imaging you login a pod and for some reason you lose the track
of interface to VN mapping (e.g., you manually changed/removed the IPs, or the
pod's application reset the IP, etc) you can count on the MAC address! later In
"service chaining" section you will run into a scenario when you need to use the
MAC address to locate the proper interface, before you can even tell which
interface should be configured with which podIP that kubernetes allocated from a
VN. check "service chaining" section for more details.

you will see multiple-interfaces pod again in sevice-chaining example later on.
in that example the pod will be based on Juniper CSRX image instead of a general
docker image. but the basic idea remains the same.

== service chaining with CSRX

=== contrail service chaining introduction

service chaining is the idea of forwarding traffic through multiple network
entity in a certain order, each network entity do specific function such as
firewall, IPS , NAT , LB , …,etc. the legacy way of doing service chaining would
use standalone HW appliances which made service chaining inflexible, expensive
and takes a long time to setup. Dynamic service chaining is where network
functions deployed as VM or Container and could be chained automatically in a
logical way. in the next example we use contrail for services chaining between
two PODs in two different networking using CSRX container L4-L7 firewall to
secure the traffic between these two networks as shown in the figure:

.service chaining
image::https://user-images.githubusercontent.com/2038044/60268925-85a4ff00-98bb-11e9-94c3-219d41038642.png[]

[NOTE]
====
- left and right networks are just a common name used for simplicity and
  expected the traffic to follow from left to right but you can use your own
  names 
- make sure to configure the network before you attached a POD to it otherwise
  POD would fail to be created 
====

=== create VNs

so let’s start create two networks using this YAML files 

----
# cat vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.10.10.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-left
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'

# cat vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.20.20.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-right
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

----
# kubectl create -f vn-left.yaml
# kubectl create -f vn-right.yaml
----

Verify using Kubectl 
 
----
# kubectl get network-attachment-definition
NAME       AGE
vn-left    19d
vn-right   17d

# kubectl describe network-attachment-definition
Name:         vn-left
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.10.10.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-25T20:28:22Z
  Generation:          1
  Resource Version:    83111
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-left
  UID:                 a44fe276-7f2b-11e9-9ff0-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>


Name:         vn-right
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.20.20.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-28T07:14:02Z
  Generation:          1
  Resource Version:    380427
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-right
  UID:                 2b8d394f-8118-11e9-b36d-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>
----

It’s a good practice to confirm these two networks are seen now in contrail
before proceeding.  From the Contrail UI, select Configure > Networking >
Networks > default-domain > k8s-default, As shown in the figure which focus on
left network

NOTE: using namespace: default object in the YAML file for a network will create
it n in domain “default-domain” and project “K8s-default”

image::https://user-images.githubusercontent.com/2038044/60268927-863d9580-98bb-11e9-965a-b50f91d811d1.png[]

=== create client pods

Create two ubuntu Pods, one in each network using the annotation object

----
# cat left-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: left-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-left" }]'
spec:
  containers:
    - name: ubuntu-left-pod-sc
      image: virtualhops/ato-ubuntu:latest
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN


# cat right-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: right-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-right" }]'
spec:
  containers:
    - name: ubuntu-right-pod-sc
      image: virtualhops/ato-ubuntu:latest
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN

# kubectl create -f right-ubuntu-sc.yaml
# kubectl create -f left-ubuntu-sc.yaml


# kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
left-ubuntu-sc    1/1     Running   0          25h
right-ubuntu-sc   1/1     Running   0          25h

----

=== create CSRX pod

create Juniper CSRX container that have one interface on the left network and
one interface on the right network using this YAML file 

----
# cat csrx1-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: csrx1-sc
  labels:
    app: webapp-sc
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left" },
       { "name": "vn-right" }
   ]'
spec:
  containers:
  - name: csrx1-sc
    image: csrx
    ports:
    - containerPort: 22
    imagePullPolicy: Never
    stdin: true
    tty: true
    securityContext:
      privileged: true

# kubectl create -f csrx1-sc.yaml


Confirm the interface placement in the correct network 

# kubectl describe pod 
Name:               csrx1-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:31 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.2",
                              "mac": "02:84:71:f4:f2:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.20.20.2",
                              "mac": "02:84:8b:4c:18:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.248",
                              "mac": "02:84:59:7e:54:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }, { "name": "vn-right" } ]
Status:             Running
IP:                 10.47.255.248
Containers:
  csrx1-sc:
    Container ID:   docker://82b7605172d937895269d76850d083b6dc6e278e41cb45b4cb8cee21283e4f17
    Image:          csrx
    Image ID:       docker://sha256:329e805012bdf081f4a15322f994e5e3116b31c90f108a19123cf52710c7617e

...<snipped>...

Name:               left-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:20 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.1",
                              "mac": "02:7d:b1:09:00:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.47.255.249",
                              "mac": "02:7d:99:ff:62:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }]
Status:             Running
IP:                 10.47.255.249
Containers:
  ubuntu-left-pod-sc:
    Container ID:   docker://2f9a22568d844c68a1c4a45de4a81478958233052e08d4473742827482b244cd
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
   
...<snipped>...

Name:               right-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 04:09:18 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.20.20.1",
                              "mac": "02:89:cc:86:48:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.252",
                              "mac": "02:89:b0:8e:98:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-right" }]
Status:             Running
IP:                 10.47.255.252
Containers:
  ubuntu-right-pod-sc:
    Container ID:   docker://4e0b6fa085905be984517a11c3774517d01f481fa43aadd76a633ef15c58cbfe
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
  
 ...<snipped>...
----

NOTE: each container has one interface belong to “cluster-wide-default” network
regardless the use of the annotations object because annotations object above
creates and put one extra interface in a specific network 

=== verify podIP

.verify podIP

Login to the left, right Pods and the CSRX to confirm the IP/MAC address 
 
----
# kubectl exec -it left-ubuntu-sc bash
root@left-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
13: eth0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:99:ff:62:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.249/12 scope global eth0
       valid_lft forever preferred_lft forever
15: eth1@if16: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:b1:09:00:8d brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.1/24 scope global eth1
       valid_lft forever preferred_lft forever



# kubectl exec -it right-ubuntu-sc bash
root@right-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
23: eth0@if24: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:b0:8e:98:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.252/12 scope global eth0
       valid_lft forever preferred_lft forever
25: eth1@if26: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:cc:86:48:8d brd ff:ff:ff:ff:ff:ff
    inet 10.20.20.1/24 scope global eth1
       valid_lft forever preferred_lft forever


# kubectl exec -it csrx1-sc cli
root@csrx1-sc>
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d
----

NOTE: unlike other PODs the CSRX didn’t acquire IP with DHCP and it start with
factory default configuration hence it need to be configured. 

NOTE: By default, CSRX eth0 is visible only from shell and used for management.
And when attaching networks, the first attach network is mapped to eth1 which is
GE-0/0/1 And the second attach is mapped to eth2 which is GE-0/0/0

.configure CSRX IP
Configure this basic setup on the CSRX, to assign the correct IP address use the
MAC/IP address mapping from the “ kubectl describe pod” command show output as
well configure default security policy to allow everything for now 

----
set interfaces ge-0/0/1 unit 0 family inet address 10.10.10.2/24
set interfaces ge-0/0/0 unit 0 family inet address 10.20.20.2/24

set security zones security-zone trust interfaces ge-0/0/0
set security zones security-zone untrust interfaces ge-0/0/1 
set security policies default-policy permit-all 
commit
----

verify the IP address assigned on the CSRX

----
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

  Logical interface ge-0/0/1.0 (Index 100)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.10.10.0/24, Local: 10.10.10.2

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d

  Logical interface ge-0/0/0.0 (Index 200)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.20.20.0/24, Local: 10.20.20.2
----

=== ping test

From the Left POD try to ping the left POD, ping would fail as there is no route 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 1999ms

root@left-ubuntu-sc:/# ip r
default via 10.47.255.254 dev eth0 
10.10.10.0/24 dev eth1  proto kernel  scope link  src 10.10.10.1 
10.32.0.0/12 dev eth0  proto kernel  scope link  src 10.47.255.249
----

Adding static route to the left and right PODs and try to ping again 

----
root@left-ubuntu-sc:/# ip r add 10.20.20.0/24 via 10.10.10.2

root@right-ubuntu-sc:/# ip r add 10.10.10.0/24 via 10.20.20.2

root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
----

Still ping failed, as we didn’t create the service chaining which will also take
care of the routing. let’s see what happen to our packets 

----
root@csrx1-sc# run show security flow session 
Total sessions: 0
----

No session on the CSRX.  

=== troubleshooting ping issue

Login to the compute node “cent22” that host this container to dump the traffic
using tshark and check the routing To get the interface linking the containers 

----
[root@cent22 ~]# vif -l
Vrouter Interface Table

Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
       Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
       D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
       Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
       Uuf=Unknown Unicast Flood, Vof=VLAN insert/strip offload, Df=Drop New Flows, L=MAC Learning Enabled
       Proxy=MAC Requests Proxied Always, Er=Etree Root, Mn=Mirror without Vlan Tag, Ig=Igmp Trap Enabled

...<snipped>...

vif0/3      OS: tapeth0-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.252
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10760  bytes:452800 errors:0
            TX packets:14239  bytes:598366 errors:0
            Drops:10744

vif0/4      OS: tapeth1-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.1
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13002  bytes:867603 errors:0
            TX packets:16435  bytes:1046981 errors:0
            Drops:10805

vif0/5      OS: tapeth0-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.249
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10933  bytes:459186 errors:0
            TX packets:14536  bytes:610512 errors:0
            Drops:10933

vif0/6      OS: tapeth1-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.1
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:12625  bytes:1102433 errors:0
            TX packets:15651  bytes:810689 errors:0
            Drops:10957

vif0/7      OS: tapeth0-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.248
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:20996  bytes:1230688 errors:0
            TX packets:27205  bytes:1142610 errors:0
            Drops:21226

vif0/8      OS: tapeth1-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.2
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13908  bytes:742243 errors:0
            TX packets:29023  bytes:1790589 errors:0
            Drops:10514

vif0/9      OS: tapeth2-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.2
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:16590  bytes:1053659 errors:0
            TX packets:31321  bytes:1635153 errors:0
            Drops:10421

...<snipped>...

----

Note that Vif0/3 and Vif0/4 are bounded with the right POD and both linked to
tapeth0-89a4e2 and tapeth1-89a4e2 respectively same goes for the left POD for
Vif0/5 and vif0/6 while vif0/7, vif 0/8 and vif0/9 are bound with CSRX1.| from
that you can also see the number of packets/bytes hits that interface as well
the VRF which is this interface belong in here VRF 3 is for the
default-cluster-network while VRF 6 for the left network and VRF 5 for the right
network in this figure you can see the interface mapping from the all
prospective (container, Linux , vr-agent) 

image::https://user-images.githubusercontent.com/2038044/60268930-863d9580-98bb-11e9-9dc3-b0c5598ff528.png[]

try to ping again from the left POD to the right POD and use tshark on the tap
interface for the right POD for further inspection 

----
[root@cent22 ~]# tshark -i tapeth1-89a4e2
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-89a4e2'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.254 (Request)
  2 0.000037656 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.253 (Request)
  3 1.379993896 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Who has 10.20.20.1?  Tell 10.20.20.253
----

Looks like the ping isn’t reaching the right POD at all , lets see on the CSRX
left network tap interface  

----
[root@cent22 ~]# tshark -i tapeth1-844f1c
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-844f1c'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Who has 0.255.255.252?  Tell 0.0.0.0
  2 0.201392098   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=410/39425, ttl=63
  3 0.201549430   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  4 1.201444156   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=411/39681, ttl=63
  5 1.201600074   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  6 1.394074095 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.254 (Request)
  7 1.394108344 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.253 (Request)
  8 2.201462515   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=412/39937, ttl=63
----

We can see the packet but there is nothing in the CSRX security prospective to
drop this packet

checking the routing table of the left network VRF by logging to the
`vrouter_vrouter-agent_1` docker in the compute node 

----
[root@cent22 ~]# docker ps | grep vrouter
9a737df53abe        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest   "/entrypoint.sh /usr…"   2 weeks ago         Up 47 hours                             vrouter_vrouter-agent_1
e25f1467403d        ci-repo.englab.juniper.net:5000/contrail-nodemgr:master-latest         "/entrypoint.sh /bin…"   2 weeks ago         Up 47 hours                             vrouter_nodemgr_1

[root@cent22 ~]# docker exec -it vrouter_vrouter-agent_1 bash
(vrouter-agent)[root@cent22 /]$ 
(vrouter-agent)[root@cent22 /]$ rt --dump 6 | grep 10.20.20.
(vrouter-agent)[root@cent22 /]$
----

Note that 6 is the routing table VRF of the left network, same would goes for
the right network VRF routing table there is missing route 

    (vrouter-agent)[root@cent22 /]$ rt --dump 5 | grep 10.10.10.
    (vrouter-agent)[root@cent22 /]$

So even if all the PODs are hosted on the same compute nodes, they can’t reach
each other. And if these PODs are hosted on different compute nodes then you
have a bigger problem to solve. Service chaining isn’t about adjusting the routes
on the containers but mainly about exchange routes between the vrouter-agent
between the compute nodes regardless of the location of the POD, as well adjust
that automatically if the POD moved to another compute node. Before we build
service chaining lets address an important concerns for network administrator
who are not fan of this kind of CLI troubleshooting, can we do the same
troubleshooting using contrail controller GUI? 

the answer is yes and lets do it.

From the Contrail Controller UI, select monitor > infrastructure > virtual
router then select the node the that host the POD , in our case “Cent22.local” 

image::https://user-images.githubusercontent.com/2038044/60268931-863d9580-98bb-11e9-9682-d330878fa386.png[]

as shown in the figure from the interface tab which is equivalent to running “
vif -l” command on the vrouter_vrouter-agent-1 container and even showing more
information notice the mapping between the instance ID and tap interface naming
where the first 6 character of the instance ID are always reflected in the tap
interface naming

to check the routing tables of each VRF move to the “routes” tab and select the
VRF you want to see

image::https://user-images.githubusercontent.com/2038044/60268935-86d62c00-98bb-11e9-8eaa-820578b11127.png[]

If we select the left network ( the name is longer as it include the domain ,
project ) we can confirm there is not 10.20.20.0/24 prefix from the right
network We can also check the mac address learned in the left network by
selecting L2 ( which is equvilant to “rt --dump 6 --family bridge” command 

image::https://user-images.githubusercontent.com/2038044/60268936-86d62c00-98bb-11e9-9050-ca104b278a1a.png[]

=== create service chaining

Now lets utilize the CSRX to service chaining using contrail command GUI

creating Service chaining is 4 steps make sure to do them in this order 

1. create Service template 
2. creating service instance based on the service template you created before
3. creating network policy and select the service instance you created before
4. apply this network policy on network   

NOTE: since contrail command GUI is the solution to provide a single point of
management for all environments, we will use it to build service changing but
you still can use the normal contrail controller GUI to build service changing
 
Login to contrail command GUI ( in our setup https://10.85.188.16:9091/) then select service > catalog > create 

image::https://user-images.githubusercontent.com/2038044/60268937-86d62c00-98bb-11e9-8744-b8213b5246ed.png[]
image::https://user-images.githubusercontent.com/2038044/60268938-876ec280-98bb-11e9-991b-a54dedadfbcd.png[]
 
insret a name of services template “myweb-CSRX-CS” in here then chose v2 ,
virtual machine ( no other option available) for service mode we will work with
In-network and firewall as service type  

image::https://user-images.githubusercontent.com/2038044/60268941-876ec280-98bb-11e9-8f68-5c49af9b06d1.png[]

Select interfaces management, left and right then click create
 
image::https://user-images.githubusercontent.com/2038044/60268942-876ec280-98bb-11e9-8c7c-ac2a95da9ab0.png[]

Now select deployment and click create to create the service instances

image::https://user-images.githubusercontent.com/2038044/60268943-876ec280-98bb-11e9-9cf8-12b240de0286.png[]

Insert a name for this service instance then select from the drop down menu the
name of the template you created before then chose the proper network from the
prospective of the CSRX being the instance (container in that case) that will do
the service chaining and click on port tuples to expand it 

image::https://user-images.githubusercontent.com/2038044/60268945-88075900-98bb-11e9-87fa-375337170b12.png[]

then for each of the three interface bound one interface of the CSRX then click create

NOTE: the name of the virtual machine interface isn’t shown in the drop down
menu instead the instance ID, you can identify that from the tap interface name
as we showed before.  In other word all you have to know is most 6 left
character for any interface belong to that container as all the interface in a
given instance ( VM or container)  share the same first characters from the left 
 
Before you procced make sure the status of the three interfaces are up and they
are showing the correct IP address of the CSRX instance 
 
image::https://user-images.githubusercontent.com/2038044/60268947-88075900-98bb-11e9-9b0a-ecf0c6a03e33.png[]

To create network policy go to overlay > network policies > create 

image::https://user-images.githubusercontent.com/2038044/60268948-88075900-98bb-11e9-88ba-f5f7a02161b0.png[]
 
Insert a name for your network policy then in the first rule add left network as source network and right network as destination with action pass 

image::https://user-images.githubusercontent.com/2038044/60268949-889fef80-98bb-11e9-844a-326b5d506038.png[]

Select advanced option to attached the service instance you create before and click create 

image::https://user-images.githubusercontent.com/2038044/60268951-889fef80-98bb-11e9-84c5-f354b3d8938e.png[]

To attach this network policy to network click virtual network and select the left network and edit 

image::https://user-images.githubusercontent.com/2038044/60268953-889fef80-98bb-11e9-8826-2626a76c3d4a.png[]

In network policies select the network policy you just created from the drop down menu then click save 
do the same for the right network

image::https://user-images.githubusercontent.com/2038044/60268955-89388600-98bb-11e9-9605-14fbc8d30fbe.png[]

=== verify service chaining

Now lets check the effect of this service changing on routing 
From the Contrail Controller module control node (http://10.85.188.16:8143 in
oursetup), select monitor > infrastructure > virtual router then select the node
the that host the POD , in our case “Cent22.local” then select the “routes” tab
and select the left VRF 
 
image::https://user-images.githubusercontent.com/2038044/60268956-89388600-98bb-11e9-9e82-7d5fbddf38f8.png[]

Now we can the right networks host routes has been leaked to the left network
(10.20.20.1/32 , 10.20.20.2/32 in this case) 

Now let’s try to ping the right pod from the left pod to see the session created on the CSRX 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
64 bytes from 10.20.20.1: icmp_seq=1 ttl=61 time=0.863 ms
64 bytes from 10.20.20.1: icmp_seq=2 ttl=61 time=0.290 ms
^C
--- 10.20.20.1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.290/0.576/0.863/0.287 ms

root@csrx1-sc# run show security flow session 
Session ID: 5378, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/2 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/2;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 

Session ID: 5379, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/3 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/3;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 
Total sessions: 2

----

=== security policy

Now let try to create security policy on the CSRX to allow only http and https

----
root@csrx1-sc# show security 
policies {
    traceoptions {
        file ayma;
        flag all;
    }
    from-zone trust to-zone untrust {
        policy only-http-s {
            match {
                source-address any;
                destination-address any;
                application [ junos-http junos-https ];
            }
            then {
                permit;
                log {
                    session-init;
                    session-close;
                }
            }
        }
        policy deny-ping {
            match {
                source-address any;
                destination-address any;
                application any;        
            }                           
            then {                      
                reject;                 
                log {                   
                    session-init;       
                    session-close;      
                }                       
            }                           
        }                               
    }                                   
    default-policy {                    
        deny-all;                       
    }                                   
}                                       
zones {                                 
    security-zone trust {               
        interfaces {                    
            ge-0/0/0.0;                 
        }                               
    }                                   
    security-zone untrust {             
        interfaces {                    
            ge-0/0/1.0;                 
        }                               
    }                                   
}
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2000ms
----

the ping failed as the policy on the CSRX drop it 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:04:01 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/8->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5394 N/A N/A -1
Jun 14 23:04:02 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/9->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5395 N/A N/A -1
Try to send http traffic from the left to the right POD and verify the session status on the CSRX
root@left-ubuntu-sc:/# wget 10.20.20.1
--2019-06-14 23:07:34--  http://10.20.20.1/
Connecting to 10.20.20.1:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 11510 (11K) [text/html]
Saving to: 'index.html.4'

100%[======================================>] 11,510      --.-K/s   in 0s      

2019-06-14 23:07:34 (278 MB/s) - 'index.html.4' saved [11510/11510]
----

And in the CSRX we can see the session creation 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Adding resolved unicast NH. dest: 10.20.20.1, proto v4 (peer initiated)
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Sending resolve request for stale ARP entry (b). NH: 5507 dest: 10.20.20.1
Jun 14 23:07:34 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CREATE: session created 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 N/A(N/A) ge-0/0/1.0 UNKNOWN UNKNOWN UNKNOWN N/A N/A -1
Jun 14 23:07:35 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CLOSE: session closed TCP FIN: 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 14(940) 12(12452) 2 UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 UNKNOWN N/A N/A -1
----

